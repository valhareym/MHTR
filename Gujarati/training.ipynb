{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "083f10f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:33:58.548288Z",
     "iopub.status.busy": "2021-11-28T02:33:58.547037Z",
     "iopub.status.idle": "2021-11-28T02:34:07.241237Z",
     "shell.execute_reply": "2021-11-28T02:34:07.240275Z"
    },
    "id": "Ty_YA-cLBpyr",
    "papermill": {
     "duration": 8.726067,
     "end_time": "2021-11-28T02:34:07.241375",
     "exception": false,
     "start_time": "2021-11-28T02:33:58.515308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.networks.nets import DenseNet121\n",
    "from monai.transforms import *\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "#print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0cc38ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:07.341161Z",
     "iopub.status.busy": "2021-11-28T02:34:07.340389Z",
     "iopub.status.idle": "2021-11-28T02:34:09.970834Z",
     "shell.execute_reply": "2021-11-28T02:34:09.972986Z"
    },
    "papermill": {
     "duration": 2.662922,
     "end_time": "2021-11-28T02:34:09.973263",
     "exception": false,
     "start_time": "2021-11-28T02:34:07.310341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '../input/gujarati-ocr-typed-gujarati-characters/Gujarati/Train' \n",
    "test_dir='../input/gujarati-ocr-typed-gujarati-characters/Gujarati/Test'\n",
    "\n",
    "class_names0 = os.listdir(data_dir)\n",
    "class_names=sorted(class_names0)\n",
    "\n",
    "num_class = len(class_names)\n",
    "image_files = [[os.path.join(data_dir, class_name, x) \n",
    "               for x in os.listdir(os.path.join(data_dir, class_name))] \n",
    "               for class_name in class_names]\n",
    "\n",
    "timage_files = [[os.path.join(test_dir, class_name, x) \n",
    "               for x in os.listdir(os.path.join(test_dir, class_name))] \n",
    "               for class_name in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4019cb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:10.059149Z",
     "iopub.status.busy": "2021-11-28T02:34:10.053265Z",
     "iopub.status.idle": "2021-11-28T02:34:10.071727Z",
     "shell.execute_reply": "2021-11-28T02:34:10.072503Z"
    },
    "papermill": {
     "duration": 0.059767,
     "end_time": "2021-11-28T02:34:10.072679",
     "exception": false,
     "start_time": "2021-11-28T02:34:10.012912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_file_list = []\n",
    "image_label_list = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    image_file_list.extend(image_files[i])\n",
    "    image_label_list.extend([i] * len(image_files[i]))\n",
    "num_total = len(image_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4b42e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:10.174340Z",
     "iopub.status.busy": "2021-11-28T02:34:10.173101Z",
     "iopub.status.idle": "2021-11-28T02:34:10.176154Z",
     "shell.execute_reply": "2021-11-28T02:34:10.175221Z"
    },
    "papermill": {
     "duration": 0.062587,
     "end_time": "2021-11-28T02:34:10.176354",
     "exception": false,
     "start_time": "2021-11-28T02:34:10.113767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "timage_file_list = []\n",
    "timage_label_list = []\n",
    "for i, class_name in enumerate(class_names):\n",
    "    timage_file_list.extend(timage_files[i])\n",
    "    timage_label_list.extend([i] * len(timage_files[i]))\n",
    "tnum_total = len(timage_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4474349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:10.269188Z",
     "iopub.status.busy": "2021-11-28T02:34:10.267969Z",
     "iopub.status.idle": "2021-11-28T02:34:10.295429Z",
     "shell.execute_reply": "2021-11-28T02:34:10.296247Z"
    },
    "id": "ZaHFhidyCBJa",
    "papermill": {
     "duration": 0.078138,
     "end_time": "2021-11-28T02:34:10.296488",
     "exception": false,
     "start_time": "2021-11-28T02:34:10.218350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total image count: 53043\n",
      "Image dimensions: 32 x 32\n",
      "Label names: ['A', 'AA', 'Ai', 'Ala', 'Alaa', 'Alai', 'Alam', 'Alau', 'Ale', 'Alee', 'Ali', 'Alo', 'Aloo', 'Alu', 'Am', 'Ana', 'Anaa', 'Anai', 'Anam', 'Anau', 'Ane', 'Anee', 'Ani', 'Ano', 'Anoo', 'Anu', 'Au', 'Ba', 'Baa', 'Bai', 'Bam', 'Bau', 'Be', 'Bee', 'Bha', 'Bhaa', 'Bhai', 'Bham', 'Bhau', 'Bhe', 'Bhee', 'Bhi', 'Bho', 'Bhoo', 'Bhu', 'Bi', 'Bo', 'Boo', 'Bu', 'Cha', 'Chaa', 'Chai', 'Cham', 'Chau', 'Che', 'Chee', 'Chha', 'Chhaa', 'Chhai', 'Chham', 'Chhau', 'Chhe', 'Chhee', 'Chhi', 'Chho', 'Chhoo', 'Chhu', 'Chi', 'Cho', 'Choo', 'Chu', 'DDO', 'DDa', 'DDaa', 'DDai', 'DDam', 'DDau', 'DDe', 'DDee', 'DDha', 'DDhaa', 'DDhai', 'DDham', 'DDhau', 'DDhe', 'DDhee', 'DDhi', 'DDho', 'DDhoo', 'DDhu', 'DDi', 'DDoo', 'DDu', 'Da', 'Daa', 'Dai', 'Dam', 'Dau', 'De', 'Dee', 'Dha', 'Dhaa', 'Dhai', 'Dham', 'Dhau', 'Dhe', 'Dhee', 'Dhi', 'Dho', 'Dhoo', 'Dhu', 'Di', 'Do', 'Doo', 'Du', 'E', 'EE', 'Ga', 'Gaa', 'Gai', 'Gam', 'Gau', 'Ge', 'Gee', 'Gha', 'Ghaa', 'Ghai', 'Gham', 'Ghau', 'Ghe', 'Ghee', 'Ghi', 'Gho', 'Ghoo', 'Ghu', 'Gi', 'Gna', 'Gnaa', 'Gnai', 'Gnam', 'Gnau', 'Gne', 'Gnee', 'Gni', 'Gno', 'Gnoo', 'Gnu', 'Go', 'Goo', 'Gu', 'Ha', 'Haa', 'Hai', 'Ham', 'Hau', 'He', 'Hee', 'Hi', 'Ho', 'Hoo', 'Hu', 'I', 'Ja', 'Jaa', 'Jai', 'Jam', 'Jau', 'Je', 'Jee', 'Ji', 'Jo', 'Joo', 'Ju', 'Ka', 'Kaa', 'Kai', 'Kam', 'Kau', 'Ke', 'Kee', 'Kha', 'Khaa', 'Khai', 'Kham', 'Khau', 'Khe', 'Khee', 'Khi', 'Kho', 'Khoo', 'Khu', 'Ki', 'Ko', 'Koo', 'Ksh', 'Ksha', 'Kshai', 'Ksham', 'Kshau', 'Kshe', 'Kshee', 'Kshi', 'Ksho', 'Kshoo', 'Kshu', 'Ku', 'La', 'Laa', 'Lai', 'Lam', 'Lau', 'Le', 'Lee', 'Li', 'Lo', 'Loo', 'Lu', 'Ma', 'Maa', 'Mai', 'Mam', 'Mau', 'Me', 'Mee', 'Mi', 'Mo', 'Moo', 'Mu', 'Na', 'Naa', 'Nai', 'Nam', 'Nau', 'Ne', 'Nee', 'Ni', 'No', 'Noo', 'Nu', 'O', 'OO', 'Pa', 'Paa', 'Pai', 'Pam', 'Pau', 'Pe', 'Pee', 'Pha', 'Phaa', 'Phai', 'Pham', 'Phau', 'Phe', 'Phee', 'Phi', 'Pho', 'Phoo', 'Phu', 'Pi', 'Po', 'Poo', 'Pu', 'Ra', 'Raa', 'Rai', 'Ram', 'Rau', 'Re', 'Ree', 'Ri', 'Ro', 'Roo', 'Ru', 'SSh', 'SSha', 'SShai', 'SSham', 'SShau', 'SShe', 'SShee', 'SShi', 'SSho', 'SShoo', 'SShu', 'Sa', 'Saa', 'Sai', 'Sam', 'Sau', 'Se', 'See', 'Sha', 'Shaa', 'Shai', 'Sham', 'Shau', 'She', 'Shee', 'Shi', 'Sho', 'Shoo', 'Shu', 'Si', 'So', 'Soo', 'Su', 'TTa', 'TTaa', 'TTai', 'TTam', 'TTau', 'TTe', 'TTee', 'TTha', 'TThaa', 'TThai', 'TTham', 'TThau', 'TThe', 'TThee', 'TThi', 'TTho', 'TThoo', 'TThu', 'TTi', 'TTo', 'TToo', 'TTu', 'Ta', 'Taa', 'Tai', 'Tam', 'Tau', 'Te', 'Tee', 'Tha', 'Thaa', 'Thai', 'Tham', 'Thau', 'The', 'Thee', 'Thi', 'Tho', 'Thoo', 'Thu', 'Ti', 'To', 'Too', 'Tu', 'U', 'Va', 'Vaa', 'Vai', 'Vam', 'Vau', 'Ve', 'Vee', 'Vi', 'Vo', 'Voo', 'Vu', 'Ya', 'Yaa', 'Yai', 'Yam', 'Yau', 'Ye', 'Yee', 'Yi', 'Yo', 'Yoo', 'Yu', 'Za', 'Zaa', 'Zai', 'Zam', 'Zau', 'Ze', 'Zee', 'Zi', 'Zo', 'Zoo', 'Zu']\n",
      "Label counts: [144, 143, 142, 140, 140, 140, 51, 140, 140, 140, 145, 140, 140, 140, 140, 140, 140, 150, 51, 140, 140, 140, 190, 145, 140, 140, 141, 140, 140, 140, 51, 140, 140, 140, 140, 140, 140, 51, 140, 140, 140, 150, 140, 140, 140, 150, 140, 140, 140, 140, 150, 140, 46, 149, 140, 140, 140, 140, 150, 51, 150, 140, 140, 150, 139, 194, 245, 160, 150, 140, 140, 140, 140, 140, 150, 51, 150, 150, 160, 140, 140, 140, 46, 140, 150, 140, 150, 140, 140, 150, 164, 140, 150, 140, 140, 150, 51, 150, 139, 150, 140, 140, 140, 102, 139, 140, 150, 150, 140, 140, 140, 150, 150, 179, 140, 142, 143, 140, 170, 170, 51, 140, 139, 150, 140, 150, 140, 51, 140, 150, 150, 150, 140, 140, 139, 160, 140, 150, 150, 51, 140, 140, 140, 150, 140, 140, 150, 159, 140, 140, 140, 160, 140, 51, 140, 220, 150, 150, 140, 140, 150, 141, 140, 140, 140, 51, 140, 150, 140, 180, 140, 140, 140, 140, 140, 135, 49, 132, 149, 170, 140, 140, 160, 51, 150, 140, 150, 150, 140, 194, 139, 160, 140, 140, 140, 140, 140, 51, 140, 140, 140, 160, 140, 188, 140, 140, 140, 140, 140, 51, 140, 140, 140, 150, 140, 140, 140, 140, 140, 140, 51, 140, 150, 140, 150, 140, 140, 140, 140, 140, 150, 51, 140, 140, 160, 149, 140, 140, 140, 143, 143, 140, 140, 140, 51, 140, 140, 140, 140, 140, 140, 51, 140, 149, 159, 150, 140, 140, 140, 150, 170, 140, 150, 140, 140, 140, 51, 140, 160, 140, 160, 140, 140, 150, 140, 140, 150, 46, 140, 140, 140, 150, 140, 140, 150, 140, 140, 150, 51, 140, 140, 150, 140, 140, 140, 51, 140, 140, 140, 160, 150, 140, 180, 150, 140, 140, 150, 140, 140, 140, 153, 140, 140, 150, 140, 140, 140, 51, 140, 150, 150, 160, 150, 204, 184, 150, 140, 194, 179, 140, 140, 140, 51, 150, 140, 140, 140, 140, 140, 51, 140, 140, 140, 150, 140, 204, 199, 150, 140, 150, 140, 143, 140, 140, 140, 51, 150, 180, 140, 150, 140, 140, 140, 140, 140, 140, 46, 140, 150, 130, 160, 140, 140, 140, 140, 140, 140, 51, 140, 140, 140, 150, 150, 140, 140]\n"
     ]
    }
   ],
   "source": [
    "image_width, image_height = Image.open(image_file_list[0]).size\n",
    "\n",
    "print('Total image count:', num_total)\n",
    "print(\"Image dimensions:\", image_width, \"x\", image_height)\n",
    "print(\"Label names:\", class_names)\n",
    "print(\"Label counts:\", [len(image_files[i]) for i in range(num_class)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed626aaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:10.467861Z",
     "iopub.status.busy": "2021-11-28T02:34:10.466868Z",
     "iopub.status.idle": "2021-11-28T02:34:12.366754Z",
     "shell.execute_reply": "2021-11-28T02:34:12.367337Z"
    },
    "id": "p7kXrcmPCQPU",
    "papermill": {
     "duration": 1.948802,
     "end_time": "2021-11-28T02:34:12.367547",
     "exception": false,
     "start_time": "2021-11-28T02:34:10.418745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots(3,3, figsize=(12,12))\n",
    "for i,k in enumerate(np.random.randint(num_total, size=9)):\n",
    "    im = Image.open(image_file_list[k])\n",
    "    arr = np.array(im)\n",
    "    plt.subplot(3,3, i+1)\n",
    "    plt.xlabel(class_names[image_label_list[k]])\n",
    "    plt.imshow(arr, cmap='gray', vmin=0, vmax=255)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5d1ba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:12.513705Z",
     "iopub.status.busy": "2021-11-28T02:34:12.498719Z",
     "iopub.status.idle": "2021-11-28T02:34:12.557299Z",
     "shell.execute_reply": "2021-11-28T02:34:12.558204Z"
    },
    "id": "WuryrHlpCYfK",
    "papermill": {
     "duration": 0.113698,
     "end_time": "2021-11-28T02:34:12.558445",
     "exception": false,
     "start_time": "2021-11-28T02:34:12.444747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47784 5259\n"
     ]
    }
   ],
   "source": [
    "valid_frac = 0.1\n",
    "trainX,trainY = [],[]\n",
    "valX,valY = [],[]\n",
    "for i in range(num_total):\n",
    "    rann = np.random.random()\n",
    "    if rann < valid_frac:\n",
    "        valX.append(image_file_list[i])\n",
    "        valY.append(image_label_list[i])\n",
    "    else:\n",
    "        trainX.append(image_file_list[i])\n",
    "        trainY.append(image_label_list[i])\n",
    "\n",
    "print(len(trainX),len(valX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6a8b6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:12.626247Z",
     "iopub.status.busy": "2021-11-28T02:34:12.613313Z",
     "iopub.status.idle": "2021-11-28T02:34:12.630435Z",
     "shell.execute_reply": "2021-11-28T02:34:12.631257Z"
    },
    "papermill": {
     "duration": 0.046784,
     "end_time": "2021-11-28T02:34:12.631490",
     "exception": false,
     "start_time": "2021-11-28T02:34:12.584706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23100\n"
     ]
    }
   ],
   "source": [
    "testX,testY = [],[] \n",
    "for i in range(tnum_total):   \n",
    "    testX.append(timage_file_list[i])\n",
    "    testY.append(timage_label_list[i])\n",
    "    \n",
    "print(len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1d77a40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:12.720304Z",
     "iopub.status.busy": "2021-11-28T02:34:12.719280Z",
     "iopub.status.idle": "2021-11-28T02:34:12.738056Z",
     "shell.execute_reply": "2021-11-28T02:34:12.737338Z"
    },
    "papermill": {
     "duration": 0.079642,
     "end_time": "2021-11-28T02:34:12.738209",
     "exception": false,
     "start_time": "2021-11-28T02:34:12.658567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainX=np.array(trainX)\n",
    "trainY=np.array(trainY)\n",
    "valX=np.array(valX)\n",
    "valY=np.array(valY)\n",
    "testX=np.array(testX)\n",
    "testY=np.array(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9668d41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:12.903372Z",
     "iopub.status.busy": "2021-11-28T02:34:12.902460Z",
     "iopub.status.idle": "2021-11-28T02:34:12.905981Z",
     "shell.execute_reply": "2021-11-28T02:34:12.905373Z"
    },
    "papermill": {
     "duration": 0.035623,
     "end_time": "2021-11-28T02:34:12.906138",
     "exception": false,
     "start_time": "2021-11-28T02:34:12.870515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SumDimension(Transform):\n",
    "    def __init__(self, dim=1):\n",
    "        self.dim = dim\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return inputs.sum(self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa3cf672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:12.966585Z",
     "iopub.status.busy": "2021-11-28T02:34:12.965512Z",
     "iopub.status.idle": "2021-11-28T02:34:12.967929Z",
     "shell.execute_reply": "2021-11-28T02:34:12.968543Z"
    },
    "papermill": {
     "duration": 0.036189,
     "end_time": "2021-11-28T02:34:12.968796",
     "exception": false,
     "start_time": "2021-11-28T02:34:12.932607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Astype(Transform):\n",
    "    def __init__(self, type='uint8'):\n",
    "        self.type = type\n",
    "    def __call__(self, inputs):\n",
    "        return inputs.astype(self.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d620b10e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:13.034294Z",
     "iopub.status.busy": "2021-11-28T02:34:13.027107Z",
     "iopub.status.idle": "2021-11-28T02:34:13.037394Z",
     "shell.execute_reply": "2021-11-28T02:34:13.036813Z"
    },
    "id": "PMsUgaYNCfrw",
    "papermill": {
     "duration": 0.041952,
     "end_time": "2021-11-28T02:34:13.037525",
     "exception": false,
     "start_time": "2021-11-28T02:34:12.995573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transforms = Compose([\n",
    "    LoadImage(image_only=True),\n",
    "    SumDimension(2),    \n",
    "    NormalizeIntensity(),\n",
    "    Astype(),\n",
    "    #ScaleIntensity(),\n",
    "    #Astype(),\n",
    "    #RandRotate(range_x=10, prob=0.2, keep_size=True),\n",
    "    #RandFlip(spatial_axis=0, prob=0.5),\n",
    "    #RandZoom(min_zoom=1.0, max_zoom=1.1, prob=0.3, keep_size=True),\n",
    "    AddChannel(),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    LoadImage(image_only=True),\n",
    "    SumDimension(2),    \n",
    "    NormalizeIntensity(),\n",
    "    Astype(),\n",
    "    #ScaleIntensity(),\n",
    "    #Astype(),    \n",
    "    AddChannel(),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "act = Activations(softmax=True)\n",
    "to_onehot = AsDiscrete(to_onehot=num_class, n_classes=num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6fef7b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:13.151632Z",
     "iopub.status.busy": "2021-11-28T02:34:13.150464Z",
     "iopub.status.idle": "2021-11-28T02:34:13.153175Z",
     "shell.execute_reply": "2021-11-28T02:34:13.153808Z"
    },
    "papermill": {
     "duration": 0.037164,
     "end_time": "2021-11-28T02:34:13.154020",
     "exception": false,
     "start_time": "2021-11-28T02:34:13.116856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MedNISTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_files, labels, transforms):\n",
    "        self.image_files = image_files\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transforms(self.image_files[index]), self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30bacf1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:13.216904Z",
     "iopub.status.busy": "2021-11-28T02:34:13.215759Z",
     "iopub.status.idle": "2021-11-28T02:34:13.218343Z",
     "shell.execute_reply": "2021-11-28T02:34:13.218982Z"
    },
    "id": "JJgCYleyCpTT",
    "papermill": {
     "duration": 0.037329,
     "end_time": "2021-11-28T02:34:13.219160",
     "exception": false,
     "start_time": "2021-11-28T02:34:13.181831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = MedNISTDataset(trainX, trainY, train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "val_ds = MedNISTDataset(valX, valY, val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, num_workers=2)\n",
    "\n",
    "test_ds = MedNISTDataset(testX, testY, val_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaff8996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:13.282823Z",
     "iopub.status.busy": "2021-11-28T02:34:13.282055Z",
     "iopub.status.idle": "2021-11-28T02:34:17.142902Z",
     "shell.execute_reply": "2021-11-28T02:34:17.142285Z"
    },
    "id": "3efM0bwsC1wS",
    "papermill": {
     "duration": 3.896167,
     "end_time": "2021-11-28T02:34:17.143096",
     "exception": false,
     "start_time": "2021-11-28T02:34:13.246929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")   #\"cuda:0\",cpu\n",
    "model = DenseNet121(\n",
    "    spatial_dims=2,            \n",
    "    in_channels=1,\n",
    "    out_channels=num_class,\n",
    ").to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
    "epoch_num = 10    ###10\n",
    "val_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4779e1f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T02:34:17.218032Z",
     "iopub.status.busy": "2021-11-28T02:34:17.208530Z",
     "iopub.status.idle": "2021-11-28T03:02:34.749666Z",
     "shell.execute_reply": "2021-11-28T03:02:34.750275Z"
    },
    "id": "MpKhLo3YDpXw",
    "papermill": {
     "duration": 1697.578702,
     "end_time": "2021-11-28T03:02:34.750521",
     "exception": false,
     "start_time": "2021-11-28T02:34:17.171819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/10\n",
      "1/746, train_loss: 6.0603\n",
      "2/746, train_loss: 6.1291\n",
      "3/746, train_loss: 5.9983\n",
      "4/746, train_loss: 6.0627\n",
      "5/746, train_loss: 6.0921\n",
      "6/746, train_loss: 6.1052\n",
      "7/746, train_loss: 6.0104\n",
      "8/746, train_loss: 5.9854\n",
      "9/746, train_loss: 6.0830\n",
      "10/746, train_loss: 6.0876\n",
      "11/746, train_loss: 6.0204\n",
      "12/746, train_loss: 5.9963\n",
      "13/746, train_loss: 6.0077\n",
      "14/746, train_loss: 6.0821\n",
      "15/746, train_loss: 6.0268\n",
      "16/746, train_loss: 6.0383\n",
      "17/746, train_loss: 6.0224\n",
      "18/746, train_loss: 6.1084\n",
      "19/746, train_loss: 5.9377\n",
      "20/746, train_loss: 6.1153\n",
      "21/746, train_loss: 6.0469\n",
      "22/746, train_loss: 5.9970\n",
      "23/746, train_loss: 6.0822\n",
      "24/746, train_loss: 6.0901\n",
      "25/746, train_loss: 6.0556\n",
      "26/746, train_loss: 5.9842\n",
      "27/746, train_loss: 6.1187\n",
      "28/746, train_loss: 6.0213\n",
      "29/746, train_loss: 6.0626\n",
      "30/746, train_loss: 5.9831\n",
      "31/746, train_loss: 5.9247\n",
      "32/746, train_loss: 5.9630\n",
      "33/746, train_loss: 6.0433\n",
      "34/746, train_loss: 5.9952\n",
      "35/746, train_loss: 5.9545\n",
      "36/746, train_loss: 5.9767\n",
      "37/746, train_loss: 6.0332\n",
      "38/746, train_loss: 6.0328\n",
      "39/746, train_loss: 6.1498\n",
      "40/746, train_loss: 6.0647\n",
      "41/746, train_loss: 5.9876\n",
      "42/746, train_loss: 5.9773\n",
      "43/746, train_loss: 6.0715\n",
      "44/746, train_loss: 6.0996\n",
      "45/746, train_loss: 5.9887\n",
      "46/746, train_loss: 6.0790\n",
      "47/746, train_loss: 6.1668\n",
      "48/746, train_loss: 6.0433\n",
      "49/746, train_loss: 5.9598\n",
      "50/746, train_loss: 5.9667\n",
      "51/746, train_loss: 5.9406\n",
      "52/746, train_loss: 5.9749\n",
      "53/746, train_loss: 6.0086\n",
      "54/746, train_loss: 6.0651\n",
      "55/746, train_loss: 5.9111\n",
      "56/746, train_loss: 6.0460\n",
      "57/746, train_loss: 5.9759\n",
      "58/746, train_loss: 5.9828\n",
      "59/746, train_loss: 5.9757\n",
      "60/746, train_loss: 6.0695\n",
      "61/746, train_loss: 5.9701\n",
      "62/746, train_loss: 6.0587\n",
      "63/746, train_loss: 6.0219\n",
      "64/746, train_loss: 5.9942\n",
      "65/746, train_loss: 6.0249\n",
      "66/746, train_loss: 6.0741\n",
      "67/746, train_loss: 6.0423\n",
      "68/746, train_loss: 6.0814\n",
      "69/746, train_loss: 6.1260\n",
      "70/746, train_loss: 6.0997\n",
      "71/746, train_loss: 5.9818\n",
      "72/746, train_loss: 6.0937\n",
      "73/746, train_loss: 6.0245\n",
      "74/746, train_loss: 5.9880\n",
      "75/746, train_loss: 6.1123\n",
      "76/746, train_loss: 5.9260\n",
      "77/746, train_loss: 6.1250\n",
      "78/746, train_loss: 5.9608\n",
      "79/746, train_loss: 5.9609\n",
      "80/746, train_loss: 5.9905\n",
      "81/746, train_loss: 5.9859\n",
      "82/746, train_loss: 5.9804\n",
      "83/746, train_loss: 6.0158\n",
      "84/746, train_loss: 6.0040\n",
      "85/746, train_loss: 5.9360\n",
      "86/746, train_loss: 6.0052\n",
      "87/746, train_loss: 5.9884\n",
      "88/746, train_loss: 6.0986\n",
      "89/746, train_loss: 5.9820\n",
      "90/746, train_loss: 6.0119\n",
      "91/746, train_loss: 6.0773\n",
      "92/746, train_loss: 5.9185\n",
      "93/746, train_loss: 6.0580\n",
      "94/746, train_loss: 6.0213\n",
      "95/746, train_loss: 6.0482\n",
      "96/746, train_loss: 5.9903\n",
      "97/746, train_loss: 5.9728\n",
      "98/746, train_loss: 5.9732\n",
      "99/746, train_loss: 5.9828\n",
      "100/746, train_loss: 6.0862\n",
      "101/746, train_loss: 6.0635\n",
      "102/746, train_loss: 5.9581\n",
      "103/746, train_loss: 5.9765\n",
      "104/746, train_loss: 5.9800\n",
      "105/746, train_loss: 5.9556\n",
      "106/746, train_loss: 5.9447\n",
      "107/746, train_loss: 6.0372\n",
      "108/746, train_loss: 5.9405\n",
      "109/746, train_loss: 5.9882\n",
      "110/746, train_loss: 6.0207\n",
      "111/746, train_loss: 6.0293\n",
      "112/746, train_loss: 5.9880\n",
      "113/746, train_loss: 6.0411\n",
      "114/746, train_loss: 5.9860\n",
      "115/746, train_loss: 5.9476\n",
      "116/746, train_loss: 5.9577\n",
      "117/746, train_loss: 5.9529\n",
      "118/746, train_loss: 6.0456\n",
      "119/746, train_loss: 6.0500\n",
      "120/746, train_loss: 6.0126\n",
      "121/746, train_loss: 5.9359\n",
      "122/746, train_loss: 5.9217\n",
      "123/746, train_loss: 5.8904\n",
      "124/746, train_loss: 6.0236\n",
      "125/746, train_loss: 6.0115\n",
      "126/746, train_loss: 5.9543\n",
      "127/746, train_loss: 5.9632\n",
      "128/746, train_loss: 5.9390\n",
      "129/746, train_loss: 5.9830\n",
      "130/746, train_loss: 5.9634\n",
      "131/746, train_loss: 6.0448\n",
      "132/746, train_loss: 5.9725\n",
      "133/746, train_loss: 5.9203\n",
      "134/746, train_loss: 5.9540\n",
      "135/746, train_loss: 5.9935\n",
      "136/746, train_loss: 6.0931\n",
      "137/746, train_loss: 6.0896\n",
      "138/746, train_loss: 5.9703\n",
      "139/746, train_loss: 5.9547\n",
      "140/746, train_loss: 5.9578\n",
      "141/746, train_loss: 5.9644\n",
      "142/746, train_loss: 5.9076\n",
      "143/746, train_loss: 6.0284\n",
      "144/746, train_loss: 6.0931\n",
      "145/746, train_loss: 5.9810\n",
      "146/746, train_loss: 6.0196\n",
      "147/746, train_loss: 5.9264\n",
      "148/746, train_loss: 5.9783\n",
      "149/746, train_loss: 5.9591\n",
      "150/746, train_loss: 6.0377\n",
      "151/746, train_loss: 5.9335\n",
      "152/746, train_loss: 5.8653\n",
      "153/746, train_loss: 6.0200\n",
      "154/746, train_loss: 5.8820\n",
      "155/746, train_loss: 6.0375\n",
      "156/746, train_loss: 5.9866\n",
      "157/746, train_loss: 6.0131\n",
      "158/746, train_loss: 5.9655\n",
      "159/746, train_loss: 5.9853\n",
      "160/746, train_loss: 6.0096\n",
      "161/746, train_loss: 6.0433\n",
      "162/746, train_loss: 5.9184\n",
      "163/746, train_loss: 5.9611\n",
      "164/746, train_loss: 6.0178\n",
      "165/746, train_loss: 6.0143\n",
      "166/746, train_loss: 5.9152\n",
      "167/746, train_loss: 5.9649\n",
      "168/746, train_loss: 5.9215\n",
      "169/746, train_loss: 6.0810\n",
      "170/746, train_loss: 5.9859\n",
      "171/746, train_loss: 5.8774\n",
      "172/746, train_loss: 6.0769\n",
      "173/746, train_loss: 5.9571\n",
      "174/746, train_loss: 6.0045\n",
      "175/746, train_loss: 6.0193\n",
      "176/746, train_loss: 5.9614\n",
      "177/746, train_loss: 5.9904\n",
      "178/746, train_loss: 5.9806\n",
      "179/746, train_loss: 5.9770\n",
      "180/746, train_loss: 6.0436\n",
      "181/746, train_loss: 5.9335\n",
      "182/746, train_loss: 5.9742\n",
      "183/746, train_loss: 5.9518\n",
      "184/746, train_loss: 5.9777\n",
      "185/746, train_loss: 5.9419\n",
      "186/746, train_loss: 5.9519\n",
      "187/746, train_loss: 6.0269\n",
      "188/746, train_loss: 5.9541\n",
      "189/746, train_loss: 5.9139\n",
      "190/746, train_loss: 5.9935\n",
      "191/746, train_loss: 5.9529\n",
      "192/746, train_loss: 5.9841\n",
      "193/746, train_loss: 5.9661\n",
      "194/746, train_loss: 5.9498\n",
      "195/746, train_loss: 6.0641\n",
      "196/746, train_loss: 5.9732\n",
      "197/746, train_loss: 5.9153\n",
      "198/746, train_loss: 5.9866\n",
      "199/746, train_loss: 5.8951\n",
      "200/746, train_loss: 5.9563\n",
      "201/746, train_loss: 5.9556\n",
      "202/746, train_loss: 5.9538\n",
      "203/746, train_loss: 6.0007\n",
      "204/746, train_loss: 5.9165\n",
      "205/746, train_loss: 6.0085\n",
      "206/746, train_loss: 5.8773\n",
      "207/746, train_loss: 5.9772\n",
      "208/746, train_loss: 5.9716\n",
      "209/746, train_loss: 6.0990\n",
      "210/746, train_loss: 5.9572\n",
      "211/746, train_loss: 5.9625\n",
      "212/746, train_loss: 5.9147\n",
      "213/746, train_loss: 5.8953\n",
      "214/746, train_loss: 5.8604\n",
      "215/746, train_loss: 5.9436\n",
      "216/746, train_loss: 5.9891\n",
      "217/746, train_loss: 5.9780\n",
      "218/746, train_loss: 5.9475\n",
      "219/746, train_loss: 5.9812\n",
      "220/746, train_loss: 5.9332\n",
      "221/746, train_loss: 5.9513\n",
      "222/746, train_loss: 5.9039\n",
      "223/746, train_loss: 5.9387\n",
      "224/746, train_loss: 5.9167\n",
      "225/746, train_loss: 5.8648\n",
      "226/746, train_loss: 5.9605\n",
      "227/746, train_loss: 6.0338\n",
      "228/746, train_loss: 5.9373\n",
      "229/746, train_loss: 5.9448\n",
      "230/746, train_loss: 5.9559\n",
      "231/746, train_loss: 5.9615\n",
      "232/746, train_loss: 5.9767\n",
      "233/746, train_loss: 6.0041\n",
      "234/746, train_loss: 5.9084\n",
      "235/746, train_loss: 5.9620\n",
      "236/746, train_loss: 5.9471\n",
      "237/746, train_loss: 5.9917\n",
      "238/746, train_loss: 5.9855\n",
      "239/746, train_loss: 5.9798\n",
      "240/746, train_loss: 5.9493\n",
      "241/746, train_loss: 5.9495\n",
      "242/746, train_loss: 5.9225\n",
      "243/746, train_loss: 5.9262\n",
      "244/746, train_loss: 5.9733\n",
      "245/746, train_loss: 5.8205\n",
      "246/746, train_loss: 5.9673\n",
      "247/746, train_loss: 5.9256\n",
      "248/746, train_loss: 5.9367\n",
      "249/746, train_loss: 5.8940\n",
      "250/746, train_loss: 5.9405\n",
      "251/746, train_loss: 5.9338\n",
      "252/746, train_loss: 5.8442\n",
      "253/746, train_loss: 5.9650\n",
      "254/746, train_loss: 5.9824\n",
      "255/746, train_loss: 5.9564\n",
      "256/746, train_loss: 5.9899\n",
      "257/746, train_loss: 5.9871\n",
      "258/746, train_loss: 5.9763\n",
      "259/746, train_loss: 5.8948\n",
      "260/746, train_loss: 5.9562\n",
      "261/746, train_loss: 5.9622\n",
      "262/746, train_loss: 5.9811\n",
      "263/746, train_loss: 6.0100\n",
      "264/746, train_loss: 5.9241\n",
      "265/746, train_loss: 5.9111\n",
      "266/746, train_loss: 5.9123\n",
      "267/746, train_loss: 5.9272\n",
      "268/746, train_loss: 6.0033\n",
      "269/746, train_loss: 5.8945\n",
      "270/746, train_loss: 5.9037\n",
      "271/746, train_loss: 5.9362\n",
      "272/746, train_loss: 5.8785\n",
      "273/746, train_loss: 5.9657\n",
      "274/746, train_loss: 5.8532\n",
      "275/746, train_loss: 5.9810\n",
      "276/746, train_loss: 5.9104\n",
      "277/746, train_loss: 5.9536\n",
      "278/746, train_loss: 5.8861\n",
      "279/746, train_loss: 5.9407\n",
      "280/746, train_loss: 6.0549\n",
      "281/746, train_loss: 5.9503\n",
      "282/746, train_loss: 5.8545\n",
      "283/746, train_loss: 5.9160\n",
      "284/746, train_loss: 5.9849\n",
      "285/746, train_loss: 6.0216\n",
      "286/746, train_loss: 5.8893\n",
      "287/746, train_loss: 5.9967\n",
      "288/746, train_loss: 5.9662\n",
      "289/746, train_loss: 5.9077\n",
      "290/746, train_loss: 5.9314\n",
      "291/746, train_loss: 5.9808\n",
      "292/746, train_loss: 5.9061\n",
      "293/746, train_loss: 5.9214\n",
      "294/746, train_loss: 5.9491\n",
      "295/746, train_loss: 5.9073\n",
      "296/746, train_loss: 5.9003\n",
      "297/746, train_loss: 5.9312\n",
      "298/746, train_loss: 5.9932\n",
      "299/746, train_loss: 5.9312\n",
      "300/746, train_loss: 5.8417\n",
      "301/746, train_loss: 5.9039\n",
      "302/746, train_loss: 6.0240\n",
      "303/746, train_loss: 6.0276\n",
      "304/746, train_loss: 5.8840\n",
      "305/746, train_loss: 5.8505\n",
      "306/746, train_loss: 5.9497\n",
      "307/746, train_loss: 5.9671\n",
      "308/746, train_loss: 5.9279\n",
      "309/746, train_loss: 5.8894\n",
      "310/746, train_loss: 5.9415\n",
      "311/746, train_loss: 5.8912\n",
      "312/746, train_loss: 5.8959\n",
      "313/746, train_loss: 5.9918\n",
      "314/746, train_loss: 5.8270\n",
      "315/746, train_loss: 5.9350\n",
      "316/746, train_loss: 6.0024\n",
      "317/746, train_loss: 5.9846\n",
      "318/746, train_loss: 5.7810\n",
      "319/746, train_loss: 5.9451\n",
      "320/746, train_loss: 5.8589\n",
      "321/746, train_loss: 5.8932\n",
      "322/746, train_loss: 5.8727\n",
      "323/746, train_loss: 5.9062\n",
      "324/746, train_loss: 5.8799\n",
      "325/746, train_loss: 5.9374\n",
      "326/746, train_loss: 5.9763\n",
      "327/746, train_loss: 5.9385\n",
      "328/746, train_loss: 5.8648\n",
      "329/746, train_loss: 5.8873\n",
      "330/746, train_loss: 5.7480\n",
      "331/746, train_loss: 6.0071\n",
      "332/746, train_loss: 6.0055\n",
      "333/746, train_loss: 5.9083\n",
      "334/746, train_loss: 5.8747\n",
      "335/746, train_loss: 5.9620\n",
      "336/746, train_loss: 5.8836\n",
      "337/746, train_loss: 5.9418\n",
      "338/746, train_loss: 5.8754\n",
      "339/746, train_loss: 5.8491\n",
      "340/746, train_loss: 5.8903\n",
      "341/746, train_loss: 5.8781\n",
      "342/746, train_loss: 5.9881\n",
      "343/746, train_loss: 5.9810\n",
      "344/746, train_loss: 5.9601\n",
      "345/746, train_loss: 5.9425\n",
      "346/746, train_loss: 5.8749\n",
      "347/746, train_loss: 5.8864\n",
      "348/746, train_loss: 5.8869\n",
      "349/746, train_loss: 5.9159\n",
      "350/746, train_loss: 5.8986\n",
      "351/746, train_loss: 5.8648\n",
      "352/746, train_loss: 5.8601\n",
      "353/746, train_loss: 5.8492\n",
      "354/746, train_loss: 5.9214\n",
      "355/746, train_loss: 5.8334\n",
      "356/746, train_loss: 5.8951\n",
      "357/746, train_loss: 5.9120\n",
      "358/746, train_loss: 5.8279\n",
      "359/746, train_loss: 5.8824\n",
      "360/746, train_loss: 5.9485\n",
      "361/746, train_loss: 5.8221\n",
      "362/746, train_loss: 5.8780\n",
      "363/746, train_loss: 5.9367\n",
      "364/746, train_loss: 5.8744\n",
      "365/746, train_loss: 5.9590\n",
      "366/746, train_loss: 5.8099\n",
      "367/746, train_loss: 5.9094\n",
      "368/746, train_loss: 5.9092\n",
      "369/746, train_loss: 5.8389\n",
      "370/746, train_loss: 5.8886\n",
      "371/746, train_loss: 5.8905\n",
      "372/746, train_loss: 5.8420\n",
      "373/746, train_loss: 5.8347\n",
      "374/746, train_loss: 5.9350\n",
      "375/746, train_loss: 5.9329\n",
      "376/746, train_loss: 5.9098\n",
      "377/746, train_loss: 5.8234\n",
      "378/746, train_loss: 5.8040\n",
      "379/746, train_loss: 5.9446\n",
      "380/746, train_loss: 5.8710\n",
      "381/746, train_loss: 5.9103\n",
      "382/746, train_loss: 5.8762\n",
      "383/746, train_loss: 5.8811\n",
      "384/746, train_loss: 5.8050\n",
      "385/746, train_loss: 5.9073\n",
      "386/746, train_loss: 5.8875\n",
      "387/746, train_loss: 5.8927\n",
      "388/746, train_loss: 5.8653\n",
      "389/746, train_loss: 5.8621\n",
      "390/746, train_loss: 5.8849\n",
      "391/746, train_loss: 5.9048\n",
      "392/746, train_loss: 5.8514\n",
      "393/746, train_loss: 5.9158\n",
      "394/746, train_loss: 5.8604\n",
      "395/746, train_loss: 5.9053\n",
      "396/746, train_loss: 5.8524\n",
      "397/746, train_loss: 5.9479\n",
      "398/746, train_loss: 5.8813\n",
      "399/746, train_loss: 5.8641\n",
      "400/746, train_loss: 5.9127\n",
      "401/746, train_loss: 5.8566\n",
      "402/746, train_loss: 5.8885\n",
      "403/746, train_loss: 5.8253\n",
      "404/746, train_loss: 5.9269\n",
      "405/746, train_loss: 5.9120\n",
      "406/746, train_loss: 5.8640\n",
      "407/746, train_loss: 5.9490\n",
      "408/746, train_loss: 5.8331\n",
      "409/746, train_loss: 5.8820\n",
      "410/746, train_loss: 5.8745\n",
      "411/746, train_loss: 5.8649\n",
      "412/746, train_loss: 5.9193\n",
      "413/746, train_loss: 5.9399\n",
      "414/746, train_loss: 5.8788\n",
      "415/746, train_loss: 5.9578\n",
      "416/746, train_loss: 5.8368\n",
      "417/746, train_loss: 5.9058\n",
      "418/746, train_loss: 5.9175\n",
      "419/746, train_loss: 5.8498\n",
      "420/746, train_loss: 5.8467\n",
      "421/746, train_loss: 5.8895\n",
      "422/746, train_loss: 5.9088\n",
      "423/746, train_loss: 5.8432\n",
      "424/746, train_loss: 5.9109\n",
      "425/746, train_loss: 5.9016\n",
      "426/746, train_loss: 5.9506\n",
      "427/746, train_loss: 5.9005\n",
      "428/746, train_loss: 5.8862\n",
      "429/746, train_loss: 5.9870\n",
      "430/746, train_loss: 5.8494\n",
      "431/746, train_loss: 5.8773\n",
      "432/746, train_loss: 5.9702\n",
      "433/746, train_loss: 5.8608\n",
      "434/746, train_loss: 5.8024\n",
      "435/746, train_loss: 5.8430\n",
      "436/746, train_loss: 5.9221\n",
      "437/746, train_loss: 5.8355\n",
      "438/746, train_loss: 5.9427\n",
      "439/746, train_loss: 5.8631\n",
      "440/746, train_loss: 5.9534\n",
      "441/746, train_loss: 5.8226\n",
      "442/746, train_loss: 5.8184\n",
      "443/746, train_loss: 5.8797\n",
      "444/746, train_loss: 5.8332\n",
      "445/746, train_loss: 5.9250\n",
      "446/746, train_loss: 5.7535\n",
      "447/746, train_loss: 5.9025\n",
      "448/746, train_loss: 5.9392\n",
      "449/746, train_loss: 5.9566\n",
      "450/746, train_loss: 5.8839\n",
      "451/746, train_loss: 5.8771\n",
      "452/746, train_loss: 5.8973\n",
      "453/746, train_loss: 5.8952\n",
      "454/746, train_loss: 5.8954\n",
      "455/746, train_loss: 5.8020\n",
      "456/746, train_loss: 5.8821\n",
      "457/746, train_loss: 5.8904\n",
      "458/746, train_loss: 5.8934\n",
      "459/746, train_loss: 5.9074\n",
      "460/746, train_loss: 5.8741\n",
      "461/746, train_loss: 5.8900\n",
      "462/746, train_loss: 5.7322\n",
      "463/746, train_loss: 5.8946\n",
      "464/746, train_loss: 5.9431\n",
      "465/746, train_loss: 5.9337\n",
      "466/746, train_loss: 5.8488\n",
      "467/746, train_loss: 5.8953\n",
      "468/746, train_loss: 5.8559\n",
      "469/746, train_loss: 5.8684\n",
      "470/746, train_loss: 5.9068\n",
      "471/746, train_loss: 5.9335\n",
      "472/746, train_loss: 5.9289\n",
      "473/746, train_loss: 5.8369\n",
      "474/746, train_loss: 5.7907\n",
      "475/746, train_loss: 5.8636\n",
      "476/746, train_loss: 5.9090\n",
      "477/746, train_loss: 5.8261\n",
      "478/746, train_loss: 5.9469\n",
      "479/746, train_loss: 5.9559\n",
      "480/746, train_loss: 5.8618\n",
      "481/746, train_loss: 5.8782\n",
      "482/746, train_loss: 5.9022\n",
      "483/746, train_loss: 5.7723\n",
      "484/746, train_loss: 5.8816\n",
      "485/746, train_loss: 5.7988\n",
      "486/746, train_loss: 5.9435\n",
      "487/746, train_loss: 5.8873\n",
      "488/746, train_loss: 5.9694\n",
      "489/746, train_loss: 5.8929\n",
      "490/746, train_loss: 5.8183\n",
      "491/746, train_loss: 5.8110\n",
      "492/746, train_loss: 5.8897\n",
      "493/746, train_loss: 5.7678\n",
      "494/746, train_loss: 5.8723\n",
      "495/746, train_loss: 5.9183\n",
      "496/746, train_loss: 5.8656\n",
      "497/746, train_loss: 5.9343\n",
      "498/746, train_loss: 5.9806\n",
      "499/746, train_loss: 5.9052\n",
      "500/746, train_loss: 5.8663\n",
      "501/746, train_loss: 5.8632\n",
      "502/746, train_loss: 5.8103\n",
      "503/746, train_loss: 5.8489\n",
      "504/746, train_loss: 5.8329\n",
      "505/746, train_loss: 5.7586\n",
      "506/746, train_loss: 5.8768\n",
      "507/746, train_loss: 5.9172\n",
      "508/746, train_loss: 5.8699\n",
      "509/746, train_loss: 5.8193\n",
      "510/746, train_loss: 5.7621\n",
      "511/746, train_loss: 5.8676\n",
      "512/746, train_loss: 5.8797\n",
      "513/746, train_loss: 5.8565\n",
      "514/746, train_loss: 5.8496\n",
      "515/746, train_loss: 5.9230\n",
      "516/746, train_loss: 5.8960\n",
      "517/746, train_loss: 5.8502\n",
      "518/746, train_loss: 5.8760\n",
      "519/746, train_loss: 5.9290\n",
      "520/746, train_loss: 5.8846\n",
      "521/746, train_loss: 5.8010\n",
      "522/746, train_loss: 5.8685\n",
      "523/746, train_loss: 5.8911\n",
      "524/746, train_loss: 5.8521\n",
      "525/746, train_loss: 5.9018\n",
      "526/746, train_loss: 5.9033\n",
      "527/746, train_loss: 5.7878\n",
      "528/746, train_loss: 5.8915\n",
      "529/746, train_loss: 5.8535\n",
      "530/746, train_loss: 5.9549\n",
      "531/746, train_loss: 5.8011\n",
      "532/746, train_loss: 5.8395\n",
      "533/746, train_loss: 5.9518\n",
      "534/746, train_loss: 5.7960\n",
      "535/746, train_loss: 5.9528\n",
      "536/746, train_loss: 5.8846\n",
      "537/746, train_loss: 5.7848\n",
      "538/746, train_loss: 5.9802\n",
      "539/746, train_loss: 5.8773\n",
      "540/746, train_loss: 5.8309\n",
      "541/746, train_loss: 5.7795\n",
      "542/746, train_loss: 5.8319\n",
      "543/746, train_loss: 5.8533\n",
      "544/746, train_loss: 5.8563\n",
      "545/746, train_loss: 5.8791\n",
      "546/746, train_loss: 5.8879\n",
      "547/746, train_loss: 5.8285\n",
      "548/746, train_loss: 5.8963\n",
      "549/746, train_loss: 5.8891\n",
      "550/746, train_loss: 5.9114\n",
      "551/746, train_loss: 5.8509\n",
      "552/746, train_loss: 5.8586\n",
      "553/746, train_loss: 5.8222\n",
      "554/746, train_loss: 5.8356\n",
      "555/746, train_loss: 5.8157\n",
      "556/746, train_loss: 5.8793\n",
      "557/746, train_loss: 5.8044\n",
      "558/746, train_loss: 5.7977\n",
      "559/746, train_loss: 5.7834\n",
      "560/746, train_loss: 5.8295\n",
      "561/746, train_loss: 5.8586\n",
      "562/746, train_loss: 5.8054\n",
      "563/746, train_loss: 5.8460\n",
      "564/746, train_loss: 5.8724\n",
      "565/746, train_loss: 5.8756\n",
      "566/746, train_loss: 5.8382\n",
      "567/746, train_loss: 5.8613\n",
      "568/746, train_loss: 5.8433\n",
      "569/746, train_loss: 5.8484\n",
      "570/746, train_loss: 5.7606\n",
      "571/746, train_loss: 5.8131\n",
      "572/746, train_loss: 5.9316\n",
      "573/746, train_loss: 5.8780\n",
      "574/746, train_loss: 5.8553\n",
      "575/746, train_loss: 5.8259\n",
      "576/746, train_loss: 5.8640\n",
      "577/746, train_loss: 5.7961\n",
      "578/746, train_loss: 5.8760\n",
      "579/746, train_loss: 5.8834\n",
      "580/746, train_loss: 5.8345\n",
      "581/746, train_loss: 5.8592\n",
      "582/746, train_loss: 5.8904\n",
      "583/746, train_loss: 5.8240\n",
      "584/746, train_loss: 5.8117\n",
      "585/746, train_loss: 5.7182\n",
      "586/746, train_loss: 5.7740\n",
      "587/746, train_loss: 5.8359\n",
      "588/746, train_loss: 5.7532\n",
      "589/746, train_loss: 5.8132\n",
      "590/746, train_loss: 5.8814\n",
      "591/746, train_loss: 5.8919\n",
      "592/746, train_loss: 5.8207\n",
      "593/746, train_loss: 5.8121\n",
      "594/746, train_loss: 5.8301\n",
      "595/746, train_loss: 5.8611\n",
      "596/746, train_loss: 5.7951\n",
      "597/746, train_loss: 5.8512\n",
      "598/746, train_loss: 5.8641\n",
      "599/746, train_loss: 5.7495\n",
      "600/746, train_loss: 5.8291\n",
      "601/746, train_loss: 5.8251\n",
      "602/746, train_loss: 5.8158\n",
      "603/746, train_loss: 5.7436\n",
      "604/746, train_loss: 5.9254\n",
      "605/746, train_loss: 5.7642\n",
      "606/746, train_loss: 5.7879\n",
      "607/746, train_loss: 5.8201\n",
      "608/746, train_loss: 5.7539\n",
      "609/746, train_loss: 5.8959\n",
      "610/746, train_loss: 5.8501\n",
      "611/746, train_loss: 5.7784\n",
      "612/746, train_loss: 5.6916\n",
      "613/746, train_loss: 5.8935\n",
      "614/746, train_loss: 5.8766\n",
      "615/746, train_loss: 5.8181\n",
      "616/746, train_loss: 5.7824\n",
      "617/746, train_loss: 5.8200\n",
      "618/746, train_loss: 5.8082\n",
      "619/746, train_loss: 5.8732\n",
      "620/746, train_loss: 5.8727\n",
      "621/746, train_loss: 5.7796\n",
      "622/746, train_loss: 5.8565\n",
      "623/746, train_loss: 5.8031\n",
      "624/746, train_loss: 5.7854\n",
      "625/746, train_loss: 5.8755\n",
      "626/746, train_loss: 5.8107\n",
      "627/746, train_loss: 5.8329\n",
      "628/746, train_loss: 5.8502\n",
      "629/746, train_loss: 5.8196\n",
      "630/746, train_loss: 5.8229\n",
      "631/746, train_loss: 5.8245\n",
      "632/746, train_loss: 5.8977\n",
      "633/746, train_loss: 5.8290\n",
      "634/746, train_loss: 5.8552\n",
      "635/746, train_loss: 5.8663\n",
      "636/746, train_loss: 5.8362\n",
      "637/746, train_loss: 5.8387\n",
      "638/746, train_loss: 5.7516\n",
      "639/746, train_loss: 5.8387\n",
      "640/746, train_loss: 5.8031\n",
      "641/746, train_loss: 5.8685\n",
      "642/746, train_loss: 5.7306\n",
      "643/746, train_loss: 5.8476\n",
      "644/746, train_loss: 5.8640\n",
      "645/746, train_loss: 5.7840\n",
      "646/746, train_loss: 5.9017\n",
      "647/746, train_loss: 5.8527\n",
      "648/746, train_loss: 5.8487\n",
      "649/746, train_loss: 5.8383\n",
      "650/746, train_loss: 5.7568\n",
      "651/746, train_loss: 5.7909\n",
      "652/746, train_loss: 5.8579\n",
      "653/746, train_loss: 5.8622\n",
      "654/746, train_loss: 5.7603\n",
      "655/746, train_loss: 5.8568\n",
      "656/746, train_loss: 5.7797\n",
      "657/746, train_loss: 5.8041\n",
      "658/746, train_loss: 5.7819\n",
      "659/746, train_loss: 5.8597\n",
      "660/746, train_loss: 5.7623\n",
      "661/746, train_loss: 5.8476\n",
      "662/746, train_loss: 5.7551\n",
      "663/746, train_loss: 5.7122\n",
      "664/746, train_loss: 5.7770\n",
      "665/746, train_loss: 5.8573\n",
      "666/746, train_loss: 5.8212\n",
      "667/746, train_loss: 5.8169\n",
      "668/746, train_loss: 5.8125\n",
      "669/746, train_loss: 5.8205\n",
      "670/746, train_loss: 5.8148\n",
      "671/746, train_loss: 5.8009\n",
      "672/746, train_loss: 5.7726\n",
      "673/746, train_loss: 5.8065\n",
      "674/746, train_loss: 5.8041\n",
      "675/746, train_loss: 5.8219\n",
      "676/746, train_loss: 5.8616\n",
      "677/746, train_loss: 5.7657\n",
      "678/746, train_loss: 5.8829\n",
      "679/746, train_loss: 5.7841\n",
      "680/746, train_loss: 5.7196\n",
      "681/746, train_loss: 5.8277\n",
      "682/746, train_loss: 5.8190\n",
      "683/746, train_loss: 5.8017\n",
      "684/746, train_loss: 5.8723\n",
      "685/746, train_loss: 5.6874\n",
      "686/746, train_loss: 5.8476\n",
      "687/746, train_loss: 5.7626\n",
      "688/746, train_loss: 5.7202\n",
      "689/746, train_loss: 5.7243\n",
      "690/746, train_loss: 5.8409\n",
      "691/746, train_loss: 5.7617\n",
      "692/746, train_loss: 5.8449\n",
      "693/746, train_loss: 5.7945\n",
      "694/746, train_loss: 5.7813\n",
      "695/746, train_loss: 5.8227\n",
      "696/746, train_loss: 5.8454\n",
      "697/746, train_loss: 5.7920\n",
      "698/746, train_loss: 5.7410\n",
      "699/746, train_loss: 5.7859\n",
      "700/746, train_loss: 5.8266\n",
      "701/746, train_loss: 5.7066\n",
      "702/746, train_loss: 5.7074\n",
      "703/746, train_loss: 5.8197\n",
      "704/746, train_loss: 5.8012\n",
      "705/746, train_loss: 5.7613\n",
      "706/746, train_loss: 5.7920\n",
      "707/746, train_loss: 5.8210\n",
      "708/746, train_loss: 5.7764\n",
      "709/746, train_loss: 5.8190\n",
      "710/746, train_loss: 5.7893\n",
      "711/746, train_loss: 5.7996\n",
      "712/746, train_loss: 5.8239\n",
      "713/746, train_loss: 5.7804\n",
      "714/746, train_loss: 5.7567\n",
      "715/746, train_loss: 5.7714\n",
      "716/746, train_loss: 5.8493\n",
      "717/746, train_loss: 5.8288\n",
      "718/746, train_loss: 5.8213\n",
      "719/746, train_loss: 5.7997\n",
      "720/746, train_loss: 5.8523\n",
      "721/746, train_loss: 5.7440\n",
      "722/746, train_loss: 5.8674\n",
      "723/746, train_loss: 5.8021\n",
      "724/746, train_loss: 5.8340\n",
      "725/746, train_loss: 5.6970\n",
      "726/746, train_loss: 5.7183\n",
      "727/746, train_loss: 5.8080\n",
      "728/746, train_loss: 5.8076\n",
      "729/746, train_loss: 5.7715\n",
      "730/746, train_loss: 5.8502\n",
      "731/746, train_loss: 5.7853\n",
      "732/746, train_loss: 5.8070\n",
      "733/746, train_loss: 5.7719\n",
      "734/746, train_loss: 5.8196\n",
      "735/746, train_loss: 5.7655\n",
      "736/746, train_loss: 5.7943\n",
      "737/746, train_loss: 5.7506\n",
      "738/746, train_loss: 5.7707\n",
      "739/746, train_loss: 5.7940\n",
      "740/746, train_loss: 5.8014\n",
      "741/746, train_loss: 5.7807\n",
      "742/746, train_loss: 5.7421\n",
      "743/746, train_loss: 5.8119\n",
      "744/746, train_loss: 5.7514\n",
      "745/746, train_loss: 5.8177\n",
      "746/746, train_loss: 5.7597\n",
      "747/746, train_loss: 5.7975\n",
      "epoch 1 average loss: 5.9057\n",
      "saved new best metric model\n",
      "current epoch: 1 current AUC: 0.6655 current accuracy: 0.0222 best AUC: 0.0222 at epoch: 1\n",
      "----------\n",
      "epoch 2/10\n",
      "1/746, train_loss: 5.6561\n",
      "2/746, train_loss: 5.7730\n",
      "3/746, train_loss: 5.7019\n",
      "4/746, train_loss: 5.7167\n",
      "5/746, train_loss: 5.7218\n",
      "6/746, train_loss: 5.7837\n",
      "7/746, train_loss: 5.6699\n",
      "8/746, train_loss: 5.7465\n",
      "9/746, train_loss: 5.7137\n",
      "10/746, train_loss: 5.7280\n",
      "11/746, train_loss: 5.7661\n",
      "12/746, train_loss: 5.6781\n",
      "13/746, train_loss: 5.7344\n",
      "14/746, train_loss: 5.7152\n",
      "15/746, train_loss: 5.7188\n",
      "16/746, train_loss: 5.7605\n",
      "17/746, train_loss: 5.8145\n",
      "18/746, train_loss: 5.6781\n",
      "19/746, train_loss: 5.7551\n",
      "20/746, train_loss: 5.7564\n",
      "21/746, train_loss: 5.7468\n",
      "22/746, train_loss: 5.7973\n",
      "23/746, train_loss: 5.7544\n",
      "24/746, train_loss: 5.6701\n",
      "25/746, train_loss: 5.7118\n",
      "26/746, train_loss: 5.6922\n",
      "27/746, train_loss: 5.6541\n",
      "28/746, train_loss: 5.6806\n",
      "29/746, train_loss: 5.7306\n",
      "30/746, train_loss: 5.6404\n",
      "31/746, train_loss: 5.6033\n",
      "32/746, train_loss: 5.7275\n",
      "33/746, train_loss: 5.7253\n",
      "34/746, train_loss: 5.7004\n",
      "35/746, train_loss: 5.6979\n",
      "36/746, train_loss: 5.6717\n",
      "37/746, train_loss: 5.8261\n",
      "38/746, train_loss: 5.6138\n",
      "39/746, train_loss: 5.6891\n",
      "40/746, train_loss: 5.7197\n",
      "41/746, train_loss: 5.7346\n",
      "42/746, train_loss: 5.6090\n",
      "43/746, train_loss: 5.7962\n",
      "44/746, train_loss: 5.6759\n",
      "45/746, train_loss: 5.7375\n",
      "46/746, train_loss: 5.7266\n",
      "47/746, train_loss: 5.7251\n",
      "48/746, train_loss: 5.7550\n",
      "49/746, train_loss: 5.7489\n",
      "50/746, train_loss: 5.6941\n",
      "51/746, train_loss: 5.8454\n",
      "52/746, train_loss: 5.7142\n",
      "53/746, train_loss: 5.6730\n",
      "54/746, train_loss: 5.6931\n",
      "55/746, train_loss: 5.7539\n",
      "56/746, train_loss: 5.6424\n",
      "57/746, train_loss: 5.7980\n",
      "58/746, train_loss: 5.7135\n",
      "59/746, train_loss: 5.6905\n",
      "60/746, train_loss: 5.7687\n",
      "61/746, train_loss: 5.6305\n",
      "62/746, train_loss: 5.6909\n",
      "63/746, train_loss: 5.6471\n",
      "64/746, train_loss: 5.7584\n",
      "65/746, train_loss: 5.5904\n",
      "66/746, train_loss: 5.7019\n",
      "67/746, train_loss: 5.7699\n",
      "68/746, train_loss: 5.6734\n",
      "69/746, train_loss: 5.7408\n",
      "70/746, train_loss: 5.8063\n",
      "71/746, train_loss: 5.6689\n",
      "72/746, train_loss: 5.7540\n",
      "73/746, train_loss: 5.7318\n",
      "74/746, train_loss: 5.6663\n",
      "75/746, train_loss: 5.7061\n",
      "76/746, train_loss: 5.7224\n",
      "77/746, train_loss: 5.6484\n",
      "78/746, train_loss: 5.7767\n",
      "79/746, train_loss: 5.7058\n",
      "80/746, train_loss: 5.6513\n",
      "81/746, train_loss: 5.6916\n",
      "82/746, train_loss: 5.6658\n",
      "83/746, train_loss: 5.7147\n",
      "84/746, train_loss: 5.7582\n",
      "85/746, train_loss: 5.6720\n",
      "86/746, train_loss: 5.6896\n",
      "87/746, train_loss: 5.6819\n",
      "88/746, train_loss: 5.6450\n",
      "89/746, train_loss: 5.6257\n",
      "90/746, train_loss: 5.6566\n",
      "91/746, train_loss: 5.6487\n",
      "92/746, train_loss: 5.7401\n",
      "93/746, train_loss: 5.7281\n",
      "94/746, train_loss: 5.7190\n",
      "95/746, train_loss: 5.6643\n",
      "96/746, train_loss: 5.7361\n",
      "97/746, train_loss: 5.6617\n",
      "98/746, train_loss: 5.6350\n",
      "99/746, train_loss: 5.7051\n",
      "100/746, train_loss: 5.6476\n",
      "101/746, train_loss: 5.7594\n",
      "102/746, train_loss: 5.6460\n",
      "103/746, train_loss: 5.6687\n",
      "104/746, train_loss: 5.6765\n",
      "105/746, train_loss: 5.7326\n",
      "106/746, train_loss: 5.7011\n",
      "107/746, train_loss: 5.6862\n",
      "108/746, train_loss: 5.7557\n",
      "109/746, train_loss: 5.6686\n",
      "110/746, train_loss: 5.7409\n",
      "111/746, train_loss: 5.6303\n",
      "112/746, train_loss: 5.6872\n",
      "113/746, train_loss: 5.6884\n",
      "114/746, train_loss: 5.6599\n",
      "115/746, train_loss: 5.6414\n",
      "116/746, train_loss: 5.6607\n",
      "117/746, train_loss: 5.7501\n",
      "118/746, train_loss: 5.6573\n",
      "119/746, train_loss: 5.7864\n",
      "120/746, train_loss: 5.6016\n",
      "121/746, train_loss: 5.6721\n",
      "122/746, train_loss: 5.6723\n",
      "123/746, train_loss: 5.7370\n",
      "124/746, train_loss: 5.6927\n",
      "125/746, train_loss: 5.6947\n",
      "126/746, train_loss: 5.7572\n",
      "127/746, train_loss: 5.6884\n",
      "128/746, train_loss: 5.7152\n",
      "129/746, train_loss: 5.6473\n",
      "130/746, train_loss: 5.6772\n",
      "131/746, train_loss: 5.6920\n",
      "132/746, train_loss: 5.6449\n",
      "133/746, train_loss: 5.6898\n",
      "134/746, train_loss: 5.7231\n",
      "135/746, train_loss: 5.7135\n",
      "136/746, train_loss: 5.6678\n",
      "137/746, train_loss: 5.6801\n",
      "138/746, train_loss: 5.7294\n",
      "139/746, train_loss: 5.5974\n",
      "140/746, train_loss: 5.6264\n",
      "141/746, train_loss: 5.6672\n",
      "142/746, train_loss: 5.7040\n",
      "143/746, train_loss: 5.6784\n",
      "144/746, train_loss: 5.6051\n",
      "145/746, train_loss: 5.7378\n",
      "146/746, train_loss: 5.6239\n",
      "147/746, train_loss: 5.7707\n",
      "148/746, train_loss: 5.7525\n",
      "149/746, train_loss: 5.6319\n",
      "150/746, train_loss: 5.6717\n",
      "151/746, train_loss: 5.7359\n",
      "152/746, train_loss: 5.6301\n",
      "153/746, train_loss: 5.7782\n",
      "154/746, train_loss: 5.7470\n",
      "155/746, train_loss: 5.6117\n",
      "156/746, train_loss: 5.6041\n",
      "157/746, train_loss: 5.7130\n",
      "158/746, train_loss: 5.6599\n",
      "159/746, train_loss: 5.7034\n",
      "160/746, train_loss: 5.6946\n",
      "161/746, train_loss: 5.6930\n",
      "162/746, train_loss: 5.7251\n",
      "163/746, train_loss: 5.7054\n",
      "164/746, train_loss: 5.7095\n",
      "165/746, train_loss: 5.6198\n",
      "166/746, train_loss: 5.7189\n",
      "167/746, train_loss: 5.7191\n",
      "168/746, train_loss: 5.6906\n",
      "169/746, train_loss: 5.6846\n",
      "170/746, train_loss: 5.5536\n",
      "171/746, train_loss: 5.7500\n",
      "172/746, train_loss: 5.7130\n",
      "173/746, train_loss: 5.6610\n",
      "174/746, train_loss: 5.6931\n",
      "175/746, train_loss: 5.5956\n",
      "176/746, train_loss: 5.6612\n",
      "177/746, train_loss: 5.7299\n",
      "178/746, train_loss: 5.6691\n",
      "179/746, train_loss: 5.6078\n",
      "180/746, train_loss: 5.6867\n",
      "181/746, train_loss: 5.7020\n",
      "182/746, train_loss: 5.6530\n",
      "183/746, train_loss: 5.6245\n",
      "184/746, train_loss: 5.6406\n",
      "185/746, train_loss: 5.6547\n",
      "186/746, train_loss: 5.7165\n",
      "187/746, train_loss: 5.7326\n",
      "188/746, train_loss: 5.6584\n",
      "189/746, train_loss: 5.7702\n",
      "190/746, train_loss: 5.7854\n",
      "191/746, train_loss: 5.6984\n",
      "192/746, train_loss: 5.6397\n",
      "193/746, train_loss: 5.6545\n",
      "194/746, train_loss: 5.7901\n",
      "195/746, train_loss: 5.7075\n",
      "196/746, train_loss: 5.7345\n",
      "197/746, train_loss: 5.6656\n",
      "198/746, train_loss: 5.7402\n",
      "199/746, train_loss: 5.7156\n",
      "200/746, train_loss: 5.6799\n",
      "201/746, train_loss: 5.6664\n",
      "202/746, train_loss: 5.6892\n",
      "203/746, train_loss: 5.6143\n",
      "204/746, train_loss: 5.7754\n",
      "205/746, train_loss: 5.7023\n",
      "206/746, train_loss: 5.6463\n",
      "207/746, train_loss: 5.7138\n",
      "208/746, train_loss: 5.6892\n",
      "209/746, train_loss: 5.6248\n",
      "210/746, train_loss: 5.6084\n",
      "211/746, train_loss: 5.6825\n",
      "212/746, train_loss: 5.6595\n",
      "213/746, train_loss: 5.5684\n",
      "214/746, train_loss: 5.6200\n",
      "215/746, train_loss: 5.5812\n",
      "216/746, train_loss: 5.7052\n",
      "217/746, train_loss: 5.5893\n",
      "218/746, train_loss: 5.6974\n",
      "219/746, train_loss: 5.6474\n",
      "220/746, train_loss: 5.7007\n",
      "221/746, train_loss: 5.7461\n",
      "222/746, train_loss: 5.6582\n",
      "223/746, train_loss: 5.6703\n",
      "224/746, train_loss: 5.6492\n",
      "225/746, train_loss: 5.5545\n",
      "226/746, train_loss: 5.5874\n",
      "227/746, train_loss: 5.7024\n",
      "228/746, train_loss: 5.7168\n",
      "229/746, train_loss: 5.6651\n",
      "230/746, train_loss: 5.5918\n",
      "231/746, train_loss: 5.6847\n",
      "232/746, train_loss: 5.6480\n",
      "233/746, train_loss: 5.6624\n",
      "234/746, train_loss: 5.7356\n",
      "235/746, train_loss: 5.6226\n",
      "236/746, train_loss: 5.6898\n",
      "237/746, train_loss: 5.6566\n",
      "238/746, train_loss: 5.6579\n",
      "239/746, train_loss: 5.6505\n",
      "240/746, train_loss: 5.6564\n",
      "241/746, train_loss: 5.6661\n",
      "242/746, train_loss: 5.6081\n",
      "243/746, train_loss: 5.6992\n",
      "244/746, train_loss: 5.6803\n",
      "245/746, train_loss: 5.6264\n",
      "246/746, train_loss: 5.6103\n",
      "247/746, train_loss: 5.6294\n",
      "248/746, train_loss: 5.6226\n",
      "249/746, train_loss: 5.5172\n",
      "250/746, train_loss: 5.7280\n",
      "251/746, train_loss: 5.7424\n",
      "252/746, train_loss: 5.6236\n",
      "253/746, train_loss: 5.6167\n",
      "254/746, train_loss: 5.6631\n",
      "255/746, train_loss: 5.6939\n",
      "256/746, train_loss: 5.6856\n",
      "257/746, train_loss: 5.6801\n",
      "258/746, train_loss: 5.6269\n",
      "259/746, train_loss: 5.7072\n",
      "260/746, train_loss: 5.6649\n",
      "261/746, train_loss: 5.6201\n",
      "262/746, train_loss: 5.6653\n",
      "263/746, train_loss: 5.7064\n",
      "264/746, train_loss: 5.6253\n",
      "265/746, train_loss: 5.5906\n",
      "266/746, train_loss: 5.6165\n",
      "267/746, train_loss: 5.6109\n",
      "268/746, train_loss: 5.6689\n",
      "269/746, train_loss: 5.7483\n",
      "270/746, train_loss: 5.5406\n",
      "271/746, train_loss: 5.6977\n",
      "272/746, train_loss: 5.6028\n",
      "273/746, train_loss: 5.6562\n",
      "274/746, train_loss: 5.5700\n",
      "275/746, train_loss: 5.6962\n",
      "276/746, train_loss: 5.6666\n",
      "277/746, train_loss: 5.6248\n",
      "278/746, train_loss: 5.5775\n",
      "279/746, train_loss: 5.7539\n",
      "280/746, train_loss: 5.5971\n",
      "281/746, train_loss: 5.7176\n",
      "282/746, train_loss: 5.6328\n",
      "283/746, train_loss: 5.5573\n",
      "284/746, train_loss: 5.6147\n",
      "285/746, train_loss: 5.5565\n",
      "286/746, train_loss: 5.6322\n",
      "287/746, train_loss: 5.6341\n",
      "288/746, train_loss: 5.6553\n",
      "289/746, train_loss: 5.6346\n",
      "290/746, train_loss: 5.6161\n",
      "291/746, train_loss: 5.6240\n",
      "292/746, train_loss: 5.6107\n",
      "293/746, train_loss: 5.6411\n",
      "294/746, train_loss: 5.6507\n",
      "295/746, train_loss: 5.7351\n",
      "296/746, train_loss: 5.6215\n",
      "297/746, train_loss: 5.6187\n",
      "298/746, train_loss: 5.6126\n",
      "299/746, train_loss: 5.6709\n",
      "300/746, train_loss: 5.7234\n",
      "301/746, train_loss: 5.6148\n",
      "302/746, train_loss: 5.6445\n",
      "303/746, train_loss: 5.5753\n",
      "304/746, train_loss: 5.7085\n",
      "305/746, train_loss: 5.7243\n",
      "306/746, train_loss: 5.6375\n",
      "307/746, train_loss: 5.7403\n",
      "308/746, train_loss: 5.6474\n",
      "309/746, train_loss: 5.6904\n",
      "310/746, train_loss: 5.5806\n",
      "311/746, train_loss: 5.7381\n",
      "312/746, train_loss: 5.6091\n",
      "313/746, train_loss: 5.6880\n",
      "314/746, train_loss: 5.7059\n",
      "315/746, train_loss: 5.6086\n",
      "316/746, train_loss: 5.6522\n",
      "317/746, train_loss: 5.6774\n",
      "318/746, train_loss: 5.7213\n",
      "319/746, train_loss: 5.7331\n",
      "320/746, train_loss: 5.5884\n",
      "321/746, train_loss: 5.6899\n",
      "322/746, train_loss: 5.6389\n",
      "323/746, train_loss: 5.6925\n",
      "324/746, train_loss: 5.6019\n",
      "325/746, train_loss: 5.6370\n",
      "326/746, train_loss: 5.6205\n",
      "327/746, train_loss: 5.5820\n",
      "328/746, train_loss: 5.7256\n",
      "329/746, train_loss: 5.5948\n",
      "330/746, train_loss: 5.6192\n",
      "331/746, train_loss: 5.6363\n",
      "332/746, train_loss: 5.6714\n",
      "333/746, train_loss: 5.6638\n",
      "334/746, train_loss: 5.6089\n",
      "335/746, train_loss: 5.6298\n",
      "336/746, train_loss: 5.6568\n",
      "337/746, train_loss: 5.5946\n",
      "338/746, train_loss: 5.6713\n",
      "339/746, train_loss: 5.6753\n",
      "340/746, train_loss: 5.6548\n",
      "341/746, train_loss: 5.6526\n",
      "342/746, train_loss: 5.5910\n",
      "343/746, train_loss: 5.6469\n",
      "344/746, train_loss: 5.7030\n",
      "345/746, train_loss: 5.6424\n",
      "346/746, train_loss: 5.6445\n",
      "347/746, train_loss: 5.5909\n",
      "348/746, train_loss: 5.6442\n",
      "349/746, train_loss: 5.6799\n",
      "350/746, train_loss: 5.6856\n",
      "351/746, train_loss: 5.6342\n",
      "352/746, train_loss: 5.6320\n",
      "353/746, train_loss: 5.6194\n",
      "354/746, train_loss: 5.7196\n",
      "355/746, train_loss: 5.6339\n",
      "356/746, train_loss: 5.6182\n",
      "357/746, train_loss: 5.6632\n",
      "358/746, train_loss: 5.6526\n",
      "359/746, train_loss: 5.6945\n",
      "360/746, train_loss: 5.6762\n",
      "361/746, train_loss: 5.6445\n",
      "362/746, train_loss: 5.5312\n",
      "363/746, train_loss: 5.4672\n",
      "364/746, train_loss: 5.6129\n",
      "365/746, train_loss: 5.7007\n",
      "366/746, train_loss: 5.6704\n",
      "367/746, train_loss: 5.6265\n",
      "368/746, train_loss: 5.5763\n",
      "369/746, train_loss: 5.6322\n",
      "370/746, train_loss: 5.5208\n",
      "371/746, train_loss: 5.6280\n",
      "372/746, train_loss: 5.6089\n",
      "373/746, train_loss: 5.6020\n",
      "374/746, train_loss: 5.5987\n",
      "375/746, train_loss: 5.5933\n",
      "376/746, train_loss: 5.6425\n",
      "377/746, train_loss: 5.5280\n",
      "378/746, train_loss: 5.5720\n",
      "379/746, train_loss: 5.6035\n",
      "380/746, train_loss: 5.4921\n",
      "381/746, train_loss: 5.5866\n",
      "382/746, train_loss: 5.5765\n",
      "383/746, train_loss: 5.6199\n",
      "384/746, train_loss: 5.6042\n",
      "385/746, train_loss: 5.6544\n",
      "386/746, train_loss: 5.6897\n",
      "387/746, train_loss: 5.6947\n",
      "388/746, train_loss: 5.6918\n",
      "389/746, train_loss: 5.5067\n",
      "390/746, train_loss: 5.5674\n",
      "391/746, train_loss: 5.5807\n",
      "392/746, train_loss: 5.6296\n",
      "393/746, train_loss: 5.6246\n",
      "394/746, train_loss: 5.5979\n",
      "395/746, train_loss: 5.5806\n",
      "396/746, train_loss: 5.6032\n",
      "397/746, train_loss: 5.7085\n",
      "398/746, train_loss: 5.7244\n",
      "399/746, train_loss: 5.5754\n",
      "400/746, train_loss: 5.6486\n",
      "401/746, train_loss: 5.6118\n",
      "402/746, train_loss: 5.5904\n",
      "403/746, train_loss: 5.6082\n",
      "404/746, train_loss: 5.5804\n",
      "405/746, train_loss: 5.6002\n",
      "406/746, train_loss: 5.5084\n",
      "407/746, train_loss: 5.5669\n",
      "408/746, train_loss: 5.6514\n",
      "409/746, train_loss: 5.6101\n",
      "410/746, train_loss: 5.5869\n",
      "411/746, train_loss: 5.6211\n",
      "412/746, train_loss: 5.5850\n",
      "413/746, train_loss: 5.5882\n",
      "414/746, train_loss: 5.5081\n",
      "415/746, train_loss: 5.6668\n",
      "416/746, train_loss: 5.5753\n",
      "417/746, train_loss: 5.5588\n",
      "418/746, train_loss: 5.6073\n",
      "419/746, train_loss: 5.6874\n",
      "420/746, train_loss: 5.4799\n",
      "421/746, train_loss: 5.6509\n",
      "422/746, train_loss: 5.5667\n",
      "423/746, train_loss: 5.6399\n",
      "424/746, train_loss: 5.6993\n",
      "425/746, train_loss: 5.6370\n",
      "426/746, train_loss: 5.5968\n",
      "427/746, train_loss: 5.6343\n",
      "428/746, train_loss: 5.6502\n",
      "429/746, train_loss: 5.5180\n",
      "430/746, train_loss: 5.5628\n",
      "431/746, train_loss: 5.5789\n",
      "432/746, train_loss: 5.5899\n",
      "433/746, train_loss: 5.7101\n",
      "434/746, train_loss: 5.5371\n",
      "435/746, train_loss: 5.4814\n",
      "436/746, train_loss: 5.5403\n",
      "437/746, train_loss: 5.5609\n",
      "438/746, train_loss: 5.5620\n",
      "439/746, train_loss: 5.6362\n",
      "440/746, train_loss: 5.5640\n",
      "441/746, train_loss: 5.6529\n",
      "442/746, train_loss: 5.5104\n",
      "443/746, train_loss: 5.6028\n",
      "444/746, train_loss: 5.5780\n",
      "445/746, train_loss: 5.6585\n",
      "446/746, train_loss: 5.5945\n",
      "447/746, train_loss: 5.5239\n",
      "448/746, train_loss: 5.5781\n",
      "449/746, train_loss: 5.6533\n",
      "450/746, train_loss: 5.5436\n",
      "451/746, train_loss: 5.5841\n",
      "452/746, train_loss: 5.6704\n",
      "453/746, train_loss: 5.4934\n",
      "454/746, train_loss: 5.6290\n",
      "455/746, train_loss: 5.5617\n",
      "456/746, train_loss: 5.6609\n",
      "457/746, train_loss: 5.7004\n",
      "458/746, train_loss: 5.6186\n",
      "459/746, train_loss: 5.6316\n",
      "460/746, train_loss: 5.5623\n",
      "461/746, train_loss: 5.4416\n",
      "462/746, train_loss: 5.7005\n",
      "463/746, train_loss: 5.5960\n",
      "464/746, train_loss: 5.5852\n",
      "465/746, train_loss: 5.5317\n",
      "466/746, train_loss: 5.6535\n",
      "467/746, train_loss: 5.6707\n",
      "468/746, train_loss: 5.5775\n",
      "469/746, train_loss: 5.7127\n",
      "470/746, train_loss: 5.6224\n",
      "471/746, train_loss: 5.6027\n",
      "472/746, train_loss: 5.6035\n",
      "473/746, train_loss: 5.6994\n",
      "474/746, train_loss: 5.6176\n",
      "475/746, train_loss: 5.5732\n",
      "476/746, train_loss: 5.6743\n",
      "477/746, train_loss: 5.6498\n",
      "478/746, train_loss: 5.6077\n",
      "479/746, train_loss: 5.6297\n",
      "480/746, train_loss: 5.6358\n",
      "481/746, train_loss: 5.6540\n",
      "482/746, train_loss: 5.6164\n",
      "483/746, train_loss: 5.5738\n",
      "484/746, train_loss: 5.5650\n",
      "485/746, train_loss: 5.5393\n",
      "486/746, train_loss: 5.6331\n",
      "487/746, train_loss: 5.6295\n",
      "488/746, train_loss: 5.6220\n",
      "489/746, train_loss: 5.6911\n",
      "490/746, train_loss: 5.6740\n",
      "491/746, train_loss: 5.5867\n",
      "492/746, train_loss: 5.5745\n",
      "493/746, train_loss: 5.5994\n",
      "494/746, train_loss: 5.6680\n",
      "495/746, train_loss: 5.6329\n",
      "496/746, train_loss: 5.6358\n",
      "497/746, train_loss: 5.5613\n",
      "498/746, train_loss: 5.6152\n",
      "499/746, train_loss: 5.5533\n",
      "500/746, train_loss: 5.5443\n",
      "501/746, train_loss: 5.6622\n",
      "502/746, train_loss: 5.5859\n",
      "503/746, train_loss: 5.5561\n",
      "504/746, train_loss: 5.6219\n",
      "505/746, train_loss: 5.6262\n",
      "506/746, train_loss: 5.5307\n",
      "507/746, train_loss: 5.6409\n",
      "508/746, train_loss: 5.5462\n",
      "509/746, train_loss: 5.5854\n",
      "510/746, train_loss: 5.5159\n",
      "511/746, train_loss: 5.5233\n",
      "512/746, train_loss: 5.5533\n",
      "513/746, train_loss: 5.6099\n",
      "514/746, train_loss: 5.5969\n",
      "515/746, train_loss: 5.5970\n",
      "516/746, train_loss: 5.5264\n",
      "517/746, train_loss: 5.4415\n",
      "518/746, train_loss: 5.6375\n",
      "519/746, train_loss: 5.6062\n",
      "520/746, train_loss: 5.5860\n",
      "521/746, train_loss: 5.4770\n",
      "522/746, train_loss: 5.6479\n",
      "523/746, train_loss: 5.5163\n",
      "524/746, train_loss: 5.5548\n",
      "525/746, train_loss: 5.6476\n",
      "526/746, train_loss: 5.4943\n",
      "527/746, train_loss: 5.6335\n",
      "528/746, train_loss: 5.5853\n",
      "529/746, train_loss: 5.4747\n",
      "530/746, train_loss: 5.5290\n",
      "531/746, train_loss: 5.5795\n",
      "532/746, train_loss: 5.6323\n",
      "533/746, train_loss: 5.5778\n",
      "534/746, train_loss: 5.5667\n",
      "535/746, train_loss: 5.6229\n",
      "536/746, train_loss: 5.5635\n",
      "537/746, train_loss: 5.5898\n",
      "538/746, train_loss: 5.4001\n",
      "539/746, train_loss: 5.6386\n",
      "540/746, train_loss: 5.4613\n",
      "541/746, train_loss: 5.5631\n",
      "542/746, train_loss: 5.5552\n",
      "543/746, train_loss: 5.5918\n",
      "544/746, train_loss: 5.4968\n",
      "545/746, train_loss: 5.5608\n",
      "546/746, train_loss: 5.4824\n",
      "547/746, train_loss: 5.6588\n",
      "548/746, train_loss: 5.5274\n",
      "549/746, train_loss: 5.6464\n",
      "550/746, train_loss: 5.5197\n",
      "551/746, train_loss: 5.6497\n",
      "552/746, train_loss: 5.6000\n",
      "553/746, train_loss: 5.5718\n",
      "554/746, train_loss: 5.6490\n",
      "555/746, train_loss: 5.5929\n",
      "556/746, train_loss: 5.5161\n",
      "557/746, train_loss: 5.5204\n",
      "558/746, train_loss: 5.5222\n",
      "559/746, train_loss: 5.5708\n",
      "560/746, train_loss: 5.5148\n",
      "561/746, train_loss: 5.5907\n",
      "562/746, train_loss: 5.5023\n",
      "563/746, train_loss: 5.5311\n",
      "564/746, train_loss: 5.5458\n",
      "565/746, train_loss: 5.6844\n",
      "566/746, train_loss: 5.6398\n",
      "567/746, train_loss: 5.5850\n",
      "568/746, train_loss: 5.5422\n",
      "569/746, train_loss: 5.5411\n",
      "570/746, train_loss: 5.6658\n",
      "571/746, train_loss: 5.6634\n",
      "572/746, train_loss: 5.5791\n",
      "573/746, train_loss: 5.6138\n",
      "574/746, train_loss: 5.5422\n",
      "575/746, train_loss: 5.5672\n",
      "576/746, train_loss: 5.4762\n",
      "577/746, train_loss: 5.6585\n",
      "578/746, train_loss: 5.5242\n",
      "579/746, train_loss: 5.5686\n",
      "580/746, train_loss: 5.6132\n",
      "581/746, train_loss: 5.5932\n",
      "582/746, train_loss: 5.5379\n",
      "583/746, train_loss: 5.6202\n",
      "584/746, train_loss: 5.5956\n",
      "585/746, train_loss: 5.5722\n",
      "586/746, train_loss: 5.5839\n",
      "587/746, train_loss: 5.5738\n",
      "588/746, train_loss: 5.6015\n",
      "589/746, train_loss: 5.5506\n",
      "590/746, train_loss: 5.6121\n",
      "591/746, train_loss: 5.5875\n",
      "592/746, train_loss: 5.6310\n",
      "593/746, train_loss: 5.5800\n",
      "594/746, train_loss: 5.6229\n",
      "595/746, train_loss: 5.6107\n",
      "596/746, train_loss: 5.6642\n",
      "597/746, train_loss: 5.5785\n",
      "598/746, train_loss: 5.5093\n",
      "599/746, train_loss: 5.5136\n",
      "600/746, train_loss: 5.5305\n",
      "601/746, train_loss: 5.5920\n",
      "602/746, train_loss: 5.6177\n",
      "603/746, train_loss: 5.5376\n",
      "604/746, train_loss: 5.6191\n",
      "605/746, train_loss: 5.4745\n",
      "606/746, train_loss: 5.6072\n",
      "607/746, train_loss: 5.5252\n",
      "608/746, train_loss: 5.5866\n",
      "609/746, train_loss: 5.6901\n",
      "610/746, train_loss: 5.5984\n",
      "611/746, train_loss: 5.5078\n",
      "612/746, train_loss: 5.5415\n",
      "613/746, train_loss: 5.5556\n",
      "614/746, train_loss: 5.5255\n",
      "615/746, train_loss: 5.5525\n",
      "616/746, train_loss: 5.4989\n",
      "617/746, train_loss: 5.6581\n",
      "618/746, train_loss: 5.4241\n",
      "619/746, train_loss: 5.5303\n",
      "620/746, train_loss: 5.4870\n",
      "621/746, train_loss: 5.5625\n",
      "622/746, train_loss: 5.5382\n",
      "623/746, train_loss: 5.5180\n",
      "624/746, train_loss: 5.5458\n",
      "625/746, train_loss: 5.6721\n",
      "626/746, train_loss: 5.5661\n",
      "627/746, train_loss: 5.4568\n",
      "628/746, train_loss: 5.6361\n",
      "629/746, train_loss: 5.5166\n",
      "630/746, train_loss: 5.5818\n",
      "631/746, train_loss: 5.4464\n",
      "632/746, train_loss: 5.6018\n",
      "633/746, train_loss: 5.5490\n",
      "634/746, train_loss: 5.5262\n",
      "635/746, train_loss: 5.5669\n",
      "636/746, train_loss: 5.5486\n",
      "637/746, train_loss: 5.5327\n",
      "638/746, train_loss: 5.6930\n",
      "639/746, train_loss: 5.6199\n",
      "640/746, train_loss: 5.5055\n",
      "641/746, train_loss: 5.6407\n",
      "642/746, train_loss: 5.4233\n",
      "643/746, train_loss: 5.6125\n",
      "644/746, train_loss: 5.4485\n",
      "645/746, train_loss: 5.5428\n",
      "646/746, train_loss: 5.5795\n",
      "647/746, train_loss: 5.6281\n",
      "648/746, train_loss: 5.4559\n",
      "649/746, train_loss: 5.5070\n",
      "650/746, train_loss: 5.5581\n",
      "651/746, train_loss: 5.6150\n",
      "652/746, train_loss: 5.4311\n",
      "653/746, train_loss: 5.4997\n",
      "654/746, train_loss: 5.5875\n",
      "655/746, train_loss: 5.5481\n",
      "656/746, train_loss: 5.5294\n",
      "657/746, train_loss: 5.5419\n",
      "658/746, train_loss: 5.6339\n",
      "659/746, train_loss: 5.6301\n",
      "660/746, train_loss: 5.4837\n",
      "661/746, train_loss: 5.5260\n",
      "662/746, train_loss: 5.4749\n",
      "663/746, train_loss: 5.5240\n",
      "664/746, train_loss: 5.6077\n",
      "665/746, train_loss: 5.5319\n",
      "666/746, train_loss: 5.5662\n",
      "667/746, train_loss: 5.5850\n",
      "668/746, train_loss: 5.4163\n",
      "669/746, train_loss: 5.4456\n",
      "670/746, train_loss: 5.4492\n",
      "671/746, train_loss: 5.5702\n",
      "672/746, train_loss: 5.5712\n",
      "673/746, train_loss: 5.5129\n",
      "674/746, train_loss: 5.5684\n",
      "675/746, train_loss: 5.5426\n",
      "676/746, train_loss: 5.5103\n",
      "677/746, train_loss: 5.4761\n",
      "678/746, train_loss: 5.5087\n",
      "679/746, train_loss: 5.5957\n",
      "680/746, train_loss: 5.5653\n",
      "681/746, train_loss: 5.4455\n",
      "682/746, train_loss: 5.4140\n",
      "683/746, train_loss: 5.5656\n",
      "684/746, train_loss: 5.6277\n",
      "685/746, train_loss: 5.6217\n",
      "686/746, train_loss: 5.4593\n",
      "687/746, train_loss: 5.4091\n",
      "688/746, train_loss: 5.4655\n",
      "689/746, train_loss: 5.5661\n",
      "690/746, train_loss: 5.5173\n",
      "691/746, train_loss: 5.6160\n",
      "692/746, train_loss: 5.4422\n",
      "693/746, train_loss: 5.5142\n",
      "694/746, train_loss: 5.6402\n",
      "695/746, train_loss: 5.4173\n",
      "696/746, train_loss: 5.6143\n",
      "697/746, train_loss: 5.4787\n",
      "698/746, train_loss: 5.4570\n",
      "699/746, train_loss: 5.5043\n",
      "700/746, train_loss: 5.5583\n",
      "701/746, train_loss: 5.4824\n",
      "702/746, train_loss: 5.3331\n",
      "703/746, train_loss: 5.6586\n",
      "704/746, train_loss: 5.5811\n",
      "705/746, train_loss: 5.6067\n",
      "706/746, train_loss: 5.6132\n",
      "707/746, train_loss: 5.5770\n",
      "708/746, train_loss: 5.6941\n",
      "709/746, train_loss: 5.5923\n",
      "710/746, train_loss: 5.6011\n",
      "711/746, train_loss: 5.5162\n",
      "712/746, train_loss: 5.4369\n",
      "713/746, train_loss: 5.4195\n",
      "714/746, train_loss: 5.4743\n",
      "715/746, train_loss: 5.4282\n",
      "716/746, train_loss: 5.4449\n",
      "717/746, train_loss: 5.5213\n",
      "718/746, train_loss: 5.5057\n",
      "719/746, train_loss: 5.6002\n",
      "720/746, train_loss: 5.4989\n",
      "721/746, train_loss: 5.5784\n",
      "722/746, train_loss: 5.5862\n",
      "723/746, train_loss: 5.5552\n",
      "724/746, train_loss: 5.6163\n",
      "725/746, train_loss: 5.5860\n",
      "726/746, train_loss: 5.5131\n",
      "727/746, train_loss: 5.4951\n",
      "728/746, train_loss: 5.5073\n",
      "729/746, train_loss: 5.6254\n",
      "730/746, train_loss: 5.4752\n",
      "731/746, train_loss: 5.4026\n",
      "732/746, train_loss: 5.6033\n",
      "733/746, train_loss: 5.4578\n",
      "734/746, train_loss: 5.5323\n",
      "735/746, train_loss: 5.4471\n",
      "736/746, train_loss: 5.5885\n",
      "737/746, train_loss: 5.5100\n",
      "738/746, train_loss: 5.5904\n",
      "739/746, train_loss: 5.5606\n",
      "740/746, train_loss: 5.5872\n",
      "741/746, train_loss: 5.4751\n",
      "742/746, train_loss: 5.4753\n",
      "743/746, train_loss: 5.6607\n",
      "744/746, train_loss: 5.5778\n",
      "745/746, train_loss: 5.5927\n",
      "746/746, train_loss: 5.4463\n",
      "747/746, train_loss: 5.5359\n",
      "epoch 2 average loss: 5.6226\n",
      "saved new best metric model\n",
      "current epoch: 2 current AUC: 0.7748 current accuracy: 0.0743 best AUC: 0.0743 at epoch: 2\n",
      "----------\n",
      "epoch 3/10\n",
      "1/746, train_loss: 5.4604\n",
      "2/746, train_loss: 5.4357\n",
      "3/746, train_loss: 5.4640\n",
      "4/746, train_loss: 5.5256\n",
      "5/746, train_loss: 5.4849\n",
      "6/746, train_loss: 5.5486\n",
      "7/746, train_loss: 5.5319\n",
      "8/746, train_loss: 5.4281\n",
      "9/746, train_loss: 5.5140\n",
      "10/746, train_loss: 5.4464\n",
      "11/746, train_loss: 5.3826\n",
      "12/746, train_loss: 5.4935\n",
      "13/746, train_loss: 5.5085\n",
      "14/746, train_loss: 5.4905\n",
      "15/746, train_loss: 5.5069\n",
      "16/746, train_loss: 5.5593\n",
      "17/746, train_loss: 5.3456\n",
      "18/746, train_loss: 5.5219\n",
      "19/746, train_loss: 5.3622\n",
      "20/746, train_loss: 5.4598\n",
      "21/746, train_loss: 5.3905\n",
      "22/746, train_loss: 5.4263\n",
      "23/746, train_loss: 5.3969\n",
      "24/746, train_loss: 5.4514\n",
      "25/746, train_loss: 5.4581\n",
      "26/746, train_loss: 5.4630\n",
      "27/746, train_loss: 5.4009\n",
      "28/746, train_loss: 5.4113\n",
      "29/746, train_loss: 5.4510\n",
      "30/746, train_loss: 5.3553\n",
      "31/746, train_loss: 5.5335\n",
      "32/746, train_loss: 5.5336\n",
      "33/746, train_loss: 5.5733\n",
      "34/746, train_loss: 5.4062\n",
      "35/746, train_loss: 5.5027\n",
      "36/746, train_loss: 5.4242\n",
      "37/746, train_loss: 5.4755\n",
      "38/746, train_loss: 5.3344\n",
      "39/746, train_loss: 5.4374\n",
      "40/746, train_loss: 5.5309\n",
      "41/746, train_loss: 5.5063\n",
      "42/746, train_loss: 5.4920\n",
      "43/746, train_loss: 5.5033\n",
      "44/746, train_loss: 5.4362\n",
      "45/746, train_loss: 5.3934\n",
      "46/746, train_loss: 5.4239\n",
      "47/746, train_loss: 5.4375\n",
      "48/746, train_loss: 5.4272\n",
      "49/746, train_loss: 5.5228\n",
      "50/746, train_loss: 5.4668\n",
      "51/746, train_loss: 5.5438\n",
      "52/746, train_loss: 5.5158\n",
      "53/746, train_loss: 5.3317\n",
      "54/746, train_loss: 5.4625\n",
      "55/746, train_loss: 5.4419\n",
      "56/746, train_loss: 5.4188\n",
      "57/746, train_loss: 5.4642\n",
      "58/746, train_loss: 5.3174\n",
      "59/746, train_loss: 5.4697\n",
      "60/746, train_loss: 5.5067\n",
      "61/746, train_loss: 5.5139\n",
      "62/746, train_loss: 5.5065\n",
      "63/746, train_loss: 5.4234\n",
      "64/746, train_loss: 5.4411\n",
      "65/746, train_loss: 5.5590\n",
      "66/746, train_loss: 5.3907\n",
      "67/746, train_loss: 5.4549\n",
      "68/746, train_loss: 5.5425\n",
      "69/746, train_loss: 5.3857\n",
      "70/746, train_loss: 5.4214\n",
      "71/746, train_loss: 5.4967\n",
      "72/746, train_loss: 5.4094\n",
      "73/746, train_loss: 5.3967\n",
      "74/746, train_loss: 5.4724\n",
      "75/746, train_loss: 5.3716\n",
      "76/746, train_loss: 5.4598\n",
      "77/746, train_loss: 5.4732\n",
      "78/746, train_loss: 5.3356\n",
      "79/746, train_loss: 5.3741\n",
      "80/746, train_loss: 5.4048\n",
      "81/746, train_loss: 5.5143\n",
      "82/746, train_loss: 5.4332\n",
      "83/746, train_loss: 5.4384\n",
      "84/746, train_loss: 5.4112\n",
      "85/746, train_loss: 5.3646\n",
      "86/746, train_loss: 5.4590\n",
      "87/746, train_loss: 5.3849\n",
      "88/746, train_loss: 5.2846\n",
      "89/746, train_loss: 5.3313\n",
      "90/746, train_loss: 5.3955\n",
      "91/746, train_loss: 5.3109\n",
      "92/746, train_loss: 5.4165\n",
      "93/746, train_loss: 5.4190\n",
      "94/746, train_loss: 5.4889\n",
      "95/746, train_loss: 5.3478\n",
      "96/746, train_loss: 5.3223\n",
      "97/746, train_loss: 5.4393\n",
      "98/746, train_loss: 5.4700\n",
      "99/746, train_loss: 5.4859\n",
      "100/746, train_loss: 5.3446\n",
      "101/746, train_loss: 5.5043\n",
      "102/746, train_loss: 5.5137\n",
      "103/746, train_loss: 5.4229\n",
      "104/746, train_loss: 5.3892\n",
      "105/746, train_loss: 5.3842\n",
      "106/746, train_loss: 5.4719\n",
      "107/746, train_loss: 5.4068\n",
      "108/746, train_loss: 5.4927\n",
      "109/746, train_loss: 5.4537\n",
      "110/746, train_loss: 5.4994\n",
      "111/746, train_loss: 5.3750\n",
      "112/746, train_loss: 5.3564\n",
      "113/746, train_loss: 5.4484\n",
      "114/746, train_loss: 5.4812\n",
      "115/746, train_loss: 5.3694\n",
      "116/746, train_loss: 5.3714\n",
      "117/746, train_loss: 5.6205\n",
      "118/746, train_loss: 5.4029\n",
      "119/746, train_loss: 5.4676\n",
      "120/746, train_loss: 5.3912\n",
      "121/746, train_loss: 5.4592\n",
      "122/746, train_loss: 5.4556\n",
      "123/746, train_loss: 5.3722\n",
      "124/746, train_loss: 5.3935\n",
      "125/746, train_loss: 5.4491\n",
      "126/746, train_loss: 5.4926\n",
      "127/746, train_loss: 5.4345\n",
      "128/746, train_loss: 5.4162\n",
      "129/746, train_loss: 5.4263\n",
      "130/746, train_loss: 5.4345\n",
      "131/746, train_loss: 5.4541\n",
      "132/746, train_loss: 5.3992\n",
      "133/746, train_loss: 5.4946\n",
      "134/746, train_loss: 5.3709\n",
      "135/746, train_loss: 5.4071\n",
      "136/746, train_loss: 5.3633\n",
      "137/746, train_loss: 5.4388\n",
      "138/746, train_loss: 5.4299\n",
      "139/746, train_loss: 5.5141\n",
      "140/746, train_loss: 5.4654\n",
      "141/746, train_loss: 5.3691\n",
      "142/746, train_loss: 5.3531\n",
      "143/746, train_loss: 5.5178\n",
      "144/746, train_loss: 5.4329\n",
      "145/746, train_loss: 5.4252\n",
      "146/746, train_loss: 5.3823\n",
      "147/746, train_loss: 5.4916\n",
      "148/746, train_loss: 5.3614\n",
      "149/746, train_loss: 5.4325\n",
      "150/746, train_loss: 5.2342\n",
      "151/746, train_loss: 5.4907\n",
      "152/746, train_loss: 5.4441\n",
      "153/746, train_loss: 5.5107\n",
      "154/746, train_loss: 5.4647\n",
      "155/746, train_loss: 5.4217\n",
      "156/746, train_loss: 5.4248\n",
      "157/746, train_loss: 5.5179\n",
      "158/746, train_loss: 5.3607\n",
      "159/746, train_loss: 5.3243\n",
      "160/746, train_loss: 5.5689\n",
      "161/746, train_loss: 5.4794\n",
      "162/746, train_loss: 5.3317\n",
      "163/746, train_loss: 5.5408\n",
      "164/746, train_loss: 5.4917\n",
      "165/746, train_loss: 5.4904\n",
      "166/746, train_loss: 5.3389\n",
      "167/746, train_loss: 5.2912\n",
      "168/746, train_loss: 5.2052\n",
      "169/746, train_loss: 5.4080\n",
      "170/746, train_loss: 5.3750\n",
      "171/746, train_loss: 5.3639\n",
      "172/746, train_loss: 5.4152\n",
      "173/746, train_loss: 5.4391\n",
      "174/746, train_loss: 5.3918\n",
      "175/746, train_loss: 5.5613\n",
      "176/746, train_loss: 5.3957\n",
      "177/746, train_loss: 5.3668\n",
      "178/746, train_loss: 5.3928\n",
      "179/746, train_loss: 5.3809\n",
      "180/746, train_loss: 5.3808\n",
      "181/746, train_loss: 5.3415\n",
      "182/746, train_loss: 5.2953\n",
      "183/746, train_loss: 5.4729\n",
      "184/746, train_loss: 5.4244\n",
      "185/746, train_loss: 5.4018\n",
      "186/746, train_loss: 5.3322\n",
      "187/746, train_loss: 5.4471\n",
      "188/746, train_loss: 5.4011\n",
      "189/746, train_loss: 5.4324\n",
      "190/746, train_loss: 5.4656\n",
      "191/746, train_loss: 5.4368\n",
      "192/746, train_loss: 5.4250\n",
      "193/746, train_loss: 5.3726\n",
      "194/746, train_loss: 5.5185\n",
      "195/746, train_loss: 5.4471\n",
      "196/746, train_loss: 5.4390\n",
      "197/746, train_loss: 5.3189\n",
      "198/746, train_loss: 5.4681\n",
      "199/746, train_loss: 5.3369\n",
      "200/746, train_loss: 5.3811\n",
      "201/746, train_loss: 5.3789\n",
      "202/746, train_loss: 5.3609\n",
      "203/746, train_loss: 5.4291\n",
      "204/746, train_loss: 5.3311\n",
      "205/746, train_loss: 5.4543\n",
      "206/746, train_loss: 5.3701\n",
      "207/746, train_loss: 5.5327\n",
      "208/746, train_loss: 5.4891\n",
      "209/746, train_loss: 5.4096\n",
      "210/746, train_loss: 5.3025\n",
      "211/746, train_loss: 5.4429\n",
      "212/746, train_loss: 5.4354\n",
      "213/746, train_loss: 5.3902\n",
      "214/746, train_loss: 5.3621\n",
      "215/746, train_loss: 5.4326\n",
      "216/746, train_loss: 5.3444\n",
      "217/746, train_loss: 5.4323\n",
      "218/746, train_loss: 5.5357\n",
      "219/746, train_loss: 5.4141\n",
      "220/746, train_loss: 5.4467\n",
      "221/746, train_loss: 5.4044\n",
      "222/746, train_loss: 5.2571\n",
      "223/746, train_loss: 5.3066\n",
      "224/746, train_loss: 5.3536\n",
      "225/746, train_loss: 5.4490\n",
      "226/746, train_loss: 5.4879\n",
      "227/746, train_loss: 5.4449\n",
      "228/746, train_loss: 5.3745\n",
      "229/746, train_loss: 5.3101\n",
      "230/746, train_loss: 5.5351\n",
      "231/746, train_loss: 5.3785\n",
      "232/746, train_loss: 5.4696\n",
      "233/746, train_loss: 5.3905\n",
      "234/746, train_loss: 5.4944\n",
      "235/746, train_loss: 5.3153\n",
      "236/746, train_loss: 5.3420\n",
      "237/746, train_loss: 5.5489\n",
      "238/746, train_loss: 5.4039\n",
      "239/746, train_loss: 5.4013\n",
      "240/746, train_loss: 5.4136\n",
      "241/746, train_loss: 5.4310\n",
      "242/746, train_loss: 5.3488\n",
      "243/746, train_loss: 5.4295\n",
      "244/746, train_loss: 5.4645\n",
      "245/746, train_loss: 5.3526\n",
      "246/746, train_loss: 5.4607\n",
      "247/746, train_loss: 5.3427\n",
      "248/746, train_loss: 5.3154\n",
      "249/746, train_loss: 5.4015\n",
      "250/746, train_loss: 5.3282\n",
      "251/746, train_loss: 5.4154\n",
      "252/746, train_loss: 5.4995\n",
      "253/746, train_loss: 5.4793\n",
      "254/746, train_loss: 5.2659\n",
      "255/746, train_loss: 5.3641\n",
      "256/746, train_loss: 5.3206\n",
      "257/746, train_loss: 5.3530\n",
      "258/746, train_loss: 5.3790\n",
      "259/746, train_loss: 5.4791\n",
      "260/746, train_loss: 5.4030\n",
      "261/746, train_loss: 5.5156\n",
      "262/746, train_loss: 5.3747\n",
      "263/746, train_loss: 5.3741\n",
      "264/746, train_loss: 5.4289\n",
      "265/746, train_loss: 5.5466\n",
      "266/746, train_loss: 5.3430\n",
      "267/746, train_loss: 5.4429\n",
      "268/746, train_loss: 5.4442\n",
      "269/746, train_loss: 5.3709\n",
      "270/746, train_loss: 5.4269\n",
      "271/746, train_loss: 5.3430\n",
      "272/746, train_loss: 5.3494\n",
      "273/746, train_loss: 5.3332\n",
      "274/746, train_loss: 5.4284\n",
      "275/746, train_loss: 5.3976\n",
      "276/746, train_loss: 5.4202\n",
      "277/746, train_loss: 5.3256\n",
      "278/746, train_loss: 5.3541\n",
      "279/746, train_loss: 5.4020\n",
      "280/746, train_loss: 5.4344\n",
      "281/746, train_loss: 5.3887\n",
      "282/746, train_loss: 5.3910\n",
      "283/746, train_loss: 5.2953\n",
      "284/746, train_loss: 5.3649\n",
      "285/746, train_loss: 5.2788\n",
      "286/746, train_loss: 5.2953\n",
      "287/746, train_loss: 5.3797\n",
      "288/746, train_loss: 5.5313\n",
      "289/746, train_loss: 5.3787\n",
      "290/746, train_loss: 5.5114\n",
      "291/746, train_loss: 5.4259\n",
      "292/746, train_loss: 5.3484\n",
      "293/746, train_loss: 5.4827\n",
      "294/746, train_loss: 5.4012\n",
      "295/746, train_loss: 5.4234\n",
      "296/746, train_loss: 5.2392\n",
      "297/746, train_loss: 5.2825\n",
      "298/746, train_loss: 5.3957\n",
      "299/746, train_loss: 5.4284\n",
      "300/746, train_loss: 5.4814\n",
      "301/746, train_loss: 5.4505\n",
      "302/746, train_loss: 5.3400\n",
      "303/746, train_loss: 5.3771\n",
      "304/746, train_loss: 5.4856\n",
      "305/746, train_loss: 5.4290\n",
      "306/746, train_loss: 5.2910\n",
      "307/746, train_loss: 5.2814\n",
      "308/746, train_loss: 5.3499\n",
      "309/746, train_loss: 5.4982\n",
      "310/746, train_loss: 5.3279\n",
      "311/746, train_loss: 5.3454\n",
      "312/746, train_loss: 5.3331\n",
      "313/746, train_loss: 5.3091\n",
      "314/746, train_loss: 5.4225\n",
      "315/746, train_loss: 5.4314\n",
      "316/746, train_loss: 5.3586\n",
      "317/746, train_loss: 5.3156\n",
      "318/746, train_loss: 5.4297\n",
      "319/746, train_loss: 5.3233\n",
      "320/746, train_loss: 5.4143\n",
      "321/746, train_loss: 5.3456\n",
      "322/746, train_loss: 5.2935\n",
      "323/746, train_loss: 5.3340\n",
      "324/746, train_loss: 5.3281\n",
      "325/746, train_loss: 5.3448\n",
      "326/746, train_loss: 5.3136\n",
      "327/746, train_loss: 5.3957\n",
      "328/746, train_loss: 5.3943\n",
      "329/746, train_loss: 5.4386\n",
      "330/746, train_loss: 5.2990\n",
      "331/746, train_loss: 5.4296\n",
      "332/746, train_loss: 5.3259\n",
      "333/746, train_loss: 5.4593\n",
      "334/746, train_loss: 5.2784\n",
      "335/746, train_loss: 5.3089\n",
      "336/746, train_loss: 5.4507\n",
      "337/746, train_loss: 5.4085\n",
      "338/746, train_loss: 5.4266\n",
      "339/746, train_loss: 5.3346\n",
      "340/746, train_loss: 5.3626\n",
      "341/746, train_loss: 5.3300\n",
      "342/746, train_loss: 5.5637\n",
      "343/746, train_loss: 5.3623\n",
      "344/746, train_loss: 5.3088\n",
      "345/746, train_loss: 5.4034\n",
      "346/746, train_loss: 5.2927\n",
      "347/746, train_loss: 5.4280\n",
      "348/746, train_loss: 5.4910\n",
      "349/746, train_loss: 5.3821\n",
      "350/746, train_loss: 5.3318\n",
      "351/746, train_loss: 5.3339\n",
      "352/746, train_loss: 5.4080\n",
      "353/746, train_loss: 5.4283\n",
      "354/746, train_loss: 5.3482\n",
      "355/746, train_loss: 5.3832\n",
      "356/746, train_loss: 5.4205\n",
      "357/746, train_loss: 5.2650\n",
      "358/746, train_loss: 5.2765\n",
      "359/746, train_loss: 5.2928\n",
      "360/746, train_loss: 5.4367\n",
      "361/746, train_loss: 5.3088\n",
      "362/746, train_loss: 5.4337\n",
      "363/746, train_loss: 5.2874\n",
      "364/746, train_loss: 5.4580\n",
      "365/746, train_loss: 5.4623\n",
      "366/746, train_loss: 5.3800\n",
      "367/746, train_loss: 5.4488\n",
      "368/746, train_loss: 5.3781\n",
      "369/746, train_loss: 5.2793\n",
      "370/746, train_loss: 5.3681\n",
      "371/746, train_loss: 5.3688\n",
      "372/746, train_loss: 5.4130\n",
      "373/746, train_loss: 5.4798\n",
      "374/746, train_loss: 5.3879\n",
      "375/746, train_loss: 5.2922\n",
      "376/746, train_loss: 5.3596\n",
      "377/746, train_loss: 5.3545\n",
      "378/746, train_loss: 5.2690\n",
      "379/746, train_loss: 5.3948\n",
      "380/746, train_loss: 5.3430\n",
      "381/746, train_loss: 5.3084\n",
      "382/746, train_loss: 5.3184\n",
      "383/746, train_loss: 5.3858\n",
      "384/746, train_loss: 5.3801\n",
      "385/746, train_loss: 5.5362\n",
      "386/746, train_loss: 5.3754\n",
      "387/746, train_loss: 5.3225\n",
      "388/746, train_loss: 5.4542\n",
      "389/746, train_loss: 5.3539\n",
      "390/746, train_loss: 5.3091\n",
      "391/746, train_loss: 5.3980\n",
      "392/746, train_loss: 5.3237\n",
      "393/746, train_loss: 5.4359\n",
      "394/746, train_loss: 5.3456\n",
      "395/746, train_loss: 5.3179\n",
      "396/746, train_loss: 5.2327\n",
      "397/746, train_loss: 5.3895\n",
      "398/746, train_loss: 5.3567\n",
      "399/746, train_loss: 5.2801\n",
      "400/746, train_loss: 5.3417\n",
      "401/746, train_loss: 5.2806\n",
      "402/746, train_loss: 5.3591\n",
      "403/746, train_loss: 5.2830\n",
      "404/746, train_loss: 5.3425\n",
      "405/746, train_loss: 5.3104\n",
      "406/746, train_loss: 5.4174\n",
      "407/746, train_loss: 5.3464\n",
      "408/746, train_loss: 5.4045\n",
      "409/746, train_loss: 5.4849\n",
      "410/746, train_loss: 5.3940\n",
      "411/746, train_loss: 5.3157\n",
      "412/746, train_loss: 5.4392\n",
      "413/746, train_loss: 5.2952\n",
      "414/746, train_loss: 5.3975\n",
      "415/746, train_loss: 5.3764\n",
      "416/746, train_loss: 5.5328\n",
      "417/746, train_loss: 5.3891\n",
      "418/746, train_loss: 5.3764\n",
      "419/746, train_loss: 5.3232\n",
      "420/746, train_loss: 5.3041\n",
      "421/746, train_loss: 5.3133\n",
      "422/746, train_loss: 5.3415\n",
      "423/746, train_loss: 5.4146\n",
      "424/746, train_loss: 5.4464\n",
      "425/746, train_loss: 5.2964\n",
      "426/746, train_loss: 5.3008\n",
      "427/746, train_loss: 5.3182\n",
      "428/746, train_loss: 5.3791\n",
      "429/746, train_loss: 5.2735\n",
      "430/746, train_loss: 5.2727\n",
      "431/746, train_loss: 5.4270\n",
      "432/746, train_loss: 5.3228\n",
      "433/746, train_loss: 5.3390\n",
      "434/746, train_loss: 5.4115\n",
      "435/746, train_loss: 5.3706\n",
      "436/746, train_loss: 5.3850\n",
      "437/746, train_loss: 5.3011\n",
      "438/746, train_loss: 5.3188\n",
      "439/746, train_loss: 5.3987\n",
      "440/746, train_loss: 5.2613\n",
      "441/746, train_loss: 5.2581\n",
      "442/746, train_loss: 5.4118\n",
      "443/746, train_loss: 5.3231\n",
      "444/746, train_loss: 5.3488\n",
      "445/746, train_loss: 5.2938\n",
      "446/746, train_loss: 5.3233\n",
      "447/746, train_loss: 5.4115\n",
      "448/746, train_loss: 5.4370\n",
      "449/746, train_loss: 5.2415\n",
      "450/746, train_loss: 5.4005\n",
      "451/746, train_loss: 5.3476\n",
      "452/746, train_loss: 5.3270\n",
      "453/746, train_loss: 5.3371\n",
      "454/746, train_loss: 5.3905\n",
      "455/746, train_loss: 5.3756\n",
      "456/746, train_loss: 5.2472\n",
      "457/746, train_loss: 5.3194\n",
      "458/746, train_loss: 5.3358\n",
      "459/746, train_loss: 5.3925\n",
      "460/746, train_loss: 5.3346\n",
      "461/746, train_loss: 5.3611\n",
      "462/746, train_loss: 5.3010\n",
      "463/746, train_loss: 5.3582\n",
      "464/746, train_loss: 5.3060\n",
      "465/746, train_loss: 5.2901\n",
      "466/746, train_loss: 5.1569\n",
      "467/746, train_loss: 5.4030\n",
      "468/746, train_loss: 5.3438\n",
      "469/746, train_loss: 5.3734\n",
      "470/746, train_loss: 5.3639\n",
      "471/746, train_loss: 5.3250\n",
      "472/746, train_loss: 5.3225\n",
      "473/746, train_loss: 5.4616\n",
      "474/746, train_loss: 5.3722\n",
      "475/746, train_loss: 5.3719\n",
      "476/746, train_loss: 5.3854\n",
      "477/746, train_loss: 5.3112\n",
      "478/746, train_loss: 5.2378\n",
      "479/746, train_loss: 5.2506\n",
      "480/746, train_loss: 5.4219\n",
      "481/746, train_loss: 5.2572\n",
      "482/746, train_loss: 5.3247\n",
      "483/746, train_loss: 5.2208\n",
      "484/746, train_loss: 5.4331\n",
      "485/746, train_loss: 5.3756\n",
      "486/746, train_loss: 5.4498\n",
      "487/746, train_loss: 5.3300\n",
      "488/746, train_loss: 5.3174\n",
      "489/746, train_loss: 5.2034\n",
      "490/746, train_loss: 5.2544\n",
      "491/746, train_loss: 5.1908\n",
      "492/746, train_loss: 5.1487\n",
      "493/746, train_loss: 5.3560\n",
      "494/746, train_loss: 5.2482\n",
      "495/746, train_loss: 5.2210\n",
      "496/746, train_loss: 5.2369\n",
      "497/746, train_loss: 5.4347\n",
      "498/746, train_loss: 5.3961\n",
      "499/746, train_loss: 5.3069\n",
      "500/746, train_loss: 5.2913\n",
      "501/746, train_loss: 5.3402\n",
      "502/746, train_loss: 5.3177\n",
      "503/746, train_loss: 5.3739\n",
      "504/746, train_loss: 5.3994\n",
      "505/746, train_loss: 5.3272\n",
      "506/746, train_loss: 5.3761\n",
      "507/746, train_loss: 5.2812\n",
      "508/746, train_loss: 5.3132\n",
      "509/746, train_loss: 5.2913\n",
      "510/746, train_loss: 5.3582\n",
      "511/746, train_loss: 5.3198\n",
      "512/746, train_loss: 5.3483\n",
      "513/746, train_loss: 5.1810\n",
      "514/746, train_loss: 5.4372\n",
      "515/746, train_loss: 5.2680\n",
      "516/746, train_loss: 5.2846\n",
      "517/746, train_loss: 5.2100\n",
      "518/746, train_loss: 5.3514\n",
      "519/746, train_loss: 5.2333\n",
      "520/746, train_loss: 5.2625\n",
      "521/746, train_loss: 5.4027\n",
      "522/746, train_loss: 5.2527\n",
      "523/746, train_loss: 5.3486\n",
      "524/746, train_loss: 5.3694\n",
      "525/746, train_loss: 5.3221\n",
      "526/746, train_loss: 5.4396\n",
      "527/746, train_loss: 5.2963\n",
      "528/746, train_loss: 5.2902\n",
      "529/746, train_loss: 5.4276\n",
      "530/746, train_loss: 5.3636\n",
      "531/746, train_loss: 5.3391\n",
      "532/746, train_loss: 5.2397\n",
      "533/746, train_loss: 5.3433\n",
      "534/746, train_loss: 5.2877\n",
      "535/746, train_loss: 5.4266\n",
      "536/746, train_loss: 5.2923\n",
      "537/746, train_loss: 5.4510\n",
      "538/746, train_loss: 5.3296\n",
      "539/746, train_loss: 5.4025\n",
      "540/746, train_loss: 5.2361\n",
      "541/746, train_loss: 5.2036\n",
      "542/746, train_loss: 5.3122\n",
      "543/746, train_loss: 5.1889\n",
      "544/746, train_loss: 5.2444\n",
      "545/746, train_loss: 5.2696\n",
      "546/746, train_loss: 5.3541\n",
      "547/746, train_loss: 5.3814\n",
      "548/746, train_loss: 5.3757\n",
      "549/746, train_loss: 5.3318\n",
      "550/746, train_loss: 5.3484\n",
      "551/746, train_loss: 5.4682\n",
      "552/746, train_loss: 5.3322\n",
      "553/746, train_loss: 5.1735\n",
      "554/746, train_loss: 5.2271\n",
      "555/746, train_loss: 5.1899\n",
      "556/746, train_loss: 5.2693\n",
      "557/746, train_loss: 5.2781\n",
      "558/746, train_loss: 5.3790\n",
      "559/746, train_loss: 5.2582\n",
      "560/746, train_loss: 5.4463\n",
      "561/746, train_loss: 5.3602\n",
      "562/746, train_loss: 5.3579\n",
      "563/746, train_loss: 5.4067\n",
      "564/746, train_loss: 5.3652\n",
      "565/746, train_loss: 5.2674\n",
      "566/746, train_loss: 5.2753\n",
      "567/746, train_loss: 5.2986\n",
      "568/746, train_loss: 5.3334\n",
      "569/746, train_loss: 5.2392\n",
      "570/746, train_loss: 5.4218\n",
      "571/746, train_loss: 5.2603\n",
      "572/746, train_loss: 5.3230\n",
      "573/746, train_loss: 5.2591\n",
      "574/746, train_loss: 5.3570\n",
      "575/746, train_loss: 5.3659\n",
      "576/746, train_loss: 5.3222\n",
      "577/746, train_loss: 5.2498\n",
      "578/746, train_loss: 5.3321\n",
      "579/746, train_loss: 5.1775\n",
      "580/746, train_loss: 5.3722\n",
      "581/746, train_loss: 5.1980\n",
      "582/746, train_loss: 5.3599\n",
      "583/746, train_loss: 5.4312\n",
      "584/746, train_loss: 5.3961\n",
      "585/746, train_loss: 5.3036\n",
      "586/746, train_loss: 5.3053\n",
      "587/746, train_loss: 5.2829\n",
      "588/746, train_loss: 5.2971\n",
      "589/746, train_loss: 5.3245\n",
      "590/746, train_loss: 5.1967\n",
      "591/746, train_loss: 5.2795\n",
      "592/746, train_loss: 5.3533\n",
      "593/746, train_loss: 5.2903\n",
      "594/746, train_loss: 5.2774\n",
      "595/746, train_loss: 5.1842\n",
      "596/746, train_loss: 5.3270\n",
      "597/746, train_loss: 5.1804\n",
      "598/746, train_loss: 5.2377\n",
      "599/746, train_loss: 5.2446\n",
      "600/746, train_loss: 5.2043\n",
      "601/746, train_loss: 5.3781\n",
      "602/746, train_loss: 5.2842\n",
      "603/746, train_loss: 5.2749\n",
      "604/746, train_loss: 5.2826\n",
      "605/746, train_loss: 5.3341\n",
      "606/746, train_loss: 5.3112\n",
      "607/746, train_loss: 5.2366\n",
      "608/746, train_loss: 5.3707\n",
      "609/746, train_loss: 5.2651\n",
      "610/746, train_loss: 5.2998\n",
      "611/746, train_loss: 5.3393\n",
      "612/746, train_loss: 5.2922\n",
      "613/746, train_loss: 5.2577\n",
      "614/746, train_loss: 5.2547\n",
      "615/746, train_loss: 5.2688\n",
      "616/746, train_loss: 5.2883\n",
      "617/746, train_loss: 5.2449\n",
      "618/746, train_loss: 5.3562\n",
      "619/746, train_loss: 5.2674\n",
      "620/746, train_loss: 5.3960\n",
      "621/746, train_loss: 5.2503\n",
      "622/746, train_loss: 5.2318\n",
      "623/746, train_loss: 5.2909\n",
      "624/746, train_loss: 5.2627\n",
      "625/746, train_loss: 5.3795\n",
      "626/746, train_loss: 5.3296\n",
      "627/746, train_loss: 5.2004\n",
      "628/746, train_loss: 5.4043\n",
      "629/746, train_loss: 5.1349\n",
      "630/746, train_loss: 5.3818\n",
      "631/746, train_loss: 5.4077\n",
      "632/746, train_loss: 5.3617\n",
      "633/746, train_loss: 5.3489\n",
      "634/746, train_loss: 5.3060\n",
      "635/746, train_loss: 5.2660\n",
      "636/746, train_loss: 5.2356\n",
      "637/746, train_loss: 5.3576\n",
      "638/746, train_loss: 5.2617\n",
      "639/746, train_loss: 5.2709\n",
      "640/746, train_loss: 5.2081\n",
      "641/746, train_loss: 5.2323\n",
      "642/746, train_loss: 5.1778\n",
      "643/746, train_loss: 5.3059\n",
      "644/746, train_loss: 5.2353\n",
      "645/746, train_loss: 5.4111\n",
      "646/746, train_loss: 5.1629\n",
      "647/746, train_loss: 5.2907\n",
      "648/746, train_loss: 5.2410\n",
      "649/746, train_loss: 5.2116\n",
      "650/746, train_loss: 5.2901\n",
      "651/746, train_loss: 5.2980\n",
      "652/746, train_loss: 5.3480\n",
      "653/746, train_loss: 5.2677\n",
      "654/746, train_loss: 5.2844\n",
      "655/746, train_loss: 5.2559\n",
      "656/746, train_loss: 5.4393\n",
      "657/746, train_loss: 5.2995\n",
      "658/746, train_loss: 5.4139\n",
      "659/746, train_loss: 5.2438\n",
      "660/746, train_loss: 5.2409\n",
      "661/746, train_loss: 5.3311\n",
      "662/746, train_loss: 5.2480\n",
      "663/746, train_loss: 5.3507\n",
      "664/746, train_loss: 5.1220\n",
      "665/746, train_loss: 5.2374\n",
      "666/746, train_loss: 5.2998\n",
      "667/746, train_loss: 5.2319\n",
      "668/746, train_loss: 5.2865\n",
      "669/746, train_loss: 5.2230\n",
      "670/746, train_loss: 5.3415\n",
      "671/746, train_loss: 5.2704\n",
      "672/746, train_loss: 5.2355\n",
      "673/746, train_loss: 5.2218\n",
      "674/746, train_loss: 5.2541\n",
      "675/746, train_loss: 5.4319\n",
      "676/746, train_loss: 5.3263\n",
      "677/746, train_loss: 5.4053\n",
      "678/746, train_loss: 5.2587\n",
      "679/746, train_loss: 5.2773\n",
      "680/746, train_loss: 5.3074\n",
      "681/746, train_loss: 5.2221\n",
      "682/746, train_loss: 5.3782\n",
      "683/746, train_loss: 5.1327\n",
      "684/746, train_loss: 5.3110\n",
      "685/746, train_loss: 5.3087\n",
      "686/746, train_loss: 5.1428\n",
      "687/746, train_loss: 5.2510\n",
      "688/746, train_loss: 5.2426\n",
      "689/746, train_loss: 5.3217\n",
      "690/746, train_loss: 5.3589\n",
      "691/746, train_loss: 5.3106\n",
      "692/746, train_loss: 5.0788\n",
      "693/746, train_loss: 5.2862\n",
      "694/746, train_loss: 5.3428\n",
      "695/746, train_loss: 5.3489\n",
      "696/746, train_loss: 5.2918\n",
      "697/746, train_loss: 5.2739\n",
      "698/746, train_loss: 5.2950\n",
      "699/746, train_loss: 5.3085\n",
      "700/746, train_loss: 5.2111\n",
      "701/746, train_loss: 5.3263\n",
      "702/746, train_loss: 5.1134\n",
      "703/746, train_loss: 5.1920\n",
      "704/746, train_loss: 5.2781\n",
      "705/746, train_loss: 5.3157\n",
      "706/746, train_loss: 5.3589\n",
      "707/746, train_loss: 5.2754\n",
      "708/746, train_loss: 5.2880\n",
      "709/746, train_loss: 5.3584\n",
      "710/746, train_loss: 5.2521\n",
      "711/746, train_loss: 5.3346\n",
      "712/746, train_loss: 5.2779\n",
      "713/746, train_loss: 5.3495\n",
      "714/746, train_loss: 5.2328\n",
      "715/746, train_loss: 5.2509\n",
      "716/746, train_loss: 5.2491\n",
      "717/746, train_loss: 5.2834\n",
      "718/746, train_loss: 5.2039\n",
      "719/746, train_loss: 5.2719\n",
      "720/746, train_loss: 5.2815\n",
      "721/746, train_loss: 5.3662\n",
      "722/746, train_loss: 5.2532\n",
      "723/746, train_loss: 5.2379\n",
      "724/746, train_loss: 5.3359\n",
      "725/746, train_loss: 5.1731\n",
      "726/746, train_loss: 5.1817\n",
      "727/746, train_loss: 5.3015\n",
      "728/746, train_loss: 5.2573\n",
      "729/746, train_loss: 5.1177\n",
      "730/746, train_loss: 5.1978\n",
      "731/746, train_loss: 5.2761\n",
      "732/746, train_loss: 5.2344\n",
      "733/746, train_loss: 5.2711\n",
      "734/746, train_loss: 5.2483\n",
      "735/746, train_loss: 5.2948\n",
      "736/746, train_loss: 5.3212\n",
      "737/746, train_loss: 5.2457\n",
      "738/746, train_loss: 5.2386\n",
      "739/746, train_loss: 5.2041\n",
      "740/746, train_loss: 5.2600\n",
      "741/746, train_loss: 5.1816\n",
      "742/746, train_loss: 5.2858\n",
      "743/746, train_loss: 5.2099\n",
      "744/746, train_loss: 5.2636\n",
      "745/746, train_loss: 5.1458\n",
      "746/746, train_loss: 5.3076\n",
      "747/746, train_loss: 5.3067\n",
      "epoch 3 average loss: 5.3606\n",
      "saved new best metric model\n",
      "current epoch: 3 current AUC: 0.8400 current accuracy: 0.1331 best AUC: 0.1331 at epoch: 3\n",
      "----------\n",
      "epoch 4/10\n",
      "1/746, train_loss: 5.2114\n",
      "2/746, train_loss: 5.0584\n",
      "3/746, train_loss: 5.0170\n",
      "4/746, train_loss: 5.1568\n",
      "5/746, train_loss: 5.2609\n",
      "6/746, train_loss: 5.0144\n",
      "7/746, train_loss: 5.2304\n",
      "8/746, train_loss: 5.1977\n",
      "9/746, train_loss: 5.1687\n",
      "10/746, train_loss: 5.1372\n",
      "11/746, train_loss: 5.2581\n",
      "12/746, train_loss: 5.0988\n",
      "13/746, train_loss: 5.1537\n",
      "14/746, train_loss: 5.1766\n",
      "15/746, train_loss: 5.1535\n",
      "16/746, train_loss: 5.1549\n",
      "17/746, train_loss: 5.2600\n",
      "18/746, train_loss: 5.1319\n",
      "19/746, train_loss: 5.1962\n",
      "20/746, train_loss: 5.3009\n",
      "21/746, train_loss: 5.0799\n",
      "22/746, train_loss: 5.2775\n",
      "23/746, train_loss: 5.1785\n",
      "24/746, train_loss: 5.1867\n",
      "25/746, train_loss: 5.1525\n",
      "26/746, train_loss: 5.2126\n",
      "27/746, train_loss: 5.1121\n",
      "28/746, train_loss: 5.1115\n",
      "29/746, train_loss: 5.1751\n",
      "30/746, train_loss: 5.1775\n",
      "31/746, train_loss: 5.2446\n",
      "32/746, train_loss: 5.2130\n",
      "33/746, train_loss: 5.2793\n",
      "34/746, train_loss: 5.2184\n",
      "35/746, train_loss: 5.2310\n",
      "36/746, train_loss: 5.2568\n",
      "37/746, train_loss: 5.1640\n",
      "38/746, train_loss: 5.1452\n",
      "39/746, train_loss: 5.1663\n",
      "40/746, train_loss: 5.0733\n",
      "41/746, train_loss: 5.1627\n",
      "42/746, train_loss: 5.1716\n",
      "43/746, train_loss: 5.3351\n",
      "44/746, train_loss: 5.1605\n",
      "45/746, train_loss: 5.2038\n",
      "46/746, train_loss: 5.2080\n",
      "47/746, train_loss: 5.2298\n",
      "48/746, train_loss: 5.2530\n",
      "49/746, train_loss: 5.1919\n",
      "50/746, train_loss: 5.1949\n",
      "51/746, train_loss: 5.0955\n",
      "52/746, train_loss: 5.2399\n",
      "53/746, train_loss: 5.1775\n",
      "54/746, train_loss: 5.3195\n",
      "55/746, train_loss: 5.1932\n",
      "56/746, train_loss: 5.0681\n",
      "57/746, train_loss: 5.1000\n",
      "58/746, train_loss: 5.2925\n",
      "59/746, train_loss: 5.2616\n",
      "60/746, train_loss: 5.2171\n",
      "61/746, train_loss: 5.1033\n",
      "62/746, train_loss: 5.1015\n",
      "63/746, train_loss: 5.2519\n",
      "64/746, train_loss: 5.1704\n",
      "65/746, train_loss: 5.0425\n",
      "66/746, train_loss: 5.1609\n",
      "67/746, train_loss: 5.1008\n",
      "68/746, train_loss: 5.1856\n",
      "69/746, train_loss: 5.1476\n",
      "70/746, train_loss: 5.1088\n",
      "71/746, train_loss: 5.1447\n",
      "72/746, train_loss: 5.1950\n",
      "73/746, train_loss: 5.1532\n",
      "74/746, train_loss: 5.1021\n",
      "75/746, train_loss: 5.1993\n",
      "76/746, train_loss: 5.1862\n",
      "77/746, train_loss: 5.1063\n",
      "78/746, train_loss: 5.2566\n",
      "79/746, train_loss: 5.2599\n",
      "80/746, train_loss: 5.3048\n",
      "81/746, train_loss: 5.2781\n",
      "82/746, train_loss: 5.2644\n",
      "83/746, train_loss: 5.1652\n",
      "84/746, train_loss: 5.2419\n",
      "85/746, train_loss: 5.1650\n",
      "86/746, train_loss: 5.1870\n",
      "87/746, train_loss: 5.0041\n",
      "88/746, train_loss: 5.2363\n",
      "89/746, train_loss: 5.2904\n",
      "90/746, train_loss: 5.2781\n",
      "91/746, train_loss: 5.2312\n",
      "92/746, train_loss: 5.1013\n",
      "93/746, train_loss: 5.1872\n",
      "94/746, train_loss: 5.1105\n",
      "95/746, train_loss: 5.1533\n",
      "96/746, train_loss: 5.0189\n",
      "97/746, train_loss: 5.2766\n",
      "98/746, train_loss: 5.1901\n",
      "99/746, train_loss: 5.2315\n",
      "100/746, train_loss: 5.1141\n",
      "101/746, train_loss: 5.1593\n",
      "102/746, train_loss: 5.2010\n",
      "103/746, train_loss: 5.1002\n",
      "104/746, train_loss: 5.2765\n",
      "105/746, train_loss: 5.2047\n",
      "106/746, train_loss: 5.0916\n",
      "107/746, train_loss: 5.2001\n",
      "108/746, train_loss: 5.2194\n",
      "109/746, train_loss: 5.2187\n",
      "110/746, train_loss: 5.1939\n",
      "111/746, train_loss: 5.2336\n",
      "112/746, train_loss: 4.9924\n",
      "113/746, train_loss: 5.2664\n",
      "114/746, train_loss: 5.1998\n",
      "115/746, train_loss: 5.2634\n",
      "116/746, train_loss: 5.1385\n",
      "117/746, train_loss: 5.1320\n",
      "118/746, train_loss: 5.2074\n",
      "119/746, train_loss: 5.2406\n",
      "120/746, train_loss: 5.1704\n",
      "121/746, train_loss: 5.2346\n",
      "122/746, train_loss: 5.1325\n",
      "123/746, train_loss: 5.1530\n",
      "124/746, train_loss: 5.1758\n",
      "125/746, train_loss: 5.2466\n",
      "126/746, train_loss: 5.2517\n",
      "127/746, train_loss: 5.1342\n",
      "128/746, train_loss: 5.0832\n",
      "129/746, train_loss: 5.0232\n",
      "130/746, train_loss: 5.0733\n",
      "131/746, train_loss: 5.1523\n",
      "132/746, train_loss: 5.1536\n",
      "133/746, train_loss: 5.2234\n",
      "134/746, train_loss: 5.2761\n",
      "135/746, train_loss: 5.1977\n",
      "136/746, train_loss: 5.1368\n",
      "137/746, train_loss: 5.2069\n",
      "138/746, train_loss: 5.1657\n",
      "139/746, train_loss: 5.2344\n",
      "140/746, train_loss: 5.3390\n",
      "141/746, train_loss: 5.1942\n",
      "142/746, train_loss: 5.2458\n",
      "143/746, train_loss: 5.1116\n",
      "144/746, train_loss: 5.2033\n",
      "145/746, train_loss: 5.1457\n",
      "146/746, train_loss: 5.1529\n",
      "147/746, train_loss: 5.0464\n",
      "148/746, train_loss: 5.2777\n",
      "149/746, train_loss: 5.1730\n",
      "150/746, train_loss: 5.1351\n",
      "151/746, train_loss: 5.1343\n",
      "152/746, train_loss: 5.2077\n",
      "153/746, train_loss: 5.1680\n",
      "154/746, train_loss: 5.1904\n",
      "155/746, train_loss: 5.0974\n",
      "156/746, train_loss: 5.2131\n",
      "157/746, train_loss: 5.2838\n",
      "158/746, train_loss: 5.0756\n",
      "159/746, train_loss: 5.1302\n",
      "160/746, train_loss: 5.1559\n",
      "161/746, train_loss: 5.1722\n",
      "162/746, train_loss: 5.0004\n",
      "163/746, train_loss: 5.1669\n",
      "164/746, train_loss: 5.2441\n",
      "165/746, train_loss: 5.1460\n",
      "166/746, train_loss: 5.2838\n",
      "167/746, train_loss: 5.0853\n",
      "168/746, train_loss: 5.1605\n",
      "169/746, train_loss: 5.2451\n",
      "170/746, train_loss: 5.0621\n",
      "171/746, train_loss: 5.1083\n",
      "172/746, train_loss: 5.1199\n",
      "173/746, train_loss: 5.1517\n",
      "174/746, train_loss: 5.1289\n",
      "175/746, train_loss: 5.2284\n",
      "176/746, train_loss: 5.1640\n",
      "177/746, train_loss: 5.3219\n",
      "178/746, train_loss: 5.1341\n",
      "179/746, train_loss: 5.2608\n",
      "180/746, train_loss: 5.2139\n",
      "181/746, train_loss: 5.1727\n",
      "182/746, train_loss: 5.0545\n",
      "183/746, train_loss: 5.1663\n",
      "184/746, train_loss: 5.1198\n",
      "185/746, train_loss: 5.2783\n",
      "186/746, train_loss: 5.0490\n",
      "187/746, train_loss: 5.0950\n",
      "188/746, train_loss: 5.1308\n",
      "189/746, train_loss: 5.2248\n",
      "190/746, train_loss: 5.0535\n",
      "191/746, train_loss: 5.1832\n",
      "192/746, train_loss: 5.3025\n",
      "193/746, train_loss: 5.1512\n",
      "194/746, train_loss: 5.0194\n",
      "195/746, train_loss: 5.0881\n",
      "196/746, train_loss: 5.1193\n",
      "197/746, train_loss: 5.1732\n",
      "198/746, train_loss: 4.9732\n",
      "199/746, train_loss: 5.0651\n",
      "200/746, train_loss: 5.0554\n",
      "201/746, train_loss: 5.0158\n",
      "202/746, train_loss: 5.3037\n",
      "203/746, train_loss: 5.1338\n",
      "204/746, train_loss: 5.1675\n",
      "205/746, train_loss: 5.0962\n",
      "206/746, train_loss: 5.0700\n",
      "207/746, train_loss: 5.1727\n",
      "208/746, train_loss: 5.1259\n",
      "209/746, train_loss: 5.1586\n",
      "210/746, train_loss: 5.2153\n",
      "211/746, train_loss: 5.2310\n",
      "212/746, train_loss: 5.1839\n",
      "213/746, train_loss: 5.0897\n",
      "214/746, train_loss: 5.0177\n",
      "215/746, train_loss: 5.1386\n",
      "216/746, train_loss: 5.1325\n",
      "217/746, train_loss: 5.1130\n",
      "218/746, train_loss: 5.1405\n",
      "219/746, train_loss: 5.1232\n",
      "220/746, train_loss: 5.0146\n",
      "221/746, train_loss: 5.0496\n",
      "222/746, train_loss: 5.2050\n",
      "223/746, train_loss: 5.1045\n",
      "224/746, train_loss: 5.2454\n",
      "225/746, train_loss: 5.1777\n",
      "226/746, train_loss: 5.0521\n",
      "227/746, train_loss: 5.0900\n",
      "228/746, train_loss: 5.0972\n",
      "229/746, train_loss: 5.1799\n",
      "230/746, train_loss: 5.2023\n",
      "231/746, train_loss: 5.2216\n",
      "232/746, train_loss: 5.1310\n",
      "233/746, train_loss: 5.1191\n",
      "234/746, train_loss: 5.1207\n",
      "235/746, train_loss: 5.1316\n",
      "236/746, train_loss: 4.9884\n",
      "237/746, train_loss: 5.0012\n",
      "238/746, train_loss: 5.1975\n",
      "239/746, train_loss: 5.1438\n",
      "240/746, train_loss: 5.2245\n",
      "241/746, train_loss: 5.0856\n",
      "242/746, train_loss: 5.1092\n",
      "243/746, train_loss: 5.1398\n",
      "244/746, train_loss: 5.0744\n",
      "245/746, train_loss: 5.0170\n",
      "246/746, train_loss: 5.1192\n",
      "247/746, train_loss: 5.0538\n",
      "248/746, train_loss: 5.1904\n",
      "249/746, train_loss: 5.1202\n",
      "250/746, train_loss: 5.0232\n",
      "251/746, train_loss: 5.1441\n",
      "252/746, train_loss: 5.0467\n",
      "253/746, train_loss: 5.1117\n",
      "254/746, train_loss: 5.1222\n",
      "255/746, train_loss: 5.0189\n",
      "256/746, train_loss: 5.1535\n",
      "257/746, train_loss: 5.0214\n",
      "258/746, train_loss: 5.1684\n",
      "259/746, train_loss: 5.1441\n",
      "260/746, train_loss: 5.0102\n",
      "261/746, train_loss: 5.2135\n",
      "262/746, train_loss: 5.2492\n",
      "263/746, train_loss: 4.9878\n",
      "264/746, train_loss: 5.2738\n",
      "265/746, train_loss: 5.2540\n",
      "266/746, train_loss: 5.2068\n",
      "267/746, train_loss: 5.1745\n",
      "268/746, train_loss: 5.0935\n",
      "269/746, train_loss: 5.0158\n",
      "270/746, train_loss: 5.0835\n",
      "271/746, train_loss: 5.1729\n",
      "272/746, train_loss: 5.0905\n",
      "273/746, train_loss: 5.1513\n",
      "274/746, train_loss: 5.1136\n",
      "275/746, train_loss: 5.1132\n",
      "276/746, train_loss: 5.0325\n",
      "277/746, train_loss: 5.1256\n",
      "278/746, train_loss: 5.1795\n",
      "279/746, train_loss: 5.1595\n",
      "280/746, train_loss: 5.0879\n",
      "281/746, train_loss: 5.2203\n",
      "282/746, train_loss: 5.1071\n",
      "283/746, train_loss: 5.0865\n",
      "284/746, train_loss: 5.1476\n",
      "285/746, train_loss: 5.1009\n",
      "286/746, train_loss: 5.1521\n",
      "287/746, train_loss: 5.1449\n",
      "288/746, train_loss: 5.0836\n",
      "289/746, train_loss: 5.1957\n",
      "290/746, train_loss: 5.1313\n",
      "291/746, train_loss: 5.1801\n",
      "292/746, train_loss: 4.9985\n",
      "293/746, train_loss: 5.0241\n",
      "294/746, train_loss: 5.0265\n",
      "295/746, train_loss: 5.0666\n",
      "296/746, train_loss: 5.2142\n",
      "297/746, train_loss: 5.0511\n",
      "298/746, train_loss: 4.9814\n",
      "299/746, train_loss: 5.2155\n",
      "300/746, train_loss: 5.1444\n",
      "301/746, train_loss: 5.0831\n",
      "302/746, train_loss: 5.1788\n",
      "303/746, train_loss: 5.0729\n",
      "304/746, train_loss: 5.1624\n",
      "305/746, train_loss: 4.9825\n",
      "306/746, train_loss: 5.0577\n",
      "307/746, train_loss: 5.0623\n",
      "308/746, train_loss: 5.0593\n",
      "309/746, train_loss: 5.2413\n",
      "310/746, train_loss: 5.2444\n",
      "311/746, train_loss: 5.0577\n",
      "312/746, train_loss: 4.9960\n",
      "313/746, train_loss: 5.0660\n",
      "314/746, train_loss: 5.1535\n",
      "315/746, train_loss: 5.0245\n",
      "316/746, train_loss: 5.1637\n",
      "317/746, train_loss: 5.1928\n",
      "318/746, train_loss: 5.1450\n",
      "319/746, train_loss: 5.0286\n",
      "320/746, train_loss: 5.1741\n",
      "321/746, train_loss: 5.0870\n",
      "322/746, train_loss: 5.1514\n",
      "323/746, train_loss: 4.9715\n",
      "324/746, train_loss: 4.9932\n",
      "325/746, train_loss: 5.2575\n",
      "326/746, train_loss: 5.1172\n",
      "327/746, train_loss: 5.1439\n",
      "328/746, train_loss: 5.1198\n",
      "329/746, train_loss: 5.1230\n",
      "330/746, train_loss: 5.0410\n",
      "331/746, train_loss: 5.0564\n",
      "332/746, train_loss: 5.0645\n",
      "333/746, train_loss: 5.1347\n",
      "334/746, train_loss: 4.9527\n",
      "335/746, train_loss: 5.1554\n",
      "336/746, train_loss: 5.0722\n",
      "337/746, train_loss: 5.0956\n",
      "338/746, train_loss: 4.9827\n",
      "339/746, train_loss: 5.0222\n",
      "340/746, train_loss: 4.9805\n",
      "341/746, train_loss: 4.9861\n",
      "342/746, train_loss: 5.1420\n",
      "343/746, train_loss: 5.1463\n",
      "344/746, train_loss: 5.1165\n",
      "345/746, train_loss: 5.1748\n",
      "346/746, train_loss: 4.9876\n",
      "347/746, train_loss: 4.9668\n",
      "348/746, train_loss: 4.9833\n",
      "349/746, train_loss: 5.2805\n",
      "350/746, train_loss: 5.1552\n",
      "351/746, train_loss: 4.9517\n",
      "352/746, train_loss: 5.0769\n",
      "353/746, train_loss: 5.2563\n",
      "354/746, train_loss: 5.1288\n",
      "355/746, train_loss: 5.0346\n",
      "356/746, train_loss: 5.0742\n",
      "357/746, train_loss: 5.0549\n",
      "358/746, train_loss: 5.1482\n",
      "359/746, train_loss: 5.1933\n",
      "360/746, train_loss: 5.1262\n",
      "361/746, train_loss: 5.2524\n",
      "362/746, train_loss: 5.1416\n",
      "363/746, train_loss: 5.0264\n",
      "364/746, train_loss: 5.0073\n",
      "365/746, train_loss: 5.1603\n",
      "366/746, train_loss: 5.0801\n",
      "367/746, train_loss: 5.0908\n",
      "368/746, train_loss: 5.0633\n",
      "369/746, train_loss: 5.0886\n",
      "370/746, train_loss: 5.0742\n",
      "371/746, train_loss: 5.1673\n",
      "372/746, train_loss: 4.9613\n",
      "373/746, train_loss: 5.0047\n",
      "374/746, train_loss: 4.9060\n",
      "375/746, train_loss: 5.1635\n",
      "376/746, train_loss: 5.0595\n",
      "377/746, train_loss: 5.1778\n",
      "378/746, train_loss: 4.9455\n",
      "379/746, train_loss: 5.0721\n",
      "380/746, train_loss: 5.0418\n",
      "381/746, train_loss: 5.0518\n",
      "382/746, train_loss: 5.1655\n",
      "383/746, train_loss: 5.1761\n",
      "384/746, train_loss: 4.9807\n",
      "385/746, train_loss: 5.0754\n",
      "386/746, train_loss: 4.8917\n",
      "387/746, train_loss: 5.1056\n",
      "388/746, train_loss: 5.0945\n",
      "389/746, train_loss: 4.9838\n",
      "390/746, train_loss: 4.8752\n",
      "391/746, train_loss: 5.0081\n",
      "392/746, train_loss: 5.1082\n",
      "393/746, train_loss: 5.0611\n",
      "394/746, train_loss: 4.9882\n",
      "395/746, train_loss: 5.1083\n",
      "396/746, train_loss: 5.0832\n",
      "397/746, train_loss: 5.2154\n",
      "398/746, train_loss: 5.0398\n",
      "399/746, train_loss: 5.2597\n",
      "400/746, train_loss: 5.0692\n",
      "401/746, train_loss: 5.1502\n",
      "402/746, train_loss: 4.9772\n",
      "403/746, train_loss: 5.1895\n",
      "404/746, train_loss: 5.1185\n",
      "405/746, train_loss: 5.0636\n",
      "406/746, train_loss: 5.1777\n",
      "407/746, train_loss: 4.9807\n",
      "408/746, train_loss: 5.1967\n",
      "409/746, train_loss: 5.1711\n",
      "410/746, train_loss: 5.3132\n",
      "411/746, train_loss: 4.9053\n",
      "412/746, train_loss: 5.0587\n",
      "413/746, train_loss: 4.9738\n",
      "414/746, train_loss: 5.0130\n",
      "415/746, train_loss: 5.0895\n",
      "416/746, train_loss: 5.0401\n",
      "417/746, train_loss: 5.0174\n",
      "418/746, train_loss: 5.0040\n",
      "419/746, train_loss: 5.0662\n",
      "420/746, train_loss: 4.8942\n",
      "421/746, train_loss: 5.2227\n",
      "422/746, train_loss: 5.0982\n",
      "423/746, train_loss: 5.1425\n",
      "424/746, train_loss: 4.9119\n",
      "425/746, train_loss: 5.0060\n",
      "426/746, train_loss: 4.9685\n",
      "427/746, train_loss: 4.9281\n",
      "428/746, train_loss: 5.1782\n",
      "429/746, train_loss: 4.9222\n",
      "430/746, train_loss: 4.9393\n",
      "431/746, train_loss: 5.0660\n",
      "432/746, train_loss: 5.0432\n",
      "433/746, train_loss: 4.9655\n",
      "434/746, train_loss: 4.9937\n",
      "435/746, train_loss: 5.2958\n",
      "436/746, train_loss: 4.9748\n",
      "437/746, train_loss: 5.0728\n",
      "438/746, train_loss: 5.1334\n",
      "439/746, train_loss: 5.1601\n",
      "440/746, train_loss: 4.9917\n",
      "441/746, train_loss: 4.9801\n",
      "442/746, train_loss: 5.0332\n",
      "443/746, train_loss: 5.0210\n",
      "444/746, train_loss: 5.0897\n",
      "445/746, train_loss: 5.1360\n",
      "446/746, train_loss: 5.1295\n",
      "447/746, train_loss: 5.0115\n",
      "448/746, train_loss: 4.9072\n",
      "449/746, train_loss: 5.1059\n",
      "450/746, train_loss: 4.9459\n",
      "451/746, train_loss: 5.0411\n",
      "452/746, train_loss: 5.0773\n",
      "453/746, train_loss: 5.0311\n",
      "454/746, train_loss: 5.2255\n",
      "455/746, train_loss: 5.1485\n",
      "456/746, train_loss: 5.0387\n",
      "457/746, train_loss: 5.1288\n",
      "458/746, train_loss: 5.0168\n",
      "459/746, train_loss: 5.2049\n",
      "460/746, train_loss: 4.9548\n",
      "461/746, train_loss: 5.1526\n",
      "462/746, train_loss: 4.9880\n",
      "463/746, train_loss: 5.1252\n",
      "464/746, train_loss: 5.0678\n",
      "465/746, train_loss: 5.1670\n",
      "466/746, train_loss: 4.9546\n",
      "467/746, train_loss: 5.0771\n",
      "468/746, train_loss: 4.9463\n",
      "469/746, train_loss: 5.1060\n",
      "470/746, train_loss: 5.0774\n",
      "471/746, train_loss: 5.0906\n",
      "472/746, train_loss: 4.9226\n",
      "473/746, train_loss: 5.0289\n",
      "474/746, train_loss: 5.0288\n",
      "475/746, train_loss: 4.9680\n",
      "476/746, train_loss: 5.0853\n",
      "477/746, train_loss: 4.9600\n",
      "478/746, train_loss: 5.0443\n",
      "479/746, train_loss: 5.0297\n",
      "480/746, train_loss: 4.9700\n",
      "481/746, train_loss: 4.9361\n",
      "482/746, train_loss: 5.1592\n",
      "483/746, train_loss: 4.8988\n",
      "484/746, train_loss: 4.9932\n",
      "485/746, train_loss: 5.1055\n",
      "486/746, train_loss: 5.0040\n",
      "487/746, train_loss: 5.0400\n",
      "488/746, train_loss: 5.1908\n",
      "489/746, train_loss: 5.0878\n",
      "490/746, train_loss: 5.0693\n",
      "491/746, train_loss: 5.0042\n",
      "492/746, train_loss: 4.9023\n",
      "493/746, train_loss: 4.9753\n",
      "494/746, train_loss: 4.9968\n",
      "495/746, train_loss: 5.0552\n",
      "496/746, train_loss: 5.1544\n",
      "497/746, train_loss: 5.0373\n",
      "498/746, train_loss: 5.1260\n",
      "499/746, train_loss: 5.0610\n",
      "500/746, train_loss: 5.1111\n",
      "501/746, train_loss: 4.9899\n",
      "502/746, train_loss: 5.1010\n",
      "503/746, train_loss: 5.0075\n",
      "504/746, train_loss: 5.1071\n",
      "505/746, train_loss: 5.0513\n",
      "506/746, train_loss: 5.0531\n",
      "507/746, train_loss: 5.0858\n",
      "508/746, train_loss: 4.9416\n",
      "509/746, train_loss: 5.0566\n",
      "510/746, train_loss: 4.9527\n",
      "511/746, train_loss: 5.0089\n",
      "512/746, train_loss: 5.1044\n",
      "513/746, train_loss: 5.0501\n",
      "514/746, train_loss: 5.1571\n",
      "515/746, train_loss: 4.8550\n",
      "516/746, train_loss: 5.0728\n",
      "517/746, train_loss: 5.1073\n",
      "518/746, train_loss: 5.0091\n",
      "519/746, train_loss: 5.2124\n",
      "520/746, train_loss: 4.9601\n",
      "521/746, train_loss: 5.0718\n",
      "522/746, train_loss: 4.9007\n",
      "523/746, train_loss: 5.0210\n",
      "524/746, train_loss: 5.2085\n",
      "525/746, train_loss: 5.0859\n",
      "526/746, train_loss: 5.0414\n",
      "527/746, train_loss: 4.8785\n",
      "528/746, train_loss: 5.0442\n",
      "529/746, train_loss: 4.9978\n",
      "530/746, train_loss: 4.9630\n",
      "531/746, train_loss: 4.8899\n",
      "532/746, train_loss: 5.0680\n",
      "533/746, train_loss: 4.9266\n",
      "534/746, train_loss: 4.9545\n",
      "535/746, train_loss: 5.1003\n",
      "536/746, train_loss: 4.9610\n",
      "537/746, train_loss: 4.8874\n",
      "538/746, train_loss: 5.1167\n",
      "539/746, train_loss: 4.9097\n",
      "540/746, train_loss: 4.9775\n",
      "541/746, train_loss: 5.0006\n",
      "542/746, train_loss: 5.0267\n",
      "543/746, train_loss: 5.0472\n",
      "544/746, train_loss: 5.1020\n",
      "545/746, train_loss: 4.9252\n",
      "546/746, train_loss: 4.9752\n",
      "547/746, train_loss: 5.1031\n",
      "548/746, train_loss: 5.1238\n",
      "549/746, train_loss: 5.0248\n",
      "550/746, train_loss: 4.9350\n",
      "551/746, train_loss: 5.1204\n",
      "552/746, train_loss: 5.0683\n",
      "553/746, train_loss: 4.9931\n",
      "554/746, train_loss: 5.0010\n",
      "555/746, train_loss: 5.0272\n",
      "556/746, train_loss: 4.8829\n",
      "557/746, train_loss: 4.9283\n",
      "558/746, train_loss: 4.9058\n",
      "559/746, train_loss: 5.1231\n",
      "560/746, train_loss: 5.0445\n",
      "561/746, train_loss: 5.0593\n",
      "562/746, train_loss: 5.0681\n",
      "563/746, train_loss: 4.8617\n",
      "564/746, train_loss: 5.0257\n",
      "565/746, train_loss: 5.0421\n",
      "566/746, train_loss: 5.1218\n",
      "567/746, train_loss: 5.1976\n",
      "568/746, train_loss: 4.9415\n",
      "569/746, train_loss: 4.9911\n",
      "570/746, train_loss: 5.1138\n",
      "571/746, train_loss: 4.9132\n",
      "572/746, train_loss: 4.9801\n",
      "573/746, train_loss: 4.9894\n",
      "574/746, train_loss: 4.9771\n",
      "575/746, train_loss: 5.0675\n",
      "576/746, train_loss: 5.1357\n",
      "577/746, train_loss: 4.9941\n",
      "578/746, train_loss: 4.9658\n",
      "579/746, train_loss: 4.9858\n",
      "580/746, train_loss: 4.9504\n",
      "581/746, train_loss: 5.0546\n",
      "582/746, train_loss: 4.9778\n",
      "583/746, train_loss: 5.0128\n",
      "584/746, train_loss: 5.0431\n",
      "585/746, train_loss: 5.0437\n",
      "586/746, train_loss: 5.0520\n",
      "587/746, train_loss: 5.0462\n",
      "588/746, train_loss: 4.8685\n",
      "589/746, train_loss: 5.0333\n",
      "590/746, train_loss: 4.9267\n",
      "591/746, train_loss: 4.9619\n",
      "592/746, train_loss: 4.9871\n",
      "593/746, train_loss: 4.9162\n",
      "594/746, train_loss: 4.9749\n",
      "595/746, train_loss: 5.0692\n",
      "596/746, train_loss: 5.0091\n",
      "597/746, train_loss: 5.1364\n",
      "598/746, train_loss: 4.9178\n",
      "599/746, train_loss: 4.9400\n",
      "600/746, train_loss: 4.9637\n",
      "601/746, train_loss: 5.1206\n",
      "602/746, train_loss: 5.0389\n",
      "603/746, train_loss: 5.0242\n",
      "604/746, train_loss: 5.0469\n",
      "605/746, train_loss: 5.0595\n",
      "606/746, train_loss: 4.9814\n",
      "607/746, train_loss: 4.9766\n",
      "608/746, train_loss: 4.9543\n",
      "609/746, train_loss: 5.1088\n",
      "610/746, train_loss: 5.0440\n",
      "611/746, train_loss: 4.9808\n",
      "612/746, train_loss: 4.9556\n",
      "613/746, train_loss: 4.9784\n",
      "614/746, train_loss: 5.0393\n",
      "615/746, train_loss: 4.9685\n",
      "616/746, train_loss: 5.0145\n",
      "617/746, train_loss: 4.8558\n",
      "618/746, train_loss: 4.9552\n",
      "619/746, train_loss: 5.0046\n",
      "620/746, train_loss: 4.9016\n",
      "621/746, train_loss: 4.8847\n",
      "622/746, train_loss: 4.9922\n",
      "623/746, train_loss: 5.1142\n",
      "624/746, train_loss: 4.8864\n",
      "625/746, train_loss: 4.9916\n",
      "626/746, train_loss: 5.0028\n",
      "627/746, train_loss: 5.1303\n",
      "628/746, train_loss: 4.9129\n",
      "629/746, train_loss: 4.9889\n",
      "630/746, train_loss: 4.8899\n",
      "631/746, train_loss: 5.0190\n",
      "632/746, train_loss: 5.0220\n",
      "633/746, train_loss: 5.0888\n",
      "634/746, train_loss: 5.0145\n",
      "635/746, train_loss: 5.0606\n",
      "636/746, train_loss: 5.0485\n",
      "637/746, train_loss: 4.8267\n",
      "638/746, train_loss: 4.9740\n",
      "639/746, train_loss: 4.9943\n",
      "640/746, train_loss: 4.9244\n",
      "641/746, train_loss: 5.0534\n",
      "642/746, train_loss: 5.0498\n",
      "643/746, train_loss: 4.9667\n",
      "644/746, train_loss: 4.9942\n",
      "645/746, train_loss: 4.8652\n",
      "646/746, train_loss: 5.0125\n",
      "647/746, train_loss: 5.0205\n",
      "648/746, train_loss: 5.0969\n",
      "649/746, train_loss: 4.9225\n",
      "650/746, train_loss: 4.8677\n",
      "651/746, train_loss: 4.9989\n",
      "652/746, train_loss: 5.0201\n",
      "653/746, train_loss: 5.1571\n",
      "654/746, train_loss: 5.1974\n",
      "655/746, train_loss: 5.0055\n",
      "656/746, train_loss: 5.0289\n",
      "657/746, train_loss: 5.1207\n",
      "658/746, train_loss: 4.8944\n",
      "659/746, train_loss: 5.1293\n",
      "660/746, train_loss: 5.0249\n",
      "661/746, train_loss: 4.9020\n",
      "662/746, train_loss: 5.0141\n",
      "663/746, train_loss: 5.2288\n",
      "664/746, train_loss: 4.9850\n",
      "665/746, train_loss: 5.0023\n",
      "666/746, train_loss: 4.9532\n",
      "667/746, train_loss: 5.0468\n",
      "668/746, train_loss: 4.9386\n",
      "669/746, train_loss: 5.0330\n",
      "670/746, train_loss: 5.0696\n",
      "671/746, train_loss: 5.0088\n",
      "672/746, train_loss: 4.8408\n",
      "673/746, train_loss: 4.9929\n",
      "674/746, train_loss: 5.0260\n",
      "675/746, train_loss: 5.0167\n",
      "676/746, train_loss: 4.9352\n",
      "677/746, train_loss: 5.0576\n",
      "678/746, train_loss: 4.9023\n",
      "679/746, train_loss: 4.9348\n",
      "680/746, train_loss: 4.8148\n",
      "681/746, train_loss: 4.9910\n",
      "682/746, train_loss: 4.9554\n",
      "683/746, train_loss: 4.9173\n",
      "684/746, train_loss: 4.9549\n",
      "685/746, train_loss: 4.9791\n",
      "686/746, train_loss: 5.0497\n",
      "687/746, train_loss: 4.9817\n",
      "688/746, train_loss: 4.8814\n",
      "689/746, train_loss: 5.0195\n",
      "690/746, train_loss: 4.9673\n",
      "691/746, train_loss: 4.9307\n",
      "692/746, train_loss: 4.9914\n",
      "693/746, train_loss: 4.8971\n",
      "694/746, train_loss: 5.0363\n",
      "695/746, train_loss: 4.8340\n",
      "696/746, train_loss: 4.9517\n",
      "697/746, train_loss: 4.8976\n",
      "698/746, train_loss: 5.0458\n",
      "699/746, train_loss: 5.0626\n",
      "700/746, train_loss: 4.8982\n",
      "701/746, train_loss: 4.9739\n",
      "702/746, train_loss: 4.9682\n",
      "703/746, train_loss: 4.7798\n",
      "704/746, train_loss: 5.1273\n",
      "705/746, train_loss: 4.9562\n",
      "706/746, train_loss: 4.8865\n",
      "707/746, train_loss: 5.0539\n",
      "708/746, train_loss: 4.8981\n",
      "709/746, train_loss: 4.8685\n",
      "710/746, train_loss: 5.0511\n",
      "711/746, train_loss: 5.0709\n",
      "712/746, train_loss: 5.1201\n",
      "713/746, train_loss: 4.9763\n",
      "714/746, train_loss: 5.0059\n",
      "715/746, train_loss: 4.9050\n",
      "716/746, train_loss: 5.0445\n",
      "717/746, train_loss: 4.9026\n",
      "718/746, train_loss: 5.0442\n",
      "719/746, train_loss: 4.9722\n",
      "720/746, train_loss: 4.9078\n",
      "721/746, train_loss: 4.9749\n",
      "722/746, train_loss: 4.9408\n",
      "723/746, train_loss: 4.8987\n",
      "724/746, train_loss: 4.9786\n",
      "725/746, train_loss: 4.9898\n",
      "726/746, train_loss: 5.1182\n",
      "727/746, train_loss: 4.9939\n",
      "728/746, train_loss: 4.9441\n",
      "729/746, train_loss: 4.9273\n",
      "730/746, train_loss: 5.0291\n",
      "731/746, train_loss: 4.9546\n",
      "732/746, train_loss: 4.8953\n",
      "733/746, train_loss: 4.9922\n",
      "734/746, train_loss: 4.9439\n",
      "735/746, train_loss: 4.9461\n",
      "736/746, train_loss: 5.1153\n",
      "737/746, train_loss: 4.9899\n",
      "738/746, train_loss: 5.0996\n",
      "739/746, train_loss: 4.8743\n",
      "740/746, train_loss: 4.9778\n",
      "741/746, train_loss: 4.9778\n",
      "742/746, train_loss: 4.9335\n",
      "743/746, train_loss: 4.9217\n",
      "744/746, train_loss: 4.9956\n",
      "745/746, train_loss: 4.8390\n",
      "746/746, train_loss: 4.9662\n",
      "747/746, train_loss: 4.9471\n",
      "epoch 4 average loss: 5.0804\n",
      "saved new best metric model\n",
      "current epoch: 4 current AUC: 0.8878 current accuracy: 0.1921 best AUC: 0.1921 at epoch: 4\n",
      "----------\n",
      "epoch 5/10\n",
      "1/746, train_loss: 4.8300\n",
      "2/746, train_loss: 4.7395\n",
      "3/746, train_loss: 4.8136\n",
      "4/746, train_loss: 4.9488\n",
      "5/746, train_loss: 4.9558\n",
      "6/746, train_loss: 4.7903\n",
      "7/746, train_loss: 4.7698\n",
      "8/746, train_loss: 4.8667\n",
      "9/746, train_loss: 4.8281\n",
      "10/746, train_loss: 5.0112\n",
      "11/746, train_loss: 4.7542\n",
      "12/746, train_loss: 4.6529\n",
      "13/746, train_loss: 4.8690\n",
      "14/746, train_loss: 4.8824\n",
      "15/746, train_loss: 4.9612\n",
      "16/746, train_loss: 4.8170\n",
      "17/746, train_loss: 4.7473\n",
      "18/746, train_loss: 4.6097\n",
      "19/746, train_loss: 4.7054\n",
      "20/746, train_loss: 4.8389\n",
      "21/746, train_loss: 5.0319\n",
      "22/746, train_loss: 4.8685\n",
      "23/746, train_loss: 4.8198\n",
      "24/746, train_loss: 4.9399\n",
      "25/746, train_loss: 4.9921\n",
      "26/746, train_loss: 4.8547\n",
      "27/746, train_loss: 4.8502\n",
      "28/746, train_loss: 4.9121\n",
      "29/746, train_loss: 4.8257\n",
      "30/746, train_loss: 4.8631\n",
      "31/746, train_loss: 4.9403\n",
      "32/746, train_loss: 4.7704\n",
      "33/746, train_loss: 5.0335\n",
      "34/746, train_loss: 4.7717\n",
      "35/746, train_loss: 4.8273\n",
      "36/746, train_loss: 4.8711\n",
      "37/746, train_loss: 4.9538\n",
      "38/746, train_loss: 4.9257\n",
      "39/746, train_loss: 4.9009\n",
      "40/746, train_loss: 4.9178\n",
      "41/746, train_loss: 4.9989\n",
      "42/746, train_loss: 4.9062\n",
      "43/746, train_loss: 4.9841\n",
      "44/746, train_loss: 4.7869\n",
      "45/746, train_loss: 4.9037\n",
      "46/746, train_loss: 4.9097\n",
      "47/746, train_loss: 4.8847\n",
      "48/746, train_loss: 4.9982\n",
      "49/746, train_loss: 4.8536\n",
      "50/746, train_loss: 4.8888\n",
      "51/746, train_loss: 4.9608\n",
      "52/746, train_loss: 4.8418\n",
      "53/746, train_loss: 5.0092\n",
      "54/746, train_loss: 4.9088\n",
      "55/746, train_loss: 4.7465\n",
      "56/746, train_loss: 4.9587\n",
      "57/746, train_loss: 4.8983\n",
      "58/746, train_loss: 4.8094\n",
      "59/746, train_loss: 4.8096\n",
      "60/746, train_loss: 4.8928\n",
      "61/746, train_loss: 4.8382\n",
      "62/746, train_loss: 4.7837\n",
      "63/746, train_loss: 4.7680\n",
      "64/746, train_loss: 4.9615\n",
      "65/746, train_loss: 4.9386\n",
      "66/746, train_loss: 4.8943\n",
      "67/746, train_loss: 4.8152\n",
      "68/746, train_loss: 4.8994\n",
      "69/746, train_loss: 4.7646\n",
      "70/746, train_loss: 5.0157\n",
      "71/746, train_loss: 4.8237\n",
      "72/746, train_loss: 4.8061\n",
      "73/746, train_loss: 4.9774\n",
      "74/746, train_loss: 4.8273\n",
      "75/746, train_loss: 4.8544\n",
      "76/746, train_loss: 5.0310\n",
      "77/746, train_loss: 4.9301\n",
      "78/746, train_loss: 4.8590\n",
      "79/746, train_loss: 4.6971\n",
      "80/746, train_loss: 4.8225\n",
      "81/746, train_loss: 4.9681\n",
      "82/746, train_loss: 4.9347\n",
      "83/746, train_loss: 5.0184\n",
      "84/746, train_loss: 4.9091\n",
      "85/746, train_loss: 4.8632\n",
      "86/746, train_loss: 4.9223\n",
      "87/746, train_loss: 4.7804\n",
      "88/746, train_loss: 4.9423\n",
      "89/746, train_loss: 4.8126\n",
      "90/746, train_loss: 4.8220\n",
      "91/746, train_loss: 4.8394\n",
      "92/746, train_loss: 4.8677\n",
      "93/746, train_loss: 4.8192\n",
      "94/746, train_loss: 5.0517\n",
      "95/746, train_loss: 4.7962\n",
      "96/746, train_loss: 4.8348\n",
      "97/746, train_loss: 4.8892\n",
      "98/746, train_loss: 4.9179\n",
      "99/746, train_loss: 4.9696\n",
      "100/746, train_loss: 4.8536\n",
      "101/746, train_loss: 4.7956\n",
      "102/746, train_loss: 4.8648\n",
      "103/746, train_loss: 4.8492\n",
      "104/746, train_loss: 4.8488\n",
      "105/746, train_loss: 4.9266\n",
      "106/746, train_loss: 4.9529\n",
      "107/746, train_loss: 4.9687\n",
      "108/746, train_loss: 4.9399\n",
      "109/746, train_loss: 4.8453\n",
      "110/746, train_loss: 4.8433\n",
      "111/746, train_loss: 4.9292\n",
      "112/746, train_loss: 4.9130\n",
      "113/746, train_loss: 4.8424\n",
      "114/746, train_loss: 4.8697\n",
      "115/746, train_loss: 4.6196\n",
      "116/746, train_loss: 4.9721\n",
      "117/746, train_loss: 4.8253\n",
      "118/746, train_loss: 4.9118\n",
      "119/746, train_loss: 4.8492\n",
      "120/746, train_loss: 4.8773\n",
      "121/746, train_loss: 4.7961\n",
      "122/746, train_loss: 4.7063\n",
      "123/746, train_loss: 4.8322\n",
      "124/746, train_loss: 4.6320\n",
      "125/746, train_loss: 4.8655\n",
      "126/746, train_loss: 4.9723\n",
      "127/746, train_loss: 5.0400\n",
      "128/746, train_loss: 4.8710\n",
      "129/746, train_loss: 4.8893\n",
      "130/746, train_loss: 4.7847\n",
      "131/746, train_loss: 4.9649\n",
      "132/746, train_loss: 4.9843\n",
      "133/746, train_loss: 4.8860\n",
      "134/746, train_loss: 4.9247\n",
      "135/746, train_loss: 4.7827\n",
      "136/746, train_loss: 4.7702\n",
      "137/746, train_loss: 4.7667\n",
      "138/746, train_loss: 4.6707\n",
      "139/746, train_loss: 4.7917\n",
      "140/746, train_loss: 4.7240\n",
      "141/746, train_loss: 4.8114\n",
      "142/746, train_loss: 4.7547\n",
      "143/746, train_loss: 4.8940\n",
      "144/746, train_loss: 4.8150\n",
      "145/746, train_loss: 4.9340\n",
      "146/746, train_loss: 4.7991\n",
      "147/746, train_loss: 4.6628\n",
      "148/746, train_loss: 4.9061\n",
      "149/746, train_loss: 4.8251\n",
      "150/746, train_loss: 4.8068\n",
      "151/746, train_loss: 4.8770\n",
      "152/746, train_loss: 4.7436\n",
      "153/746, train_loss: 4.9195\n",
      "154/746, train_loss: 4.7170\n",
      "155/746, train_loss: 4.8496\n",
      "156/746, train_loss: 4.8454\n",
      "157/746, train_loss: 4.7570\n",
      "158/746, train_loss: 4.9514\n",
      "159/746, train_loss: 4.8672\n",
      "160/746, train_loss: 4.8251\n",
      "161/746, train_loss: 5.0095\n",
      "162/746, train_loss: 4.9480\n",
      "163/746, train_loss: 4.9591\n",
      "164/746, train_loss: 4.8087\n",
      "165/746, train_loss: 4.9645\n",
      "166/746, train_loss: 4.6959\n",
      "167/746, train_loss: 4.9044\n",
      "168/746, train_loss: 4.8957\n",
      "169/746, train_loss: 4.9055\n",
      "170/746, train_loss: 4.7687\n",
      "171/746, train_loss: 4.7764\n",
      "172/746, train_loss: 4.7785\n",
      "173/746, train_loss: 4.7763\n",
      "174/746, train_loss: 4.8436\n",
      "175/746, train_loss: 4.8223\n",
      "176/746, train_loss: 4.8064\n",
      "177/746, train_loss: 4.8277\n",
      "178/746, train_loss: 4.9401\n",
      "179/746, train_loss: 4.8704\n",
      "180/746, train_loss: 4.7750\n",
      "181/746, train_loss: 4.8107\n",
      "182/746, train_loss: 4.8586\n",
      "183/746, train_loss: 4.8998\n",
      "184/746, train_loss: 4.8874\n",
      "185/746, train_loss: 4.7746\n",
      "186/746, train_loss: 4.9738\n",
      "187/746, train_loss: 4.9034\n",
      "188/746, train_loss: 4.8733\n",
      "189/746, train_loss: 4.7984\n",
      "190/746, train_loss: 4.8255\n",
      "191/746, train_loss: 4.9276\n",
      "192/746, train_loss: 4.8456\n",
      "193/746, train_loss: 4.8931\n",
      "194/746, train_loss: 4.8306\n",
      "195/746, train_loss: 4.9151\n",
      "196/746, train_loss: 4.7975\n",
      "197/746, train_loss: 4.9330\n",
      "198/746, train_loss: 4.7682\n",
      "199/746, train_loss: 4.8190\n",
      "200/746, train_loss: 4.8292\n",
      "201/746, train_loss: 4.7980\n",
      "202/746, train_loss: 4.8923\n",
      "203/746, train_loss: 4.8755\n",
      "204/746, train_loss: 4.8285\n",
      "205/746, train_loss: 4.8892\n",
      "206/746, train_loss: 4.8558\n",
      "207/746, train_loss: 4.7968\n",
      "208/746, train_loss: 4.7091\n",
      "209/746, train_loss: 4.8463\n",
      "210/746, train_loss: 4.6923\n",
      "211/746, train_loss: 4.7481\n",
      "212/746, train_loss: 4.7825\n",
      "213/746, train_loss: 4.8691\n",
      "214/746, train_loss: 4.8089\n",
      "215/746, train_loss: 4.7391\n",
      "216/746, train_loss: 4.7322\n",
      "217/746, train_loss: 4.6825\n",
      "218/746, train_loss: 4.8040\n",
      "219/746, train_loss: 4.6839\n",
      "220/746, train_loss: 4.7637\n",
      "221/746, train_loss: 4.7412\n",
      "222/746, train_loss: 4.8411\n",
      "223/746, train_loss: 4.7917\n",
      "224/746, train_loss: 4.7745\n",
      "225/746, train_loss: 4.9457\n",
      "226/746, train_loss: 4.9150\n",
      "227/746, train_loss: 4.9042\n",
      "228/746, train_loss: 5.0415\n",
      "229/746, train_loss: 4.7650\n",
      "230/746, train_loss: 4.8245\n",
      "231/746, train_loss: 4.8787\n",
      "232/746, train_loss: 4.8227\n",
      "233/746, train_loss: 4.9874\n",
      "234/746, train_loss: 4.7265\n",
      "235/746, train_loss: 4.8982\n",
      "236/746, train_loss: 4.8843\n",
      "237/746, train_loss: 4.9463\n",
      "238/746, train_loss: 4.7645\n",
      "239/746, train_loss: 4.8584\n",
      "240/746, train_loss: 4.9674\n",
      "241/746, train_loss: 4.9379\n",
      "242/746, train_loss: 4.8472\n",
      "243/746, train_loss: 4.8359\n",
      "244/746, train_loss: 4.7563\n",
      "245/746, train_loss: 4.8911\n",
      "246/746, train_loss: 4.7937\n",
      "247/746, train_loss: 4.8689\n",
      "248/746, train_loss: 4.7315\n",
      "249/746, train_loss: 4.8800\n",
      "250/746, train_loss: 4.7231\n",
      "251/746, train_loss: 4.9514\n",
      "252/746, train_loss: 4.7523\n",
      "253/746, train_loss: 4.7252\n",
      "254/746, train_loss: 4.7422\n",
      "255/746, train_loss: 4.6932\n",
      "256/746, train_loss: 4.9382\n",
      "257/746, train_loss: 4.8180\n",
      "258/746, train_loss: 4.8113\n",
      "259/746, train_loss: 4.8334\n",
      "260/746, train_loss: 4.8627\n",
      "261/746, train_loss: 4.7269\n",
      "262/746, train_loss: 4.8324\n",
      "263/746, train_loss: 4.7188\n",
      "264/746, train_loss: 4.9643\n",
      "265/746, train_loss: 4.6706\n",
      "266/746, train_loss: 4.8936\n",
      "267/746, train_loss: 4.5627\n",
      "268/746, train_loss: 4.8682\n",
      "269/746, train_loss: 4.7846\n",
      "270/746, train_loss: 4.7777\n",
      "271/746, train_loss: 4.8815\n",
      "272/746, train_loss: 4.6748\n",
      "273/746, train_loss: 4.9913\n",
      "274/746, train_loss: 4.7285\n",
      "275/746, train_loss: 4.7688\n",
      "276/746, train_loss: 4.8781\n",
      "277/746, train_loss: 4.8831\n",
      "278/746, train_loss: 4.9144\n",
      "279/746, train_loss: 4.8554\n",
      "280/746, train_loss: 4.8805\n",
      "281/746, train_loss: 4.6198\n",
      "282/746, train_loss: 4.8923\n",
      "283/746, train_loss: 4.8390\n",
      "284/746, train_loss: 4.7821\n",
      "285/746, train_loss: 4.7844\n",
      "286/746, train_loss: 4.7947\n",
      "287/746, train_loss: 4.7064\n",
      "288/746, train_loss: 4.7504\n",
      "289/746, train_loss: 4.7253\n",
      "290/746, train_loss: 4.7834\n",
      "291/746, train_loss: 4.9003\n",
      "292/746, train_loss: 4.8553\n",
      "293/746, train_loss: 4.7497\n",
      "294/746, train_loss: 4.8128\n",
      "295/746, train_loss: 4.7320\n",
      "296/746, train_loss: 4.7468\n",
      "297/746, train_loss: 4.7876\n",
      "298/746, train_loss: 4.7320\n",
      "299/746, train_loss: 4.7637\n",
      "300/746, train_loss: 4.9481\n",
      "301/746, train_loss: 4.9165\n",
      "302/746, train_loss: 4.8163\n",
      "303/746, train_loss: 4.8961\n",
      "304/746, train_loss: 5.0148\n",
      "305/746, train_loss: 4.7465\n",
      "306/746, train_loss: 4.9342\n",
      "307/746, train_loss: 4.7660\n",
      "308/746, train_loss: 4.7176\n",
      "309/746, train_loss: 4.8693\n",
      "310/746, train_loss: 4.7023\n",
      "311/746, train_loss: 4.9646\n",
      "312/746, train_loss: 4.8296\n",
      "313/746, train_loss: 4.6802\n",
      "314/746, train_loss: 4.8419\n",
      "315/746, train_loss: 4.8951\n",
      "316/746, train_loss: 4.7478\n",
      "317/746, train_loss: 4.7544\n",
      "318/746, train_loss: 4.6983\n",
      "319/746, train_loss: 4.7684\n",
      "320/746, train_loss: 4.9077\n",
      "321/746, train_loss: 4.7767\n",
      "322/746, train_loss: 4.8999\n",
      "323/746, train_loss: 4.9649\n",
      "324/746, train_loss: 4.8060\n",
      "325/746, train_loss: 4.7962\n",
      "326/746, train_loss: 4.8482\n",
      "327/746, train_loss: 4.9664\n",
      "328/746, train_loss: 4.5958\n",
      "329/746, train_loss: 4.9013\n",
      "330/746, train_loss: 4.7161\n",
      "331/746, train_loss: 4.8709\n",
      "332/746, train_loss: 4.8922\n",
      "333/746, train_loss: 4.9009\n",
      "334/746, train_loss: 4.7997\n",
      "335/746, train_loss: 4.7704\n",
      "336/746, train_loss: 4.7445\n",
      "337/746, train_loss: 4.7664\n",
      "338/746, train_loss: 4.8643\n",
      "339/746, train_loss: 5.0574\n",
      "340/746, train_loss: 4.7669\n",
      "341/746, train_loss: 4.9171\n",
      "342/746, train_loss: 5.0737\n",
      "343/746, train_loss: 4.7432\n",
      "344/746, train_loss: 4.6226\n",
      "345/746, train_loss: 4.7331\n",
      "346/746, train_loss: 4.9197\n",
      "347/746, train_loss: 5.0853\n",
      "348/746, train_loss: 4.8100\n",
      "349/746, train_loss: 4.9135\n",
      "350/746, train_loss: 4.8005\n",
      "351/746, train_loss: 4.7729\n",
      "352/746, train_loss: 4.8379\n",
      "353/746, train_loss: 5.0633\n",
      "354/746, train_loss: 4.8090\n",
      "355/746, train_loss: 4.7458\n",
      "356/746, train_loss: 4.7877\n",
      "357/746, train_loss: 4.6046\n",
      "358/746, train_loss: 4.7938\n",
      "359/746, train_loss: 4.7888\n",
      "360/746, train_loss: 4.6917\n",
      "361/746, train_loss: 4.7944\n",
      "362/746, train_loss: 4.8735\n",
      "363/746, train_loss: 4.8522\n",
      "364/746, train_loss: 4.8543\n",
      "365/746, train_loss: 5.0313\n",
      "366/746, train_loss: 4.8778\n",
      "367/746, train_loss: 4.7758\n",
      "368/746, train_loss: 4.8738\n",
      "369/746, train_loss: 4.7979\n",
      "370/746, train_loss: 4.7697\n",
      "371/746, train_loss: 4.9261\n",
      "372/746, train_loss: 4.8555\n",
      "373/746, train_loss: 4.8605\n",
      "374/746, train_loss: 4.6263\n",
      "375/746, train_loss: 4.7373\n",
      "376/746, train_loss: 4.8814\n",
      "377/746, train_loss: 4.8764\n",
      "378/746, train_loss: 4.6667\n",
      "379/746, train_loss: 4.8914\n",
      "380/746, train_loss: 4.8244\n",
      "381/746, train_loss: 4.9093\n",
      "382/746, train_loss: 4.8600\n",
      "383/746, train_loss: 4.7793\n",
      "384/746, train_loss: 4.8401\n",
      "385/746, train_loss: 4.6584\n",
      "386/746, train_loss: 4.7998\n",
      "387/746, train_loss: 4.8042\n",
      "388/746, train_loss: 4.6206\n",
      "389/746, train_loss: 4.8449\n",
      "390/746, train_loss: 4.8210\n",
      "391/746, train_loss: 4.7470\n",
      "392/746, train_loss: 4.8398\n",
      "393/746, train_loss: 4.6405\n",
      "394/746, train_loss: 4.7279\n",
      "395/746, train_loss: 4.8197\n",
      "396/746, train_loss: 4.8178\n",
      "397/746, train_loss: 4.7439\n",
      "398/746, train_loss: 4.8038\n",
      "399/746, train_loss: 4.8468\n",
      "400/746, train_loss: 4.8885\n",
      "401/746, train_loss: 4.6137\n",
      "402/746, train_loss: 4.7947\n",
      "403/746, train_loss: 4.8377\n",
      "404/746, train_loss: 4.8208\n",
      "405/746, train_loss: 4.7074\n",
      "406/746, train_loss: 4.6568\n",
      "407/746, train_loss: 4.7569\n",
      "408/746, train_loss: 4.6032\n",
      "409/746, train_loss: 4.7419\n",
      "410/746, train_loss: 4.7806\n",
      "411/746, train_loss: 4.7658\n",
      "412/746, train_loss: 4.7271\n",
      "413/746, train_loss: 4.6314\n",
      "414/746, train_loss: 4.8699\n",
      "415/746, train_loss: 4.7148\n",
      "416/746, train_loss: 4.5540\n",
      "417/746, train_loss: 4.8335\n",
      "418/746, train_loss: 4.7691\n",
      "419/746, train_loss: 4.7854\n",
      "420/746, train_loss: 4.7570\n",
      "421/746, train_loss: 4.7938\n",
      "422/746, train_loss: 4.6935\n",
      "423/746, train_loss: 4.7997\n",
      "424/746, train_loss: 4.9108\n",
      "425/746, train_loss: 4.7343\n",
      "426/746, train_loss: 4.8234\n",
      "427/746, train_loss: 4.7914\n",
      "428/746, train_loss: 4.7916\n",
      "429/746, train_loss: 4.7352\n",
      "430/746, train_loss: 4.7606\n",
      "431/746, train_loss: 4.7542\n",
      "432/746, train_loss: 4.8249\n",
      "433/746, train_loss: 4.7280\n",
      "434/746, train_loss: 4.5764\n",
      "435/746, train_loss: 4.5739\n",
      "436/746, train_loss: 4.6877\n",
      "437/746, train_loss: 4.8978\n",
      "438/746, train_loss: 4.8784\n",
      "439/746, train_loss: 4.8190\n",
      "440/746, train_loss: 4.8488\n",
      "441/746, train_loss: 4.7184\n",
      "442/746, train_loss: 4.7050\n",
      "443/746, train_loss: 4.6948\n",
      "444/746, train_loss: 4.6127\n",
      "445/746, train_loss: 4.5897\n",
      "446/746, train_loss: 4.6455\n",
      "447/746, train_loss: 4.7550\n",
      "448/746, train_loss: 4.7177\n",
      "449/746, train_loss: 4.6622\n",
      "450/746, train_loss: 4.7901\n",
      "451/746, train_loss: 4.8925\n",
      "452/746, train_loss: 4.7798\n",
      "453/746, train_loss: 4.8165\n",
      "454/746, train_loss: 4.6418\n",
      "455/746, train_loss: 4.8572\n",
      "456/746, train_loss: 4.7675\n",
      "457/746, train_loss: 4.6260\n",
      "458/746, train_loss: 4.7854\n",
      "459/746, train_loss: 4.6189\n",
      "460/746, train_loss: 4.5368\n",
      "461/746, train_loss: 4.9253\n",
      "462/746, train_loss: 4.8857\n",
      "463/746, train_loss: 4.7261\n",
      "464/746, train_loss: 4.7798\n",
      "465/746, train_loss: 4.8898\n",
      "466/746, train_loss: 4.7655\n",
      "467/746, train_loss: 4.8885\n",
      "468/746, train_loss: 4.6896\n",
      "469/746, train_loss: 4.8074\n",
      "470/746, train_loss: 4.9593\n",
      "471/746, train_loss: 4.6844\n",
      "472/746, train_loss: 4.7873\n",
      "473/746, train_loss: 4.7126\n",
      "474/746, train_loss: 4.6771\n",
      "475/746, train_loss: 4.7280\n",
      "476/746, train_loss: 4.8332\n",
      "477/746, train_loss: 4.8691\n",
      "478/746, train_loss: 4.5816\n",
      "479/746, train_loss: 4.8797\n",
      "480/746, train_loss: 4.6501\n",
      "481/746, train_loss: 4.7030\n",
      "482/746, train_loss: 4.9067\n",
      "483/746, train_loss: 4.6992\n",
      "484/746, train_loss: 4.8302\n",
      "485/746, train_loss: 4.8003\n",
      "486/746, train_loss: 4.8730\n",
      "487/746, train_loss: 4.7672\n",
      "488/746, train_loss: 4.6415\n",
      "489/746, train_loss: 4.8161\n",
      "490/746, train_loss: 4.8046\n",
      "491/746, train_loss: 4.7213\n",
      "492/746, train_loss: 4.7166\n",
      "493/746, train_loss: 4.8434\n",
      "494/746, train_loss: 4.7834\n",
      "495/746, train_loss: 4.9181\n",
      "496/746, train_loss: 4.6665\n",
      "497/746, train_loss: 4.6905\n",
      "498/746, train_loss: 4.6801\n",
      "499/746, train_loss: 4.7830\n",
      "500/746, train_loss: 4.7803\n",
      "501/746, train_loss: 4.7432\n",
      "502/746, train_loss: 4.7083\n",
      "503/746, train_loss: 4.7628\n",
      "504/746, train_loss: 4.8602\n",
      "505/746, train_loss: 4.6543\n",
      "506/746, train_loss: 4.7615\n",
      "507/746, train_loss: 4.5789\n",
      "508/746, train_loss: 4.7654\n",
      "509/746, train_loss: 4.8095\n",
      "510/746, train_loss: 4.7597\n",
      "511/746, train_loss: 4.7275\n",
      "512/746, train_loss: 4.9375\n",
      "513/746, train_loss: 4.7253\n",
      "514/746, train_loss: 4.7983\n",
      "515/746, train_loss: 4.6589\n",
      "516/746, train_loss: 4.6934\n",
      "517/746, train_loss: 4.8009\n",
      "518/746, train_loss: 4.8211\n",
      "519/746, train_loss: 4.8648\n",
      "520/746, train_loss: 4.6193\n",
      "521/746, train_loss: 4.8017\n",
      "522/746, train_loss: 4.6508\n",
      "523/746, train_loss: 4.6337\n",
      "524/746, train_loss: 4.7510\n",
      "525/746, train_loss: 4.5981\n",
      "526/746, train_loss: 4.5970\n",
      "527/746, train_loss: 4.6192\n",
      "528/746, train_loss: 4.7816\n",
      "529/746, train_loss: 4.6883\n",
      "530/746, train_loss: 4.6928\n",
      "531/746, train_loss: 4.8171\n",
      "532/746, train_loss: 4.8194\n",
      "533/746, train_loss: 4.7529\n",
      "534/746, train_loss: 4.7717\n",
      "535/746, train_loss: 4.8051\n",
      "536/746, train_loss: 4.8032\n",
      "537/746, train_loss: 4.6613\n",
      "538/746, train_loss: 4.5352\n",
      "539/746, train_loss: 4.5964\n",
      "540/746, train_loss: 4.6536\n",
      "541/746, train_loss: 4.7246\n",
      "542/746, train_loss: 4.7734\n",
      "543/746, train_loss: 4.9080\n",
      "544/746, train_loss: 4.8505\n",
      "545/746, train_loss: 4.7365\n",
      "546/746, train_loss: 4.6945\n",
      "547/746, train_loss: 4.6961\n",
      "548/746, train_loss: 4.8001\n",
      "549/746, train_loss: 4.6440\n",
      "550/746, train_loss: 4.7398\n",
      "551/746, train_loss: 4.7942\n",
      "552/746, train_loss: 4.6950\n",
      "553/746, train_loss: 4.6767\n",
      "554/746, train_loss: 4.7640\n",
      "555/746, train_loss: 4.7181\n",
      "556/746, train_loss: 4.7393\n",
      "557/746, train_loss: 4.8547\n",
      "558/746, train_loss: 4.6610\n",
      "559/746, train_loss: 4.8091\n",
      "560/746, train_loss: 4.7845\n",
      "561/746, train_loss: 4.5543\n",
      "562/746, train_loss: 4.8831\n",
      "563/746, train_loss: 4.7833\n",
      "564/746, train_loss: 4.7521\n",
      "565/746, train_loss: 4.6298\n",
      "566/746, train_loss: 4.7344\n",
      "567/746, train_loss: 4.6142\n",
      "568/746, train_loss: 4.6457\n",
      "569/746, train_loss: 4.5466\n",
      "570/746, train_loss: 4.7487\n",
      "571/746, train_loss: 4.6169\n",
      "572/746, train_loss: 4.8662\n",
      "573/746, train_loss: 4.9030\n",
      "574/746, train_loss: 4.7383\n",
      "575/746, train_loss: 4.7031\n",
      "576/746, train_loss: 4.8047\n",
      "577/746, train_loss: 4.7773\n",
      "578/746, train_loss: 4.7455\n",
      "579/746, train_loss: 4.7181\n",
      "580/746, train_loss: 4.6675\n",
      "581/746, train_loss: 4.7928\n",
      "582/746, train_loss: 4.8031\n",
      "583/746, train_loss: 4.8136\n",
      "584/746, train_loss: 4.6510\n",
      "585/746, train_loss: 4.8224\n",
      "586/746, train_loss: 4.8068\n",
      "587/746, train_loss: 4.7394\n",
      "588/746, train_loss: 4.7510\n",
      "589/746, train_loss: 4.8347\n",
      "590/746, train_loss: 4.6839\n",
      "591/746, train_loss: 4.7637\n",
      "592/746, train_loss: 4.6033\n",
      "593/746, train_loss: 4.6185\n",
      "594/746, train_loss: 4.6683\n",
      "595/746, train_loss: 4.8831\n",
      "596/746, train_loss: 4.6708\n",
      "597/746, train_loss: 4.6641\n",
      "598/746, train_loss: 4.5434\n",
      "599/746, train_loss: 4.6232\n",
      "600/746, train_loss: 4.7519\n",
      "601/746, train_loss: 4.8422\n",
      "602/746, train_loss: 4.5370\n",
      "603/746, train_loss: 4.5434\n",
      "604/746, train_loss: 4.6369\n",
      "605/746, train_loss: 4.6728\n",
      "606/746, train_loss: 4.5907\n",
      "607/746, train_loss: 4.6803\n",
      "608/746, train_loss: 4.7860\n",
      "609/746, train_loss: 4.7821\n",
      "610/746, train_loss: 4.6957\n",
      "611/746, train_loss: 4.9417\n",
      "612/746, train_loss: 4.5618\n",
      "613/746, train_loss: 4.6901\n",
      "614/746, train_loss: 4.7935\n",
      "615/746, train_loss: 4.8023\n",
      "616/746, train_loss: 4.6817\n",
      "617/746, train_loss: 4.6488\n",
      "618/746, train_loss: 4.7662\n",
      "619/746, train_loss: 4.6764\n",
      "620/746, train_loss: 4.8042\n",
      "621/746, train_loss: 4.7763\n",
      "622/746, train_loss: 5.0887\n",
      "623/746, train_loss: 4.7084\n",
      "624/746, train_loss: 4.8020\n",
      "625/746, train_loss: 4.7310\n",
      "626/746, train_loss: 4.8065\n",
      "627/746, train_loss: 4.7459\n",
      "628/746, train_loss: 4.4414\n",
      "629/746, train_loss: 4.7625\n",
      "630/746, train_loss: 4.6351\n",
      "631/746, train_loss: 4.6203\n",
      "632/746, train_loss: 4.7612\n",
      "633/746, train_loss: 4.7582\n",
      "634/746, train_loss: 4.6798\n",
      "635/746, train_loss: 4.6787\n",
      "636/746, train_loss: 4.7040\n",
      "637/746, train_loss: 4.8073\n",
      "638/746, train_loss: 4.8642\n",
      "639/746, train_loss: 4.7325\n",
      "640/746, train_loss: 4.8039\n",
      "641/746, train_loss: 4.6556\n",
      "642/746, train_loss: 4.7136\n",
      "643/746, train_loss: 4.7133\n",
      "644/746, train_loss: 4.7198\n",
      "645/746, train_loss: 4.7114\n",
      "646/746, train_loss: 4.6650\n",
      "647/746, train_loss: 4.7446\n",
      "648/746, train_loss: 4.6431\n",
      "649/746, train_loss: 4.7456\n",
      "650/746, train_loss: 4.6892\n",
      "651/746, train_loss: 4.6488\n",
      "652/746, train_loss: 4.4469\n",
      "653/746, train_loss: 4.7485\n",
      "654/746, train_loss: 4.6209\n",
      "655/746, train_loss: 4.6683\n",
      "656/746, train_loss: 4.6494\n",
      "657/746, train_loss: 4.5561\n",
      "658/746, train_loss: 4.6453\n",
      "659/746, train_loss: 4.6320\n",
      "660/746, train_loss: 4.5457\n",
      "661/746, train_loss: 4.7204\n",
      "662/746, train_loss: 4.8786\n",
      "663/746, train_loss: 4.7599\n",
      "664/746, train_loss: 4.7531\n",
      "665/746, train_loss: 4.8028\n",
      "666/746, train_loss: 4.7666\n",
      "667/746, train_loss: 4.7686\n",
      "668/746, train_loss: 4.7661\n",
      "669/746, train_loss: 4.7453\n",
      "670/746, train_loss: 4.6627\n",
      "671/746, train_loss: 4.6551\n",
      "672/746, train_loss: 4.7743\n",
      "673/746, train_loss: 4.5625\n",
      "674/746, train_loss: 4.3751\n",
      "675/746, train_loss: 4.8129\n",
      "676/746, train_loss: 4.7446\n",
      "677/746, train_loss: 4.6958\n",
      "678/746, train_loss: 4.6831\n",
      "679/746, train_loss: 4.8413\n",
      "680/746, train_loss: 4.7592\n",
      "681/746, train_loss: 4.6200\n",
      "682/746, train_loss: 4.7389\n",
      "683/746, train_loss: 4.6447\n",
      "684/746, train_loss: 4.6726\n",
      "685/746, train_loss: 4.6840\n",
      "686/746, train_loss: 4.6789\n",
      "687/746, train_loss: 4.6956\n",
      "688/746, train_loss: 4.6785\n",
      "689/746, train_loss: 4.6617\n",
      "690/746, train_loss: 4.7715\n",
      "691/746, train_loss: 4.4885\n",
      "692/746, train_loss: 4.6623\n",
      "693/746, train_loss: 4.7789\n",
      "694/746, train_loss: 4.7472\n",
      "695/746, train_loss: 4.6590\n",
      "696/746, train_loss: 4.7026\n",
      "697/746, train_loss: 4.6749\n",
      "698/746, train_loss: 4.7206\n",
      "699/746, train_loss: 4.6606\n",
      "700/746, train_loss: 4.7368\n",
      "701/746, train_loss: 4.7061\n",
      "702/746, train_loss: 4.7001\n",
      "703/746, train_loss: 4.5514\n",
      "704/746, train_loss: 4.6567\n",
      "705/746, train_loss: 4.8975\n",
      "706/746, train_loss: 4.5806\n",
      "707/746, train_loss: 4.7513\n",
      "708/746, train_loss: 4.7663\n",
      "709/746, train_loss: 4.8499\n",
      "710/746, train_loss: 4.6970\n",
      "711/746, train_loss: 4.4932\n",
      "712/746, train_loss: 4.8263\n",
      "713/746, train_loss: 4.7320\n",
      "714/746, train_loss: 4.6343\n",
      "715/746, train_loss: 4.4647\n",
      "716/746, train_loss: 4.6836\n",
      "717/746, train_loss: 4.6383\n",
      "718/746, train_loss: 4.7324\n",
      "719/746, train_loss: 4.5752\n",
      "720/746, train_loss: 4.5675\n",
      "721/746, train_loss: 4.6070\n",
      "722/746, train_loss: 4.6783\n",
      "723/746, train_loss: 4.6663\n",
      "724/746, train_loss: 4.6665\n",
      "725/746, train_loss: 4.7323\n",
      "726/746, train_loss: 4.6146\n",
      "727/746, train_loss: 4.7255\n",
      "728/746, train_loss: 4.5647\n",
      "729/746, train_loss: 4.8958\n",
      "730/746, train_loss: 4.7853\n",
      "731/746, train_loss: 4.4934\n",
      "732/746, train_loss: 4.8040\n",
      "733/746, train_loss: 4.8550\n",
      "734/746, train_loss: 4.7282\n",
      "735/746, train_loss: 4.5857\n",
      "736/746, train_loss: 4.6013\n",
      "737/746, train_loss: 4.7105\n",
      "738/746, train_loss: 4.4991\n",
      "739/746, train_loss: 4.7265\n",
      "740/746, train_loss: 4.6488\n",
      "741/746, train_loss: 4.5844\n",
      "742/746, train_loss: 4.8617\n",
      "743/746, train_loss: 4.6180\n",
      "744/746, train_loss: 4.5637\n",
      "745/746, train_loss: 4.6409\n",
      "746/746, train_loss: 4.9329\n",
      "747/746, train_loss: 4.4250\n",
      "epoch 5 average loss: 4.7852\n",
      "saved new best metric model\n",
      "current epoch: 5 current AUC: 0.9192 current accuracy: 0.2445 best AUC: 0.2445 at epoch: 5\n",
      "----------\n",
      "epoch 6/10\n",
      "1/746, train_loss: 4.5100\n",
      "2/746, train_loss: 4.5984\n",
      "3/746, train_loss: 4.5902\n",
      "4/746, train_loss: 4.8230\n",
      "5/746, train_loss: 4.6519\n",
      "6/746, train_loss: 4.3834\n",
      "7/746, train_loss: 4.5863\n",
      "8/746, train_loss: 4.5898\n",
      "9/746, train_loss: 4.6444\n",
      "10/746, train_loss: 4.4253\n",
      "11/746, train_loss: 4.5806\n",
      "12/746, train_loss: 4.5862\n",
      "13/746, train_loss: 4.5271\n",
      "14/746, train_loss: 4.5964\n",
      "15/746, train_loss: 4.4338\n",
      "16/746, train_loss: 4.4071\n",
      "17/746, train_loss: 4.4724\n",
      "18/746, train_loss: 4.4497\n",
      "19/746, train_loss: 4.6925\n",
      "20/746, train_loss: 4.6879\n",
      "21/746, train_loss: 4.6038\n",
      "22/746, train_loss: 4.6355\n",
      "23/746, train_loss: 4.7154\n",
      "24/746, train_loss: 4.4318\n",
      "25/746, train_loss: 4.6227\n",
      "26/746, train_loss: 4.5469\n",
      "27/746, train_loss: 4.6347\n",
      "28/746, train_loss: 4.5783\n",
      "29/746, train_loss: 4.6291\n",
      "30/746, train_loss: 4.5817\n",
      "31/746, train_loss: 4.4656\n",
      "32/746, train_loss: 4.4816\n",
      "33/746, train_loss: 4.7621\n",
      "34/746, train_loss: 4.4425\n",
      "35/746, train_loss: 4.7331\n",
      "36/746, train_loss: 4.7487\n",
      "37/746, train_loss: 4.5471\n",
      "38/746, train_loss: 4.6235\n",
      "39/746, train_loss: 4.4549\n",
      "40/746, train_loss: 4.5822\n",
      "41/746, train_loss: 4.7424\n",
      "42/746, train_loss: 4.6199\n",
      "43/746, train_loss: 4.5010\n",
      "44/746, train_loss: 4.6189\n",
      "45/746, train_loss: 4.4310\n",
      "46/746, train_loss: 4.5572\n",
      "47/746, train_loss: 4.6680\n",
      "48/746, train_loss: 4.4441\n",
      "49/746, train_loss: 4.6276\n",
      "50/746, train_loss: 4.4464\n",
      "51/746, train_loss: 4.6814\n",
      "52/746, train_loss: 4.7163\n",
      "53/746, train_loss: 4.5185\n",
      "54/746, train_loss: 4.8022\n",
      "55/746, train_loss: 4.5124\n",
      "56/746, train_loss: 4.4715\n",
      "57/746, train_loss: 4.5009\n",
      "58/746, train_loss: 4.4559\n",
      "59/746, train_loss: 4.6398\n",
      "60/746, train_loss: 4.4538\n",
      "61/746, train_loss: 4.8468\n",
      "62/746, train_loss: 4.4701\n",
      "63/746, train_loss: 4.6720\n",
      "64/746, train_loss: 4.8462\n",
      "65/746, train_loss: 4.6220\n",
      "66/746, train_loss: 4.4533\n",
      "67/746, train_loss: 4.6059\n",
      "68/746, train_loss: 4.3685\n",
      "69/746, train_loss: 4.6183\n",
      "70/746, train_loss: 4.6248\n",
      "71/746, train_loss: 4.6566\n",
      "72/746, train_loss: 4.6429\n",
      "73/746, train_loss: 4.4521\n",
      "74/746, train_loss: 4.3931\n",
      "75/746, train_loss: 4.6348\n",
      "76/746, train_loss: 4.5451\n",
      "77/746, train_loss: 4.4996\n",
      "78/746, train_loss: 4.5023\n",
      "79/746, train_loss: 4.4625\n",
      "80/746, train_loss: 4.6406\n",
      "81/746, train_loss: 4.5734\n",
      "82/746, train_loss: 4.6797\n",
      "83/746, train_loss: 4.5927\n",
      "84/746, train_loss: 4.6453\n",
      "85/746, train_loss: 4.6965\n",
      "86/746, train_loss: 4.7129\n",
      "87/746, train_loss: 4.4595\n",
      "88/746, train_loss: 4.5150\n",
      "89/746, train_loss: 4.6231\n",
      "90/746, train_loss: 4.5921\n",
      "91/746, train_loss: 4.5515\n",
      "92/746, train_loss: 4.7481\n",
      "93/746, train_loss: 4.5996\n",
      "94/746, train_loss: 4.7597\n",
      "95/746, train_loss: 4.5553\n",
      "96/746, train_loss: 4.5504\n",
      "97/746, train_loss: 4.4218\n",
      "98/746, train_loss: 4.6827\n",
      "99/746, train_loss: 4.6249\n",
      "100/746, train_loss: 4.5529\n",
      "101/746, train_loss: 4.7851\n",
      "102/746, train_loss: 4.5720\n",
      "103/746, train_loss: 4.5858\n",
      "104/746, train_loss: 4.5617\n",
      "105/746, train_loss: 4.5301\n",
      "106/746, train_loss: 4.4412\n",
      "107/746, train_loss: 4.5433\n",
      "108/746, train_loss: 4.4085\n",
      "109/746, train_loss: 4.4416\n",
      "110/746, train_loss: 4.3836\n",
      "111/746, train_loss: 4.5249\n",
      "112/746, train_loss: 4.6708\n",
      "113/746, train_loss: 4.4642\n",
      "114/746, train_loss: 4.6545\n",
      "115/746, train_loss: 4.5530\n",
      "116/746, train_loss: 4.6478\n",
      "117/746, train_loss: 4.6246\n",
      "118/746, train_loss: 4.5168\n",
      "119/746, train_loss: 4.5512\n",
      "120/746, train_loss: 4.6562\n",
      "121/746, train_loss: 4.4653\n",
      "122/746, train_loss: 4.4068\n",
      "123/746, train_loss: 4.6383\n",
      "124/746, train_loss: 4.4470\n",
      "125/746, train_loss: 4.6134\n",
      "126/746, train_loss: 4.4669\n",
      "127/746, train_loss: 4.7987\n",
      "128/746, train_loss: 4.3044\n",
      "129/746, train_loss: 4.5554\n",
      "130/746, train_loss: 4.5943\n",
      "131/746, train_loss: 4.5945\n",
      "132/746, train_loss: 4.5437\n",
      "133/746, train_loss: 4.6942\n",
      "134/746, train_loss: 4.7832\n",
      "135/746, train_loss: 4.6071\n",
      "136/746, train_loss: 4.3965\n",
      "137/746, train_loss: 4.5446\n",
      "138/746, train_loss: 4.3475\n",
      "139/746, train_loss: 4.5812\n",
      "140/746, train_loss: 4.5424\n",
      "141/746, train_loss: 4.6643\n",
      "142/746, train_loss: 4.4315\n",
      "143/746, train_loss: 4.4968\n",
      "144/746, train_loss: 4.4821\n",
      "145/746, train_loss: 4.4584\n",
      "146/746, train_loss: 4.6443\n",
      "147/746, train_loss: 4.5370\n",
      "148/746, train_loss: 4.5165\n",
      "149/746, train_loss: 4.4619\n",
      "150/746, train_loss: 4.4300\n",
      "151/746, train_loss: 4.5649\n",
      "152/746, train_loss: 4.4315\n",
      "153/746, train_loss: 4.7011\n",
      "154/746, train_loss: 4.4271\n",
      "155/746, train_loss: 4.5752\n",
      "156/746, train_loss: 4.5188\n",
      "157/746, train_loss: 4.7227\n",
      "158/746, train_loss: 4.4257\n",
      "159/746, train_loss: 4.5599\n",
      "160/746, train_loss: 4.4602\n",
      "161/746, train_loss: 4.4370\n",
      "162/746, train_loss: 4.5719\n",
      "163/746, train_loss: 4.5856\n",
      "164/746, train_loss: 4.5747\n",
      "165/746, train_loss: 4.5629\n",
      "166/746, train_loss: 4.5765\n",
      "167/746, train_loss: 4.5114\n",
      "168/746, train_loss: 4.5956\n",
      "169/746, train_loss: 4.3863\n",
      "170/746, train_loss: 4.4671\n",
      "171/746, train_loss: 4.7407\n",
      "172/746, train_loss: 4.3310\n",
      "173/746, train_loss: 4.5095\n",
      "174/746, train_loss: 4.4128\n",
      "175/746, train_loss: 4.6290\n",
      "176/746, train_loss: 4.3997\n",
      "177/746, train_loss: 4.5272\n",
      "178/746, train_loss: 4.5023\n",
      "179/746, train_loss: 4.5497\n",
      "180/746, train_loss: 4.6432\n",
      "181/746, train_loss: 4.3742\n",
      "182/746, train_loss: 4.4230\n",
      "183/746, train_loss: 4.7851\n",
      "184/746, train_loss: 4.6702\n",
      "185/746, train_loss: 4.4768\n",
      "186/746, train_loss: 4.3931\n",
      "187/746, train_loss: 4.6006\n",
      "188/746, train_loss: 4.4742\n",
      "189/746, train_loss: 4.6422\n",
      "190/746, train_loss: 4.5148\n",
      "191/746, train_loss: 4.3145\n",
      "192/746, train_loss: 4.5832\n",
      "193/746, train_loss: 4.5796\n",
      "194/746, train_loss: 4.4712\n",
      "195/746, train_loss: 4.6939\n",
      "196/746, train_loss: 4.4686\n",
      "197/746, train_loss: 4.4486\n",
      "198/746, train_loss: 4.6529\n",
      "199/746, train_loss: 4.7060\n",
      "200/746, train_loss: 4.5313\n",
      "201/746, train_loss: 4.2839\n",
      "202/746, train_loss: 4.4600\n",
      "203/746, train_loss: 4.5595\n",
      "204/746, train_loss: 4.7710\n",
      "205/746, train_loss: 4.5625\n",
      "206/746, train_loss: 4.4744\n",
      "207/746, train_loss: 4.7231\n",
      "208/746, train_loss: 4.5245\n",
      "209/746, train_loss: 4.6154\n",
      "210/746, train_loss: 4.5905\n",
      "211/746, train_loss: 4.5390\n",
      "212/746, train_loss: 4.6522\n",
      "213/746, train_loss: 4.4443\n",
      "214/746, train_loss: 4.4324\n",
      "215/746, train_loss: 4.6970\n",
      "216/746, train_loss: 4.8001\n",
      "217/746, train_loss: 4.4041\n",
      "218/746, train_loss: 4.7206\n",
      "219/746, train_loss: 4.5468\n",
      "220/746, train_loss: 4.5277\n",
      "221/746, train_loss: 4.5958\n",
      "222/746, train_loss: 4.4325\n",
      "223/746, train_loss: 4.5899\n",
      "224/746, train_loss: 4.6932\n",
      "225/746, train_loss: 4.5037\n",
      "226/746, train_loss: 4.5360\n",
      "227/746, train_loss: 4.5499\n",
      "228/746, train_loss: 4.4928\n",
      "229/746, train_loss: 4.4444\n",
      "230/746, train_loss: 4.4423\n",
      "231/746, train_loss: 4.4190\n",
      "232/746, train_loss: 4.5318\n",
      "233/746, train_loss: 4.4924\n",
      "234/746, train_loss: 4.5333\n",
      "235/746, train_loss: 4.4352\n",
      "236/746, train_loss: 4.5020\n",
      "237/746, train_loss: 4.4861\n",
      "238/746, train_loss: 4.6300\n",
      "239/746, train_loss: 4.4950\n",
      "240/746, train_loss: 4.4585\n",
      "241/746, train_loss: 4.5760\n",
      "242/746, train_loss: 4.5557\n",
      "243/746, train_loss: 4.7578\n",
      "244/746, train_loss: 4.5138\n",
      "245/746, train_loss: 4.4251\n",
      "246/746, train_loss: 4.5478\n",
      "247/746, train_loss: 4.4663\n",
      "248/746, train_loss: 4.4172\n",
      "249/746, train_loss: 4.5288\n",
      "250/746, train_loss: 4.4001\n",
      "251/746, train_loss: 4.6080\n",
      "252/746, train_loss: 4.5283\n",
      "253/746, train_loss: 4.5958\n",
      "254/746, train_loss: 4.4696\n",
      "255/746, train_loss: 4.6109\n",
      "256/746, train_loss: 4.5648\n",
      "257/746, train_loss: 4.5464\n",
      "258/746, train_loss: 4.5037\n",
      "259/746, train_loss: 4.6532\n",
      "260/746, train_loss: 4.4062\n",
      "261/746, train_loss: 4.5240\n",
      "262/746, train_loss: 4.4663\n",
      "263/746, train_loss: 4.6160\n",
      "264/746, train_loss: 4.7553\n",
      "265/746, train_loss: 4.5853\n",
      "266/746, train_loss: 4.6345\n",
      "267/746, train_loss: 4.5742\n",
      "268/746, train_loss: 4.6948\n",
      "269/746, train_loss: 4.5793\n",
      "270/746, train_loss: 4.4508\n",
      "271/746, train_loss: 4.5144\n",
      "272/746, train_loss: 4.6974\n",
      "273/746, train_loss: 4.6364\n",
      "274/746, train_loss: 4.2593\n",
      "275/746, train_loss: 4.5289\n",
      "276/746, train_loss: 4.4019\n",
      "277/746, train_loss: 4.2908\n",
      "278/746, train_loss: 4.6636\n",
      "279/746, train_loss: 4.5540\n",
      "280/746, train_loss: 4.6756\n",
      "281/746, train_loss: 4.4916\n",
      "282/746, train_loss: 4.6497\n",
      "283/746, train_loss: 4.7255\n",
      "284/746, train_loss: 4.5187\n",
      "285/746, train_loss: 4.5509\n",
      "286/746, train_loss: 4.6567\n",
      "287/746, train_loss: 4.3024\n",
      "288/746, train_loss: 4.5254\n",
      "289/746, train_loss: 4.6162\n",
      "290/746, train_loss: 4.5186\n",
      "291/746, train_loss: 4.5262\n",
      "292/746, train_loss: 4.5165\n",
      "293/746, train_loss: 4.6298\n",
      "294/746, train_loss: 4.5750\n",
      "295/746, train_loss: 4.5169\n",
      "296/746, train_loss: 4.5457\n",
      "297/746, train_loss: 4.3282\n",
      "298/746, train_loss: 4.5286\n",
      "299/746, train_loss: 4.6065\n",
      "300/746, train_loss: 4.5804\n",
      "301/746, train_loss: 4.5325\n",
      "302/746, train_loss: 4.4911\n",
      "303/746, train_loss: 4.6084\n",
      "304/746, train_loss: 4.5088\n",
      "305/746, train_loss: 4.5505\n",
      "306/746, train_loss: 4.4157\n",
      "307/746, train_loss: 4.4087\n",
      "308/746, train_loss: 4.5561\n",
      "309/746, train_loss: 4.5442\n",
      "310/746, train_loss: 4.6383\n",
      "311/746, train_loss: 4.4172\n",
      "312/746, train_loss: 4.1907\n",
      "313/746, train_loss: 4.5335\n",
      "314/746, train_loss: 4.5200\n",
      "315/746, train_loss: 4.7467\n",
      "316/746, train_loss: 4.6167\n",
      "317/746, train_loss: 4.5189\n",
      "318/746, train_loss: 4.3279\n",
      "319/746, train_loss: 4.5226\n",
      "320/746, train_loss: 4.3831\n",
      "321/746, train_loss: 4.2158\n",
      "322/746, train_loss: 4.6542\n",
      "323/746, train_loss: 4.2325\n",
      "324/746, train_loss: 4.6559\n",
      "325/746, train_loss: 4.5074\n",
      "326/746, train_loss: 4.3322\n",
      "327/746, train_loss: 4.4025\n",
      "328/746, train_loss: 4.5145\n",
      "329/746, train_loss: 4.5637\n",
      "330/746, train_loss: 4.5983\n",
      "331/746, train_loss: 4.7165\n",
      "332/746, train_loss: 4.5741\n",
      "333/746, train_loss: 4.5838\n",
      "334/746, train_loss: 4.4622\n",
      "335/746, train_loss: 4.6163\n",
      "336/746, train_loss: 4.5019\n",
      "337/746, train_loss: 4.3750\n",
      "338/746, train_loss: 4.5828\n",
      "339/746, train_loss: 4.5731\n",
      "340/746, train_loss: 4.6236\n",
      "341/746, train_loss: 4.4345\n",
      "342/746, train_loss: 4.4164\n",
      "343/746, train_loss: 4.6078\n",
      "344/746, train_loss: 4.4898\n",
      "345/746, train_loss: 4.6251\n",
      "346/746, train_loss: 4.3267\n",
      "347/746, train_loss: 4.4287\n",
      "348/746, train_loss: 4.4348\n",
      "349/746, train_loss: 4.6016\n",
      "350/746, train_loss: 4.4614\n",
      "351/746, train_loss: 4.4840\n",
      "352/746, train_loss: 4.4477\n",
      "353/746, train_loss: 4.4060\n",
      "354/746, train_loss: 4.4220\n",
      "355/746, train_loss: 4.4391\n",
      "356/746, train_loss: 4.3436\n",
      "357/746, train_loss: 4.3175\n",
      "358/746, train_loss: 4.2696\n",
      "359/746, train_loss: 4.3476\n",
      "360/746, train_loss: 4.2620\n",
      "361/746, train_loss: 4.3727\n",
      "362/746, train_loss: 4.5489\n",
      "363/746, train_loss: 4.4065\n",
      "364/746, train_loss: 4.4674\n",
      "365/746, train_loss: 4.5066\n",
      "366/746, train_loss: 4.6504\n",
      "367/746, train_loss: 4.4113\n",
      "368/746, train_loss: 4.4456\n",
      "369/746, train_loss: 4.3573\n",
      "370/746, train_loss: 4.3303\n",
      "371/746, train_loss: 4.5896\n",
      "372/746, train_loss: 4.2665\n",
      "373/746, train_loss: 4.5839\n",
      "374/746, train_loss: 4.4766\n",
      "375/746, train_loss: 4.4854\n",
      "376/746, train_loss: 4.5958\n",
      "377/746, train_loss: 4.4345\n",
      "378/746, train_loss: 4.5173\n",
      "379/746, train_loss: 4.4966\n",
      "380/746, train_loss: 4.5722\n",
      "381/746, train_loss: 4.3439\n",
      "382/746, train_loss: 4.4199\n",
      "383/746, train_loss: 4.6569\n",
      "384/746, train_loss: 4.5790\n",
      "385/746, train_loss: 4.6650\n",
      "386/746, train_loss: 4.5563\n",
      "387/746, train_loss: 4.4198\n",
      "388/746, train_loss: 4.4046\n",
      "389/746, train_loss: 4.5762\n",
      "390/746, train_loss: 4.5295\n",
      "391/746, train_loss: 4.4132\n",
      "392/746, train_loss: 4.3816\n",
      "393/746, train_loss: 4.4096\n",
      "394/746, train_loss: 4.5666\n",
      "395/746, train_loss: 4.5024\n",
      "396/746, train_loss: 4.5462\n",
      "397/746, train_loss: 4.3907\n",
      "398/746, train_loss: 4.5508\n",
      "399/746, train_loss: 4.3975\n",
      "400/746, train_loss: 4.5020\n",
      "401/746, train_loss: 4.4614\n",
      "402/746, train_loss: 4.6133\n",
      "403/746, train_loss: 4.3848\n",
      "404/746, train_loss: 4.4677\n",
      "405/746, train_loss: 4.3535\n",
      "406/746, train_loss: 4.4261\n",
      "407/746, train_loss: 4.4919\n",
      "408/746, train_loss: 4.6717\n",
      "409/746, train_loss: 4.4504\n",
      "410/746, train_loss: 4.6246\n",
      "411/746, train_loss: 4.5700\n",
      "412/746, train_loss: 4.3558\n",
      "413/746, train_loss: 4.4585\n",
      "414/746, train_loss: 4.3283\n",
      "415/746, train_loss: 4.4646\n",
      "416/746, train_loss: 4.4315\n",
      "417/746, train_loss: 4.4697\n",
      "418/746, train_loss: 4.3901\n",
      "419/746, train_loss: 4.6680\n",
      "420/746, train_loss: 4.5692\n",
      "421/746, train_loss: 4.5456\n",
      "422/746, train_loss: 4.4303\n",
      "423/746, train_loss: 4.5366\n",
      "424/746, train_loss: 4.6919\n",
      "425/746, train_loss: 4.4328\n",
      "426/746, train_loss: 4.3763\n",
      "427/746, train_loss: 4.4368\n",
      "428/746, train_loss: 4.3478\n",
      "429/746, train_loss: 4.4249\n",
      "430/746, train_loss: 4.3268\n",
      "431/746, train_loss: 4.5602\n",
      "432/746, train_loss: 4.5848\n",
      "433/746, train_loss: 4.3167\n",
      "434/746, train_loss: 4.3100\n",
      "435/746, train_loss: 4.5542\n",
      "436/746, train_loss: 4.2883\n",
      "437/746, train_loss: 4.3409\n",
      "438/746, train_loss: 4.6031\n",
      "439/746, train_loss: 4.4435\n",
      "440/746, train_loss: 4.5978\n",
      "441/746, train_loss: 4.5472\n",
      "442/746, train_loss: 4.4876\n",
      "443/746, train_loss: 4.6320\n",
      "444/746, train_loss: 4.2671\n",
      "445/746, train_loss: 4.4368\n",
      "446/746, train_loss: 4.4959\n",
      "447/746, train_loss: 4.4990\n",
      "448/746, train_loss: 4.4742\n",
      "449/746, train_loss: 4.3044\n",
      "450/746, train_loss: 4.5258\n",
      "451/746, train_loss: 4.3108\n",
      "452/746, train_loss: 4.5840\n",
      "453/746, train_loss: 4.3311\n",
      "454/746, train_loss: 4.3394\n",
      "455/746, train_loss: 4.5552\n",
      "456/746, train_loss: 4.3380\n",
      "457/746, train_loss: 4.3238\n",
      "458/746, train_loss: 4.3818\n",
      "459/746, train_loss: 4.5538\n",
      "460/746, train_loss: 4.3535\n",
      "461/746, train_loss: 4.4775\n",
      "462/746, train_loss: 4.3200\n",
      "463/746, train_loss: 4.4204\n",
      "464/746, train_loss: 4.3099\n",
      "465/746, train_loss: 4.3391\n",
      "466/746, train_loss: 4.4098\n",
      "467/746, train_loss: 4.2802\n",
      "468/746, train_loss: 4.4531\n",
      "469/746, train_loss: 4.2910\n",
      "470/746, train_loss: 4.5124\n",
      "471/746, train_loss: 4.6655\n",
      "472/746, train_loss: 4.4357\n",
      "473/746, train_loss: 4.5680\n",
      "474/746, train_loss: 4.3610\n",
      "475/746, train_loss: 4.4596\n",
      "476/746, train_loss: 4.6372\n",
      "477/746, train_loss: 4.4135\n",
      "478/746, train_loss: 4.4892\n",
      "479/746, train_loss: 4.5857\n",
      "480/746, train_loss: 4.3892\n",
      "481/746, train_loss: 4.2842\n",
      "482/746, train_loss: 4.1531\n",
      "483/746, train_loss: 4.4390\n",
      "484/746, train_loss: 4.5102\n",
      "485/746, train_loss: 4.5005\n",
      "486/746, train_loss: 4.3630\n",
      "487/746, train_loss: 4.4862\n",
      "488/746, train_loss: 4.3613\n",
      "489/746, train_loss: 4.6265\n",
      "490/746, train_loss: 4.6158\n",
      "491/746, train_loss: 4.5542\n",
      "492/746, train_loss: 4.5828\n",
      "493/746, train_loss: 4.4000\n",
      "494/746, train_loss: 4.3796\n",
      "495/746, train_loss: 4.5012\n",
      "496/746, train_loss: 4.3664\n",
      "497/746, train_loss: 4.4556\n",
      "498/746, train_loss: 4.4484\n",
      "499/746, train_loss: 4.5099\n",
      "500/746, train_loss: 4.3518\n",
      "501/746, train_loss: 4.4095\n",
      "502/746, train_loss: 4.2575\n",
      "503/746, train_loss: 4.4392\n",
      "504/746, train_loss: 4.5389\n",
      "505/746, train_loss: 4.3774\n",
      "506/746, train_loss: 4.4965\n",
      "507/746, train_loss: 4.3933\n",
      "508/746, train_loss: 4.3833\n",
      "509/746, train_loss: 4.5779\n",
      "510/746, train_loss: 4.5236\n",
      "511/746, train_loss: 4.5404\n",
      "512/746, train_loss: 4.5411\n",
      "513/746, train_loss: 4.4727\n",
      "514/746, train_loss: 4.3424\n",
      "515/746, train_loss: 4.3624\n",
      "516/746, train_loss: 4.2357\n",
      "517/746, train_loss: 4.3506\n",
      "518/746, train_loss: 4.5803\n",
      "519/746, train_loss: 4.4527\n",
      "520/746, train_loss: 4.5151\n",
      "521/746, train_loss: 4.3300\n",
      "522/746, train_loss: 4.5428\n",
      "523/746, train_loss: 4.2116\n",
      "524/746, train_loss: 4.2378\n",
      "525/746, train_loss: 4.5329\n",
      "526/746, train_loss: 4.3109\n",
      "527/746, train_loss: 4.2573\n",
      "528/746, train_loss: 4.4327\n",
      "529/746, train_loss: 4.5128\n",
      "530/746, train_loss: 4.4033\n",
      "531/746, train_loss: 4.3036\n",
      "532/746, train_loss: 4.1507\n",
      "533/746, train_loss: 4.3023\n",
      "534/746, train_loss: 4.2931\n",
      "535/746, train_loss: 4.4998\n",
      "536/746, train_loss: 4.4952\n",
      "537/746, train_loss: 4.4561\n",
      "538/746, train_loss: 4.6217\n",
      "539/746, train_loss: 4.4239\n",
      "540/746, train_loss: 4.3949\n",
      "541/746, train_loss: 4.5725\n",
      "542/746, train_loss: 4.3340\n",
      "543/746, train_loss: 4.4144\n",
      "544/746, train_loss: 4.6404\n",
      "545/746, train_loss: 4.4177\n",
      "546/746, train_loss: 4.4236\n",
      "547/746, train_loss: 4.2687\n",
      "548/746, train_loss: 4.4481\n",
      "549/746, train_loss: 4.3663\n",
      "550/746, train_loss: 4.5161\n",
      "551/746, train_loss: 4.4070\n",
      "552/746, train_loss: 4.4910\n",
      "553/746, train_loss: 4.1911\n",
      "554/746, train_loss: 4.4968\n",
      "555/746, train_loss: 4.4873\n",
      "556/746, train_loss: 4.3571\n",
      "557/746, train_loss: 4.3488\n",
      "558/746, train_loss: 4.3641\n",
      "559/746, train_loss: 4.4626\n",
      "560/746, train_loss: 4.1835\n",
      "561/746, train_loss: 4.5355\n",
      "562/746, train_loss: 4.5609\n",
      "563/746, train_loss: 4.3542\n",
      "564/746, train_loss: 4.5367\n",
      "565/746, train_loss: 4.3537\n",
      "566/746, train_loss: 4.3612\n",
      "567/746, train_loss: 4.3260\n",
      "568/746, train_loss: 4.4450\n",
      "569/746, train_loss: 4.4646\n",
      "570/746, train_loss: 4.3877\n",
      "571/746, train_loss: 4.4518\n",
      "572/746, train_loss: 4.2861\n",
      "573/746, train_loss: 4.4122\n",
      "574/746, train_loss: 4.3984\n",
      "575/746, train_loss: 4.4695\n",
      "576/746, train_loss: 4.4600\n",
      "577/746, train_loss: 4.3228\n",
      "578/746, train_loss: 4.4143\n",
      "579/746, train_loss: 4.5415\n",
      "580/746, train_loss: 4.3237\n",
      "581/746, train_loss: 4.4130\n",
      "582/746, train_loss: 4.4827\n",
      "583/746, train_loss: 4.4542\n",
      "584/746, train_loss: 4.3187\n",
      "585/746, train_loss: 4.4165\n",
      "586/746, train_loss: 4.3659\n",
      "587/746, train_loss: 4.3560\n",
      "588/746, train_loss: 4.6142\n",
      "589/746, train_loss: 4.3314\n",
      "590/746, train_loss: 4.3521\n",
      "591/746, train_loss: 4.6314\n",
      "592/746, train_loss: 4.2098\n",
      "593/746, train_loss: 4.6117\n",
      "594/746, train_loss: 4.3783\n",
      "595/746, train_loss: 4.3327\n",
      "596/746, train_loss: 4.4509\n",
      "597/746, train_loss: 4.3205\n",
      "598/746, train_loss: 4.1392\n",
      "599/746, train_loss: 4.3775\n",
      "600/746, train_loss: 4.4190\n",
      "601/746, train_loss: 4.3410\n",
      "602/746, train_loss: 4.4777\n",
      "603/746, train_loss: 4.4910\n",
      "604/746, train_loss: 4.2363\n",
      "605/746, train_loss: 4.1588\n",
      "606/746, train_loss: 4.4733\n",
      "607/746, train_loss: 4.5424\n",
      "608/746, train_loss: 4.4402\n",
      "609/746, train_loss: 4.3741\n",
      "610/746, train_loss: 4.2841\n",
      "611/746, train_loss: 4.3717\n",
      "612/746, train_loss: 4.4093\n",
      "613/746, train_loss: 4.5631\n",
      "614/746, train_loss: 4.4065\n",
      "615/746, train_loss: 4.3961\n",
      "616/746, train_loss: 4.2342\n",
      "617/746, train_loss: 4.6349\n",
      "618/746, train_loss: 4.3062\n",
      "619/746, train_loss: 4.3680\n",
      "620/746, train_loss: 4.4768\n",
      "621/746, train_loss: 4.1469\n",
      "622/746, train_loss: 4.4964\n",
      "623/746, train_loss: 4.3907\n",
      "624/746, train_loss: 4.3517\n",
      "625/746, train_loss: 4.2730\n",
      "626/746, train_loss: 4.3973\n",
      "627/746, train_loss: 4.3752\n",
      "628/746, train_loss: 4.4921\n",
      "629/746, train_loss: 4.4083\n",
      "630/746, train_loss: 4.4138\n",
      "631/746, train_loss: 4.3975\n",
      "632/746, train_loss: 4.2884\n",
      "633/746, train_loss: 4.3839\n",
      "634/746, train_loss: 4.3325\n",
      "635/746, train_loss: 4.2549\n",
      "636/746, train_loss: 4.3181\n",
      "637/746, train_loss: 4.0750\n",
      "638/746, train_loss: 4.4792\n",
      "639/746, train_loss: 4.4323\n",
      "640/746, train_loss: 4.2609\n",
      "641/746, train_loss: 4.2112\n",
      "642/746, train_loss: 4.4118\n",
      "643/746, train_loss: 4.5865\n",
      "644/746, train_loss: 4.4808\n",
      "645/746, train_loss: 4.5416\n",
      "646/746, train_loss: 4.3512\n",
      "647/746, train_loss: 4.4036\n",
      "648/746, train_loss: 4.4549\n",
      "649/746, train_loss: 4.5007\n",
      "650/746, train_loss: 4.4007\n",
      "651/746, train_loss: 4.1518\n",
      "652/746, train_loss: 4.2975\n",
      "653/746, train_loss: 4.2288\n",
      "654/746, train_loss: 4.2436\n",
      "655/746, train_loss: 4.3544\n",
      "656/746, train_loss: 4.4523\n",
      "657/746, train_loss: 4.5282\n",
      "658/746, train_loss: 4.4066\n",
      "659/746, train_loss: 4.4173\n",
      "660/746, train_loss: 4.4280\n",
      "661/746, train_loss: 4.5273\n",
      "662/746, train_loss: 4.3157\n",
      "663/746, train_loss: 4.3186\n",
      "664/746, train_loss: 4.5366\n",
      "665/746, train_loss: 4.4493\n",
      "666/746, train_loss: 4.3796\n",
      "667/746, train_loss: 4.1675\n",
      "668/746, train_loss: 4.2838\n",
      "669/746, train_loss: 4.3122\n",
      "670/746, train_loss: 4.4230\n",
      "671/746, train_loss: 4.2947\n",
      "672/746, train_loss: 4.4155\n",
      "673/746, train_loss: 4.5234\n",
      "674/746, train_loss: 4.4438\n",
      "675/746, train_loss: 4.6018\n",
      "676/746, train_loss: 4.2591\n",
      "677/746, train_loss: 4.3794\n",
      "678/746, train_loss: 4.2711\n",
      "679/746, train_loss: 4.5075\n",
      "680/746, train_loss: 4.4961\n",
      "681/746, train_loss: 4.2722\n",
      "682/746, train_loss: 4.1936\n",
      "683/746, train_loss: 4.2743\n",
      "684/746, train_loss: 4.3046\n",
      "685/746, train_loss: 4.1038\n",
      "686/746, train_loss: 4.1810\n",
      "687/746, train_loss: 4.5152\n",
      "688/746, train_loss: 4.4116\n",
      "689/746, train_loss: 4.6456\n",
      "690/746, train_loss: 4.2866\n",
      "691/746, train_loss: 4.3268\n",
      "692/746, train_loss: 4.2394\n",
      "693/746, train_loss: 4.1636\n",
      "694/746, train_loss: 4.3050\n",
      "695/746, train_loss: 4.3778\n",
      "696/746, train_loss: 4.4023\n",
      "697/746, train_loss: 4.4395\n",
      "698/746, train_loss: 4.3067\n",
      "699/746, train_loss: 4.5144\n",
      "700/746, train_loss: 4.1460\n",
      "701/746, train_loss: 4.4095\n",
      "702/746, train_loss: 4.3820\n",
      "703/746, train_loss: 4.2469\n",
      "704/746, train_loss: 4.3616\n",
      "705/746, train_loss: 4.4861\n",
      "706/746, train_loss: 4.5381\n",
      "707/746, train_loss: 4.2033\n",
      "708/746, train_loss: 4.2106\n",
      "709/746, train_loss: 4.2191\n",
      "710/746, train_loss: 4.2925\n",
      "711/746, train_loss: 4.5450\n",
      "712/746, train_loss: 4.1645\n",
      "713/746, train_loss: 4.2934\n",
      "714/746, train_loss: 4.3501\n",
      "715/746, train_loss: 4.4307\n",
      "716/746, train_loss: 4.2935\n",
      "717/746, train_loss: 4.1280\n",
      "718/746, train_loss: 4.2286\n",
      "719/746, train_loss: 4.2495\n",
      "720/746, train_loss: 4.4789\n",
      "721/746, train_loss: 4.4443\n",
      "722/746, train_loss: 4.4725\n",
      "723/746, train_loss: 4.3556\n",
      "724/746, train_loss: 4.3850\n",
      "725/746, train_loss: 4.4001\n",
      "726/746, train_loss: 4.2788\n",
      "727/746, train_loss: 4.3639\n",
      "728/746, train_loss: 4.3977\n",
      "729/746, train_loss: 4.4125\n",
      "730/746, train_loss: 4.4011\n",
      "731/746, train_loss: 4.4736\n",
      "732/746, train_loss: 4.2103\n",
      "733/746, train_loss: 4.4220\n",
      "734/746, train_loss: 4.3896\n",
      "735/746, train_loss: 4.5561\n",
      "736/746, train_loss: 4.2664\n",
      "737/746, train_loss: 4.3820\n",
      "738/746, train_loss: 4.3010\n",
      "739/746, train_loss: 4.5374\n",
      "740/746, train_loss: 4.3444\n",
      "741/746, train_loss: 4.1981\n",
      "742/746, train_loss: 4.3768\n",
      "743/746, train_loss: 4.1520\n",
      "744/746, train_loss: 4.5729\n",
      "745/746, train_loss: 4.3757\n",
      "746/746, train_loss: 4.4187\n",
      "747/746, train_loss: 4.4662\n",
      "epoch 6 average loss: 4.4750\n",
      "saved new best metric model\n",
      "current epoch: 6 current AUC: 0.9395 current accuracy: 0.2875 best AUC: 0.2875 at epoch: 6\n",
      "----------\n",
      "epoch 7/10\n",
      "1/746, train_loss: 4.2583\n",
      "2/746, train_loss: 4.5225\n",
      "3/746, train_loss: 4.3279\n",
      "4/746, train_loss: 4.4625\n",
      "5/746, train_loss: 4.3532\n",
      "6/746, train_loss: 4.3140\n",
      "7/746, train_loss: 4.4156\n",
      "8/746, train_loss: 4.3690\n",
      "9/746, train_loss: 4.4409\n",
      "10/746, train_loss: 4.2848\n",
      "11/746, train_loss: 4.2370\n",
      "12/746, train_loss: 4.3346\n",
      "13/746, train_loss: 4.2442\n",
      "14/746, train_loss: 4.2396\n",
      "15/746, train_loss: 4.3784\n",
      "16/746, train_loss: 4.3114\n",
      "17/746, train_loss: 4.1529\n",
      "18/746, train_loss: 4.1206\n",
      "19/746, train_loss: 4.3111\n",
      "20/746, train_loss: 4.3112\n",
      "21/746, train_loss: 4.2288\n",
      "22/746, train_loss: 4.2580\n",
      "23/746, train_loss: 4.2378\n",
      "24/746, train_loss: 4.2694\n",
      "25/746, train_loss: 4.1809\n",
      "26/746, train_loss: 4.3406\n",
      "27/746, train_loss: 4.1862\n",
      "28/746, train_loss: 4.1462\n",
      "29/746, train_loss: 4.2021\n",
      "30/746, train_loss: 4.3780\n",
      "31/746, train_loss: 4.1239\n",
      "32/746, train_loss: 4.6036\n",
      "33/746, train_loss: 4.2669\n",
      "34/746, train_loss: 4.3221\n",
      "35/746, train_loss: 4.0535\n",
      "36/746, train_loss: 4.4133\n",
      "37/746, train_loss: 4.3480\n",
      "38/746, train_loss: 4.2113\n",
      "39/746, train_loss: 4.2754\n",
      "40/746, train_loss: 4.4023\n",
      "41/746, train_loss: 4.4160\n",
      "42/746, train_loss: 4.3024\n",
      "43/746, train_loss: 4.4995\n",
      "44/746, train_loss: 4.1914\n",
      "45/746, train_loss: 4.3642\n",
      "46/746, train_loss: 4.0689\n",
      "47/746, train_loss: 4.2510\n",
      "48/746, train_loss: 4.2032\n",
      "49/746, train_loss: 4.1388\n",
      "50/746, train_loss: 4.3167\n",
      "51/746, train_loss: 4.2192\n",
      "52/746, train_loss: 4.4161\n",
      "53/746, train_loss: 4.5520\n",
      "54/746, train_loss: 4.2874\n",
      "55/746, train_loss: 4.3467\n",
      "56/746, train_loss: 4.3413\n",
      "57/746, train_loss: 4.1778\n",
      "58/746, train_loss: 4.1289\n",
      "59/746, train_loss: 4.3599\n",
      "60/746, train_loss: 4.2270\n",
      "61/746, train_loss: 4.3322\n",
      "62/746, train_loss: 4.2306\n",
      "63/746, train_loss: 4.3438\n",
      "64/746, train_loss: 4.4425\n",
      "65/746, train_loss: 4.4367\n",
      "66/746, train_loss: 4.4586\n",
      "67/746, train_loss: 4.2575\n",
      "68/746, train_loss: 4.1826\n",
      "69/746, train_loss: 4.2189\n",
      "70/746, train_loss: 4.2111\n",
      "71/746, train_loss: 4.3530\n",
      "72/746, train_loss: 4.2248\n",
      "73/746, train_loss: 4.4075\n",
      "74/746, train_loss: 4.2350\n",
      "75/746, train_loss: 4.3889\n",
      "76/746, train_loss: 4.0819\n",
      "77/746, train_loss: 4.2522\n",
      "78/746, train_loss: 4.2907\n",
      "79/746, train_loss: 4.1633\n",
      "80/746, train_loss: 4.2106\n",
      "81/746, train_loss: 4.2854\n",
      "82/746, train_loss: 4.2549\n",
      "83/746, train_loss: 4.1614\n",
      "84/746, train_loss: 4.4081\n",
      "85/746, train_loss: 4.2330\n",
      "86/746, train_loss: 4.2303\n",
      "87/746, train_loss: 4.3837\n",
      "88/746, train_loss: 4.2384\n",
      "89/746, train_loss: 4.0584\n",
      "90/746, train_loss: 4.3496\n",
      "91/746, train_loss: 4.1607\n",
      "92/746, train_loss: 4.2794\n",
      "93/746, train_loss: 4.4549\n",
      "94/746, train_loss: 4.0951\n",
      "95/746, train_loss: 4.4094\n",
      "96/746, train_loss: 4.3030\n",
      "97/746, train_loss: 4.0902\n",
      "98/746, train_loss: 4.2137\n",
      "99/746, train_loss: 4.2451\n",
      "100/746, train_loss: 4.1585\n",
      "101/746, train_loss: 4.1905\n",
      "102/746, train_loss: 4.0857\n",
      "103/746, train_loss: 4.2129\n",
      "104/746, train_loss: 4.3212\n",
      "105/746, train_loss: 4.1513\n",
      "106/746, train_loss: 4.3118\n",
      "107/746, train_loss: 4.2552\n",
      "108/746, train_loss: 4.1731\n",
      "109/746, train_loss: 4.0895\n",
      "110/746, train_loss: 4.1607\n",
      "111/746, train_loss: 4.2170\n",
      "112/746, train_loss: 4.2622\n",
      "113/746, train_loss: 4.4596\n",
      "114/746, train_loss: 4.4028\n",
      "115/746, train_loss: 3.9200\n",
      "116/746, train_loss: 4.2810\n",
      "117/746, train_loss: 3.9699\n",
      "118/746, train_loss: 3.9650\n",
      "119/746, train_loss: 4.4055\n",
      "120/746, train_loss: 4.2079\n",
      "121/746, train_loss: 4.3521\n",
      "122/746, train_loss: 4.2591\n",
      "123/746, train_loss: 4.3526\n",
      "124/746, train_loss: 4.1283\n",
      "125/746, train_loss: 4.3787\n",
      "126/746, train_loss: 4.2889\n",
      "127/746, train_loss: 4.2875\n",
      "128/746, train_loss: 4.5500\n",
      "129/746, train_loss: 4.3153\n",
      "130/746, train_loss: 4.2327\n",
      "131/746, train_loss: 4.2312\n",
      "132/746, train_loss: 4.2244\n",
      "133/746, train_loss: 4.3438\n",
      "134/746, train_loss: 4.0537\n",
      "135/746, train_loss: 4.3659\n",
      "136/746, train_loss: 4.2482\n",
      "137/746, train_loss: 4.4127\n",
      "138/746, train_loss: 4.2997\n",
      "139/746, train_loss: 4.2713\n",
      "140/746, train_loss: 4.3441\n",
      "141/746, train_loss: 4.3192\n",
      "142/746, train_loss: 4.2488\n",
      "143/746, train_loss: 4.2089\n",
      "144/746, train_loss: 4.3041\n",
      "145/746, train_loss: 4.2417\n",
      "146/746, train_loss: 4.2671\n",
      "147/746, train_loss: 4.1038\n",
      "148/746, train_loss: 4.3178\n",
      "149/746, train_loss: 4.2078\n",
      "150/746, train_loss: 4.2492\n",
      "151/746, train_loss: 4.1116\n",
      "152/746, train_loss: 4.3348\n",
      "153/746, train_loss: 4.3277\n",
      "154/746, train_loss: 4.0533\n",
      "155/746, train_loss: 4.1815\n",
      "156/746, train_loss: 4.2443\n",
      "157/746, train_loss: 4.1341\n",
      "158/746, train_loss: 4.2979\n",
      "159/746, train_loss: 4.3228\n",
      "160/746, train_loss: 4.2348\n",
      "161/746, train_loss: 4.2205\n",
      "162/746, train_loss: 4.2043\n",
      "163/746, train_loss: 4.3137\n",
      "164/746, train_loss: 4.1133\n",
      "165/746, train_loss: 4.3129\n",
      "166/746, train_loss: 4.2278\n",
      "167/746, train_loss: 4.2080\n",
      "168/746, train_loss: 4.3955\n",
      "169/746, train_loss: 4.2256\n",
      "170/746, train_loss: 4.1802\n",
      "171/746, train_loss: 4.0364\n",
      "172/746, train_loss: 4.1103\n",
      "173/746, train_loss: 4.2763\n",
      "174/746, train_loss: 4.1670\n",
      "175/746, train_loss: 4.2046\n",
      "176/746, train_loss: 4.3527\n",
      "177/746, train_loss: 3.9028\n",
      "178/746, train_loss: 4.1694\n",
      "179/746, train_loss: 4.2091\n",
      "180/746, train_loss: 4.2110\n",
      "181/746, train_loss: 4.3057\n",
      "182/746, train_loss: 4.1407\n",
      "183/746, train_loss: 4.2676\n",
      "184/746, train_loss: 4.0912\n",
      "185/746, train_loss: 4.2727\n",
      "186/746, train_loss: 4.1264\n",
      "187/746, train_loss: 4.2005\n",
      "188/746, train_loss: 4.3215\n",
      "189/746, train_loss: 4.3052\n",
      "190/746, train_loss: 4.2532\n",
      "191/746, train_loss: 4.2102\n",
      "192/746, train_loss: 4.2194\n",
      "193/746, train_loss: 4.1990\n",
      "194/746, train_loss: 4.1009\n",
      "195/746, train_loss: 4.2334\n",
      "196/746, train_loss: 4.3616\n",
      "197/746, train_loss: 4.0726\n",
      "198/746, train_loss: 4.3531\n",
      "199/746, train_loss: 4.0465\n",
      "200/746, train_loss: 4.1832\n",
      "201/746, train_loss: 4.1464\n",
      "202/746, train_loss: 4.1470\n",
      "203/746, train_loss: 4.1197\n",
      "204/746, train_loss: 4.1097\n",
      "205/746, train_loss: 4.1559\n",
      "206/746, train_loss: 3.8618\n",
      "207/746, train_loss: 4.0219\n",
      "208/746, train_loss: 4.3407\n",
      "209/746, train_loss: 4.1429\n",
      "210/746, train_loss: 4.2714\n",
      "211/746, train_loss: 4.0401\n",
      "212/746, train_loss: 4.2515\n",
      "213/746, train_loss: 4.2907\n",
      "214/746, train_loss: 4.1930\n",
      "215/746, train_loss: 3.9717\n",
      "216/746, train_loss: 4.3792\n",
      "217/746, train_loss: 4.2985\n",
      "218/746, train_loss: 4.2284\n",
      "219/746, train_loss: 4.3188\n",
      "220/746, train_loss: 4.3389\n",
      "221/746, train_loss: 4.2332\n",
      "222/746, train_loss: 4.2027\n",
      "223/746, train_loss: 4.1952\n",
      "224/746, train_loss: 4.3003\n",
      "225/746, train_loss: 4.0421\n",
      "226/746, train_loss: 4.2114\n",
      "227/746, train_loss: 4.2532\n",
      "228/746, train_loss: 4.2079\n",
      "229/746, train_loss: 4.2017\n",
      "230/746, train_loss: 4.2855\n",
      "231/746, train_loss: 4.0765\n",
      "232/746, train_loss: 4.1609\n",
      "233/746, train_loss: 4.1592\n",
      "234/746, train_loss: 4.1458\n",
      "235/746, train_loss: 4.3664\n",
      "236/746, train_loss: 4.3693\n",
      "237/746, train_loss: 4.1063\n",
      "238/746, train_loss: 4.3938\n",
      "239/746, train_loss: 4.2885\n",
      "240/746, train_loss: 4.2138\n",
      "241/746, train_loss: 4.2171\n",
      "242/746, train_loss: 3.9825\n",
      "243/746, train_loss: 4.0510\n",
      "244/746, train_loss: 4.3044\n",
      "245/746, train_loss: 4.2116\n",
      "246/746, train_loss: 4.2818\n",
      "247/746, train_loss: 4.2308\n",
      "248/746, train_loss: 4.1951\n",
      "249/746, train_loss: 4.2545\n",
      "250/746, train_loss: 4.2401\n",
      "251/746, train_loss: 4.0911\n",
      "252/746, train_loss: 4.2199\n",
      "253/746, train_loss: 4.1181\n",
      "254/746, train_loss: 3.9418\n",
      "255/746, train_loss: 4.2746\n",
      "256/746, train_loss: 4.2667\n",
      "257/746, train_loss: 4.2490\n",
      "258/746, train_loss: 4.3043\n",
      "259/746, train_loss: 4.2107\n",
      "260/746, train_loss: 4.1408\n",
      "261/746, train_loss: 4.1911\n",
      "262/746, train_loss: 4.2406\n",
      "263/746, train_loss: 4.1098\n",
      "264/746, train_loss: 4.1377\n",
      "265/746, train_loss: 4.4488\n",
      "266/746, train_loss: 4.4241\n",
      "267/746, train_loss: 4.1321\n",
      "268/746, train_loss: 3.9085\n",
      "269/746, train_loss: 4.0206\n",
      "270/746, train_loss: 4.2332\n",
      "271/746, train_loss: 4.3329\n",
      "272/746, train_loss: 3.9737\n",
      "273/746, train_loss: 4.2176\n",
      "274/746, train_loss: 4.1164\n",
      "275/746, train_loss: 4.3679\n",
      "276/746, train_loss: 4.2170\n",
      "277/746, train_loss: 4.1137\n",
      "278/746, train_loss: 4.3318\n",
      "279/746, train_loss: 4.2224\n",
      "280/746, train_loss: 4.2083\n",
      "281/746, train_loss: 4.2706\n",
      "282/746, train_loss: 4.2710\n",
      "283/746, train_loss: 4.3097\n",
      "284/746, train_loss: 4.3791\n",
      "285/746, train_loss: 4.3081\n",
      "286/746, train_loss: 4.2229\n",
      "287/746, train_loss: 4.0879\n",
      "288/746, train_loss: 4.1635\n",
      "289/746, train_loss: 4.2764\n",
      "290/746, train_loss: 4.0791\n",
      "291/746, train_loss: 4.3703\n",
      "292/746, train_loss: 3.9391\n",
      "293/746, train_loss: 4.4121\n",
      "294/746, train_loss: 4.2636\n",
      "295/746, train_loss: 4.0473\n",
      "296/746, train_loss: 4.2273\n",
      "297/746, train_loss: 4.1370\n",
      "298/746, train_loss: 4.3668\n",
      "299/746, train_loss: 4.2410\n",
      "300/746, train_loss: 3.9945\n",
      "301/746, train_loss: 4.1176\n",
      "302/746, train_loss: 4.1530\n",
      "303/746, train_loss: 4.1900\n",
      "304/746, train_loss: 3.9828\n",
      "305/746, train_loss: 4.2726\n",
      "306/746, train_loss: 4.2833\n",
      "307/746, train_loss: 4.4642\n",
      "308/746, train_loss: 4.1895\n",
      "309/746, train_loss: 3.9912\n",
      "310/746, train_loss: 4.1848\n",
      "311/746, train_loss: 3.9317\n",
      "312/746, train_loss: 4.2277\n",
      "313/746, train_loss: 3.9404\n",
      "314/746, train_loss: 4.1247\n",
      "315/746, train_loss: 4.1887\n",
      "316/746, train_loss: 4.2066\n",
      "317/746, train_loss: 4.3308\n",
      "318/746, train_loss: 4.1106\n",
      "319/746, train_loss: 4.0025\n",
      "320/746, train_loss: 4.2387\n",
      "321/746, train_loss: 4.1022\n",
      "322/746, train_loss: 4.1170\n",
      "323/746, train_loss: 4.3180\n",
      "324/746, train_loss: 4.2384\n",
      "325/746, train_loss: 4.2241\n",
      "326/746, train_loss: 4.2210\n",
      "327/746, train_loss: 4.3274\n",
      "328/746, train_loss: 3.9781\n",
      "329/746, train_loss: 4.2143\n",
      "330/746, train_loss: 4.2875\n",
      "331/746, train_loss: 4.2941\n",
      "332/746, train_loss: 4.1416\n",
      "333/746, train_loss: 4.1145\n",
      "334/746, train_loss: 4.1672\n",
      "335/746, train_loss: 4.1791\n",
      "336/746, train_loss: 4.0983\n",
      "337/746, train_loss: 4.1720\n",
      "338/746, train_loss: 4.1538\n",
      "339/746, train_loss: 4.2605\n",
      "340/746, train_loss: 4.0955\n",
      "341/746, train_loss: 4.2782\n",
      "342/746, train_loss: 4.2436\n",
      "343/746, train_loss: 4.3408\n",
      "344/746, train_loss: 4.2256\n",
      "345/746, train_loss: 4.3494\n",
      "346/746, train_loss: 3.9249\n",
      "347/746, train_loss: 4.2563\n",
      "348/746, train_loss: 4.1952\n",
      "349/746, train_loss: 4.0920\n",
      "350/746, train_loss: 4.0989\n",
      "351/746, train_loss: 4.3260\n",
      "352/746, train_loss: 4.3453\n",
      "353/746, train_loss: 4.1310\n",
      "354/746, train_loss: 4.1983\n",
      "355/746, train_loss: 4.3453\n",
      "356/746, train_loss: 4.0647\n",
      "357/746, train_loss: 4.4032\n",
      "358/746, train_loss: 4.1910\n",
      "359/746, train_loss: 3.9526\n",
      "360/746, train_loss: 4.1464\n",
      "361/746, train_loss: 4.1585\n",
      "362/746, train_loss: 4.0141\n",
      "363/746, train_loss: 4.1830\n",
      "364/746, train_loss: 4.2616\n",
      "365/746, train_loss: 4.1622\n",
      "366/746, train_loss: 4.1522\n",
      "367/746, train_loss: 3.9818\n",
      "368/746, train_loss: 4.1886\n",
      "369/746, train_loss: 4.1650\n",
      "370/746, train_loss: 4.2033\n",
      "371/746, train_loss: 3.9450\n",
      "372/746, train_loss: 4.1266\n",
      "373/746, train_loss: 4.1303\n",
      "374/746, train_loss: 4.3534\n",
      "375/746, train_loss: 3.9243\n",
      "376/746, train_loss: 4.2582\n",
      "377/746, train_loss: 4.3022\n",
      "378/746, train_loss: 4.0258\n",
      "379/746, train_loss: 4.1084\n",
      "380/746, train_loss: 4.2190\n",
      "381/746, train_loss: 4.1658\n",
      "382/746, train_loss: 4.4098\n",
      "383/746, train_loss: 4.0133\n",
      "384/746, train_loss: 4.1987\n",
      "385/746, train_loss: 4.1537\n",
      "386/746, train_loss: 4.2541\n",
      "387/746, train_loss: 4.4000\n",
      "388/746, train_loss: 4.0523\n",
      "389/746, train_loss: 3.9971\n",
      "390/746, train_loss: 4.1547\n",
      "391/746, train_loss: 4.1857\n",
      "392/746, train_loss: 3.9860\n",
      "393/746, train_loss: 4.2157\n",
      "394/746, train_loss: 4.2153\n",
      "395/746, train_loss: 4.2027\n",
      "396/746, train_loss: 4.0471\n",
      "397/746, train_loss: 4.1897\n",
      "398/746, train_loss: 4.2755\n",
      "399/746, train_loss: 3.9948\n",
      "400/746, train_loss: 4.0856\n",
      "401/746, train_loss: 4.4312\n",
      "402/746, train_loss: 4.0426\n",
      "403/746, train_loss: 4.1195\n",
      "404/746, train_loss: 4.1460\n",
      "405/746, train_loss: 3.9868\n",
      "406/746, train_loss: 4.0782\n",
      "407/746, train_loss: 4.1597\n",
      "408/746, train_loss: 3.9669\n",
      "409/746, train_loss: 4.1214\n",
      "410/746, train_loss: 4.1160\n",
      "411/746, train_loss: 4.3743\n",
      "412/746, train_loss: 3.9509\n",
      "413/746, train_loss: 4.1712\n",
      "414/746, train_loss: 4.2375\n",
      "415/746, train_loss: 4.1910\n",
      "416/746, train_loss: 4.2450\n",
      "417/746, train_loss: 4.0972\n",
      "418/746, train_loss: 4.1999\n",
      "419/746, train_loss: 4.1649\n",
      "420/746, train_loss: 4.2393\n",
      "421/746, train_loss: 4.0506\n",
      "422/746, train_loss: 4.4292\n",
      "423/746, train_loss: 3.8886\n",
      "424/746, train_loss: 4.1511\n",
      "425/746, train_loss: 4.1668\n",
      "426/746, train_loss: 4.1399\n",
      "427/746, train_loss: 4.1522\n",
      "428/746, train_loss: 4.3345\n",
      "429/746, train_loss: 4.2846\n",
      "430/746, train_loss: 3.9850\n",
      "431/746, train_loss: 4.1213\n",
      "432/746, train_loss: 3.9240\n",
      "433/746, train_loss: 4.0682\n",
      "434/746, train_loss: 4.3704\n",
      "435/746, train_loss: 4.3003\n",
      "436/746, train_loss: 4.4294\n",
      "437/746, train_loss: 4.2085\n",
      "438/746, train_loss: 4.0433\n",
      "439/746, train_loss: 4.0330\n",
      "440/746, train_loss: 4.1875\n",
      "441/746, train_loss: 4.2843\n",
      "442/746, train_loss: 4.2137\n",
      "443/746, train_loss: 4.2897\n",
      "444/746, train_loss: 4.1801\n",
      "445/746, train_loss: 4.0749\n",
      "446/746, train_loss: 4.1173\n",
      "447/746, train_loss: 4.0796\n",
      "448/746, train_loss: 4.2575\n",
      "449/746, train_loss: 4.1411\n",
      "450/746, train_loss: 4.2877\n",
      "451/746, train_loss: 3.9761\n",
      "452/746, train_loss: 4.1186\n",
      "453/746, train_loss: 4.3952\n",
      "454/746, train_loss: 4.2402\n",
      "455/746, train_loss: 4.1936\n",
      "456/746, train_loss: 4.2901\n",
      "457/746, train_loss: 3.9504\n",
      "458/746, train_loss: 4.1379\n",
      "459/746, train_loss: 4.1803\n",
      "460/746, train_loss: 4.0641\n",
      "461/746, train_loss: 4.0995\n",
      "462/746, train_loss: 4.3290\n",
      "463/746, train_loss: 4.0970\n",
      "464/746, train_loss: 4.0624\n",
      "465/746, train_loss: 4.2204\n",
      "466/746, train_loss: 4.0959\n",
      "467/746, train_loss: 4.2493\n",
      "468/746, train_loss: 4.1932\n",
      "469/746, train_loss: 4.0424\n",
      "470/746, train_loss: 3.8963\n",
      "471/746, train_loss: 4.3834\n",
      "472/746, train_loss: 3.9741\n",
      "473/746, train_loss: 4.0644\n",
      "474/746, train_loss: 3.9570\n",
      "475/746, train_loss: 4.1503\n",
      "476/746, train_loss: 4.0707\n",
      "477/746, train_loss: 4.1720\n",
      "478/746, train_loss: 4.0752\n",
      "479/746, train_loss: 4.2749\n",
      "480/746, train_loss: 3.9876\n",
      "481/746, train_loss: 4.1322\n",
      "482/746, train_loss: 4.2481\n",
      "483/746, train_loss: 4.1133\n",
      "484/746, train_loss: 3.9830\n",
      "485/746, train_loss: 4.1585\n",
      "486/746, train_loss: 4.3289\n",
      "487/746, train_loss: 4.0886\n",
      "488/746, train_loss: 4.3750\n",
      "489/746, train_loss: 4.0483\n",
      "490/746, train_loss: 4.0751\n",
      "491/746, train_loss: 4.1043\n",
      "492/746, train_loss: 3.9191\n",
      "493/746, train_loss: 4.2069\n",
      "494/746, train_loss: 4.1197\n",
      "495/746, train_loss: 4.1778\n",
      "496/746, train_loss: 4.2040\n",
      "497/746, train_loss: 4.1101\n",
      "498/746, train_loss: 4.0554\n",
      "499/746, train_loss: 3.8157\n",
      "500/746, train_loss: 3.7487\n",
      "501/746, train_loss: 4.1289\n",
      "502/746, train_loss: 4.2866\n",
      "503/746, train_loss: 4.2327\n",
      "504/746, train_loss: 4.1059\n",
      "505/746, train_loss: 4.1725\n",
      "506/746, train_loss: 4.0620\n",
      "507/746, train_loss: 3.9808\n",
      "508/746, train_loss: 4.0544\n",
      "509/746, train_loss: 4.1530\n",
      "510/746, train_loss: 4.1939\n",
      "511/746, train_loss: 4.0868\n",
      "512/746, train_loss: 3.8681\n",
      "513/746, train_loss: 4.1667\n",
      "514/746, train_loss: 4.1026\n",
      "515/746, train_loss: 4.1267\n",
      "516/746, train_loss: 4.0106\n",
      "517/746, train_loss: 4.0728\n",
      "518/746, train_loss: 4.2140\n",
      "519/746, train_loss: 4.2096\n",
      "520/746, train_loss: 4.0537\n",
      "521/746, train_loss: 4.1169\n",
      "522/746, train_loss: 4.0748\n",
      "523/746, train_loss: 4.2986\n",
      "524/746, train_loss: 4.1608\n",
      "525/746, train_loss: 4.3031\n",
      "526/746, train_loss: 3.9522\n",
      "527/746, train_loss: 4.2418\n",
      "528/746, train_loss: 4.0575\n",
      "529/746, train_loss: 3.8891\n",
      "530/746, train_loss: 3.9930\n",
      "531/746, train_loss: 3.9891\n",
      "532/746, train_loss: 4.1051\n",
      "533/746, train_loss: 4.1626\n",
      "534/746, train_loss: 4.2203\n",
      "535/746, train_loss: 4.0647\n",
      "536/746, train_loss: 4.0368\n",
      "537/746, train_loss: 4.0993\n",
      "538/746, train_loss: 4.1881\n",
      "539/746, train_loss: 4.0815\n",
      "540/746, train_loss: 4.1729\n",
      "541/746, train_loss: 4.0395\n",
      "542/746, train_loss: 3.9810\n",
      "543/746, train_loss: 4.0367\n",
      "544/746, train_loss: 3.8486\n",
      "545/746, train_loss: 4.3223\n",
      "546/746, train_loss: 4.0639\n",
      "547/746, train_loss: 3.9495\n",
      "548/746, train_loss: 4.2832\n",
      "549/746, train_loss: 4.1457\n",
      "550/746, train_loss: 4.2239\n",
      "551/746, train_loss: 4.1447\n",
      "552/746, train_loss: 4.2753\n",
      "553/746, train_loss: 4.1838\n",
      "554/746, train_loss: 4.0727\n",
      "555/746, train_loss: 4.1662\n",
      "556/746, train_loss: 4.2975\n",
      "557/746, train_loss: 4.1719\n",
      "558/746, train_loss: 3.9652\n",
      "559/746, train_loss: 4.1315\n",
      "560/746, train_loss: 4.1195\n",
      "561/746, train_loss: 3.9772\n",
      "562/746, train_loss: 4.1333\n",
      "563/746, train_loss: 4.0320\n",
      "564/746, train_loss: 4.0413\n",
      "565/746, train_loss: 4.1174\n",
      "566/746, train_loss: 4.1246\n",
      "567/746, train_loss: 3.9858\n",
      "568/746, train_loss: 4.0357\n",
      "569/746, train_loss: 4.0565\n",
      "570/746, train_loss: 4.0364\n",
      "571/746, train_loss: 3.8767\n",
      "572/746, train_loss: 4.1979\n",
      "573/746, train_loss: 4.0661\n",
      "574/746, train_loss: 4.2151\n",
      "575/746, train_loss: 4.1359\n",
      "576/746, train_loss: 4.0448\n",
      "577/746, train_loss: 4.0253\n",
      "578/746, train_loss: 4.0016\n",
      "579/746, train_loss: 4.1997\n",
      "580/746, train_loss: 3.8118\n",
      "581/746, train_loss: 4.0468\n",
      "582/746, train_loss: 4.0153\n",
      "583/746, train_loss: 4.1089\n",
      "584/746, train_loss: 4.1289\n",
      "585/746, train_loss: 3.9492\n",
      "586/746, train_loss: 3.9704\n",
      "587/746, train_loss: 3.9487\n",
      "588/746, train_loss: 3.9934\n",
      "589/746, train_loss: 4.1046\n",
      "590/746, train_loss: 4.2036\n",
      "591/746, train_loss: 4.0146\n",
      "592/746, train_loss: 4.0891\n",
      "593/746, train_loss: 4.2640\n",
      "594/746, train_loss: 3.9891\n",
      "595/746, train_loss: 4.1103\n",
      "596/746, train_loss: 4.0132\n",
      "597/746, train_loss: 4.2271\n",
      "598/746, train_loss: 3.9166\n",
      "599/746, train_loss: 4.1008\n",
      "600/746, train_loss: 3.9047\n",
      "601/746, train_loss: 4.0646\n",
      "602/746, train_loss: 4.1400\n",
      "603/746, train_loss: 4.2329\n",
      "604/746, train_loss: 3.9323\n",
      "605/746, train_loss: 4.0638\n",
      "606/746, train_loss: 4.0436\n",
      "607/746, train_loss: 3.9775\n",
      "608/746, train_loss: 4.0074\n",
      "609/746, train_loss: 3.9241\n",
      "610/746, train_loss: 4.0837\n",
      "611/746, train_loss: 3.9881\n",
      "612/746, train_loss: 3.9050\n",
      "613/746, train_loss: 4.1872\n",
      "614/746, train_loss: 3.9977\n",
      "615/746, train_loss: 4.1195\n",
      "616/746, train_loss: 3.9820\n",
      "617/746, train_loss: 3.9441\n",
      "618/746, train_loss: 4.1758\n",
      "619/746, train_loss: 4.1252\n",
      "620/746, train_loss: 3.8743\n",
      "621/746, train_loss: 4.0921\n",
      "622/746, train_loss: 4.2648\n",
      "623/746, train_loss: 3.9903\n",
      "624/746, train_loss: 4.0103\n",
      "625/746, train_loss: 3.9044\n",
      "626/746, train_loss: 4.0831\n",
      "627/746, train_loss: 4.0002\n",
      "628/746, train_loss: 4.0954\n",
      "629/746, train_loss: 4.0246\n",
      "630/746, train_loss: 4.1321\n",
      "631/746, train_loss: 4.0314\n",
      "632/746, train_loss: 4.3314\n",
      "633/746, train_loss: 4.0088\n",
      "634/746, train_loss: 4.1100\n",
      "635/746, train_loss: 3.9486\n",
      "636/746, train_loss: 4.3482\n",
      "637/746, train_loss: 4.0518\n",
      "638/746, train_loss: 3.8881\n",
      "639/746, train_loss: 4.0830\n",
      "640/746, train_loss: 4.2456\n",
      "641/746, train_loss: 4.2349\n",
      "642/746, train_loss: 3.9605\n",
      "643/746, train_loss: 4.0744\n",
      "644/746, train_loss: 3.9122\n",
      "645/746, train_loss: 4.2705\n",
      "646/746, train_loss: 3.7446\n",
      "647/746, train_loss: 4.1226\n",
      "648/746, train_loss: 4.1655\n",
      "649/746, train_loss: 4.1152\n",
      "650/746, train_loss: 4.1379\n",
      "651/746, train_loss: 4.1596\n",
      "652/746, train_loss: 4.0641\n",
      "653/746, train_loss: 4.2438\n",
      "654/746, train_loss: 4.2052\n",
      "655/746, train_loss: 3.9648\n",
      "656/746, train_loss: 4.1602\n",
      "657/746, train_loss: 4.0160\n",
      "658/746, train_loss: 4.0939\n",
      "659/746, train_loss: 3.9922\n",
      "660/746, train_loss: 3.9505\n",
      "661/746, train_loss: 3.8685\n",
      "662/746, train_loss: 4.1244\n",
      "663/746, train_loss: 4.0712\n",
      "664/746, train_loss: 3.9205\n",
      "665/746, train_loss: 4.1251\n",
      "666/746, train_loss: 4.0662\n",
      "667/746, train_loss: 4.1622\n",
      "668/746, train_loss: 4.1161\n",
      "669/746, train_loss: 4.1672\n",
      "670/746, train_loss: 4.0175\n",
      "671/746, train_loss: 4.0410\n",
      "672/746, train_loss: 4.1184\n",
      "673/746, train_loss: 4.0816\n",
      "674/746, train_loss: 4.0799\n",
      "675/746, train_loss: 3.9647\n",
      "676/746, train_loss: 3.9540\n",
      "677/746, train_loss: 3.9030\n",
      "678/746, train_loss: 4.0525\n",
      "679/746, train_loss: 4.0728\n",
      "680/746, train_loss: 4.1611\n",
      "681/746, train_loss: 4.0629\n",
      "682/746, train_loss: 4.0973\n",
      "683/746, train_loss: 4.0267\n",
      "684/746, train_loss: 3.9519\n",
      "685/746, train_loss: 4.1602\n",
      "686/746, train_loss: 4.2133\n",
      "687/746, train_loss: 4.0025\n",
      "688/746, train_loss: 4.0400\n",
      "689/746, train_loss: 4.2049\n",
      "690/746, train_loss: 4.1483\n",
      "691/746, train_loss: 4.2583\n",
      "692/746, train_loss: 4.0574\n",
      "693/746, train_loss: 4.0369\n",
      "694/746, train_loss: 4.1657\n",
      "695/746, train_loss: 4.0062\n",
      "696/746, train_loss: 3.9470\n",
      "697/746, train_loss: 4.2160\n",
      "698/746, train_loss: 4.2188\n",
      "699/746, train_loss: 4.1048\n",
      "700/746, train_loss: 4.0827\n",
      "701/746, train_loss: 3.9527\n",
      "702/746, train_loss: 3.8445\n",
      "703/746, train_loss: 4.0488\n",
      "704/746, train_loss: 4.0461\n",
      "705/746, train_loss: 4.1396\n",
      "706/746, train_loss: 4.0562\n",
      "707/746, train_loss: 3.8182\n",
      "708/746, train_loss: 4.2415\n",
      "709/746, train_loss: 3.9470\n",
      "710/746, train_loss: 3.8917\n",
      "711/746, train_loss: 4.1042\n",
      "712/746, train_loss: 4.1339\n",
      "713/746, train_loss: 3.6715\n",
      "714/746, train_loss: 4.1315\n",
      "715/746, train_loss: 4.1111\n",
      "716/746, train_loss: 4.2105\n",
      "717/746, train_loss: 4.1202\n",
      "718/746, train_loss: 3.8787\n",
      "719/746, train_loss: 4.0508\n",
      "720/746, train_loss: 3.9704\n",
      "721/746, train_loss: 3.9137\n",
      "722/746, train_loss: 3.9675\n",
      "723/746, train_loss: 3.8562\n",
      "724/746, train_loss: 4.0386\n",
      "725/746, train_loss: 3.9943\n",
      "726/746, train_loss: 3.9218\n",
      "727/746, train_loss: 4.1367\n",
      "728/746, train_loss: 3.9844\n",
      "729/746, train_loss: 3.9046\n",
      "730/746, train_loss: 4.0949\n",
      "731/746, train_loss: 4.1327\n",
      "732/746, train_loss: 3.8996\n",
      "733/746, train_loss: 3.8820\n",
      "734/746, train_loss: 4.0003\n",
      "735/746, train_loss: 4.1029\n",
      "736/746, train_loss: 4.1772\n",
      "737/746, train_loss: 4.0543\n",
      "738/746, train_loss: 4.0573\n",
      "739/746, train_loss: 3.9786\n",
      "740/746, train_loss: 3.8958\n",
      "741/746, train_loss: 4.0708\n",
      "742/746, train_loss: 3.9898\n",
      "743/746, train_loss: 4.2505\n",
      "744/746, train_loss: 4.0048\n",
      "745/746, train_loss: 3.9890\n",
      "746/746, train_loss: 4.0937\n",
      "747/746, train_loss: 4.0309\n",
      "epoch 7 average loss: 4.1603\n",
      "saved new best metric model\n",
      "current epoch: 7 current AUC: 0.9534 current accuracy: 0.3341 best AUC: 0.3341 at epoch: 7\n",
      "----------\n",
      "epoch 8/10\n",
      "1/746, train_loss: 4.1193\n",
      "2/746, train_loss: 3.9119\n",
      "3/746, train_loss: 4.0502\n",
      "4/746, train_loss: 4.0411\n",
      "5/746, train_loss: 4.0224\n",
      "6/746, train_loss: 3.9609\n",
      "7/746, train_loss: 3.9295\n",
      "8/746, train_loss: 3.9929\n",
      "9/746, train_loss: 4.1303\n",
      "10/746, train_loss: 3.9789\n",
      "11/746, train_loss: 3.9057\n",
      "12/746, train_loss: 3.7725\n",
      "13/746, train_loss: 4.0987\n",
      "14/746, train_loss: 4.0007\n",
      "15/746, train_loss: 3.8715\n",
      "16/746, train_loss: 3.8558\n",
      "17/746, train_loss: 3.7451\n",
      "18/746, train_loss: 4.0001\n",
      "19/746, train_loss: 3.9204\n",
      "20/746, train_loss: 4.1045\n",
      "21/746, train_loss: 3.9166\n",
      "22/746, train_loss: 3.8156\n",
      "23/746, train_loss: 3.8340\n",
      "24/746, train_loss: 3.9324\n",
      "25/746, train_loss: 3.9016\n",
      "26/746, train_loss: 4.0872\n",
      "27/746, train_loss: 4.0391\n",
      "28/746, train_loss: 4.2032\n",
      "29/746, train_loss: 3.7583\n",
      "30/746, train_loss: 4.0356\n",
      "31/746, train_loss: 3.8583\n",
      "32/746, train_loss: 3.8895\n",
      "33/746, train_loss: 3.7540\n",
      "34/746, train_loss: 3.8871\n",
      "35/746, train_loss: 3.9566\n",
      "36/746, train_loss: 3.9158\n",
      "37/746, train_loss: 4.0551\n",
      "38/746, train_loss: 3.9798\n",
      "39/746, train_loss: 3.7946\n",
      "40/746, train_loss: 3.9184\n",
      "41/746, train_loss: 3.9347\n",
      "42/746, train_loss: 4.0706\n",
      "43/746, train_loss: 3.8827\n",
      "44/746, train_loss: 4.0780\n",
      "45/746, train_loss: 4.0796\n",
      "46/746, train_loss: 4.0706\n",
      "47/746, train_loss: 4.0242\n",
      "48/746, train_loss: 3.9141\n",
      "49/746, train_loss: 3.7646\n",
      "50/746, train_loss: 3.9115\n",
      "51/746, train_loss: 4.0805\n",
      "52/746, train_loss: 4.0191\n",
      "53/746, train_loss: 3.8363\n",
      "54/746, train_loss: 3.8528\n",
      "55/746, train_loss: 4.2289\n",
      "56/746, train_loss: 3.9488\n",
      "57/746, train_loss: 3.9529\n",
      "58/746, train_loss: 3.8307\n",
      "59/746, train_loss: 3.8464\n",
      "60/746, train_loss: 3.9180\n",
      "61/746, train_loss: 3.9282\n",
      "62/746, train_loss: 3.8009\n",
      "63/746, train_loss: 4.0388\n",
      "64/746, train_loss: 4.0053\n",
      "65/746, train_loss: 3.9694\n",
      "66/746, train_loss: 3.9437\n",
      "67/746, train_loss: 4.0845\n",
      "68/746, train_loss: 3.9460\n",
      "69/746, train_loss: 3.7185\n",
      "70/746, train_loss: 4.0269\n",
      "71/746, train_loss: 3.9145\n",
      "72/746, train_loss: 4.0039\n",
      "73/746, train_loss: 3.8245\n",
      "74/746, train_loss: 3.8969\n",
      "75/746, train_loss: 3.8693\n",
      "76/746, train_loss: 3.8079\n",
      "77/746, train_loss: 4.0170\n",
      "78/746, train_loss: 3.9870\n",
      "79/746, train_loss: 3.9797\n",
      "80/746, train_loss: 3.9944\n",
      "81/746, train_loss: 3.9584\n",
      "82/746, train_loss: 4.1484\n",
      "83/746, train_loss: 3.7453\n",
      "84/746, train_loss: 3.9227\n",
      "85/746, train_loss: 4.0148\n",
      "86/746, train_loss: 3.9989\n",
      "87/746, train_loss: 3.7601\n",
      "88/746, train_loss: 3.8972\n",
      "89/746, train_loss: 3.8874\n",
      "90/746, train_loss: 3.8718\n",
      "91/746, train_loss: 3.9105\n",
      "92/746, train_loss: 3.8579\n",
      "93/746, train_loss: 3.9943\n",
      "94/746, train_loss: 3.9566\n",
      "95/746, train_loss: 3.6595\n",
      "96/746, train_loss: 3.7258\n",
      "97/746, train_loss: 3.9338\n",
      "98/746, train_loss: 3.8586\n",
      "99/746, train_loss: 4.0618\n",
      "100/746, train_loss: 3.9750\n",
      "101/746, train_loss: 3.9031\n",
      "102/746, train_loss: 3.9178\n",
      "103/746, train_loss: 4.0985\n",
      "104/746, train_loss: 4.0062\n",
      "105/746, train_loss: 4.0894\n",
      "106/746, train_loss: 3.8417\n",
      "107/746, train_loss: 3.6714\n",
      "108/746, train_loss: 3.8027\n",
      "109/746, train_loss: 3.7913\n",
      "110/746, train_loss: 3.8930\n",
      "111/746, train_loss: 4.0927\n",
      "112/746, train_loss: 4.0286\n",
      "113/746, train_loss: 3.7154\n",
      "114/746, train_loss: 3.8019\n",
      "115/746, train_loss: 3.9376\n",
      "116/746, train_loss: 4.0248\n",
      "117/746, train_loss: 4.0210\n",
      "118/746, train_loss: 3.9373\n",
      "119/746, train_loss: 3.9914\n",
      "120/746, train_loss: 3.8900\n",
      "121/746, train_loss: 3.8879\n",
      "122/746, train_loss: 3.9425\n",
      "123/746, train_loss: 4.3108\n",
      "124/746, train_loss: 4.0700\n",
      "125/746, train_loss: 3.8410\n",
      "126/746, train_loss: 4.0049\n",
      "127/746, train_loss: 3.7257\n",
      "128/746, train_loss: 3.9590\n",
      "129/746, train_loss: 3.9587\n",
      "130/746, train_loss: 3.9512\n",
      "131/746, train_loss: 3.9297\n",
      "132/746, train_loss: 4.0691\n",
      "133/746, train_loss: 3.8753\n",
      "134/746, train_loss: 3.7004\n",
      "135/746, train_loss: 3.8216\n",
      "136/746, train_loss: 3.9437\n",
      "137/746, train_loss: 3.7832\n",
      "138/746, train_loss: 4.0996\n",
      "139/746, train_loss: 4.0828\n",
      "140/746, train_loss: 4.0342\n",
      "141/746, train_loss: 4.0268\n",
      "142/746, train_loss: 3.7531\n",
      "143/746, train_loss: 3.9085\n",
      "144/746, train_loss: 3.8134\n",
      "145/746, train_loss: 4.0988\n",
      "146/746, train_loss: 3.9026\n",
      "147/746, train_loss: 3.8389\n",
      "148/746, train_loss: 4.0311\n",
      "149/746, train_loss: 3.7632\n",
      "150/746, train_loss: 3.9882\n",
      "151/746, train_loss: 4.0101\n",
      "152/746, train_loss: 3.9842\n",
      "153/746, train_loss: 3.8022\n",
      "154/746, train_loss: 3.9449\n",
      "155/746, train_loss: 3.7797\n",
      "156/746, train_loss: 4.0145\n",
      "157/746, train_loss: 3.7447\n",
      "158/746, train_loss: 3.9385\n",
      "159/746, train_loss: 3.7580\n",
      "160/746, train_loss: 3.8316\n",
      "161/746, train_loss: 3.8636\n",
      "162/746, train_loss: 4.1161\n",
      "163/746, train_loss: 4.0080\n",
      "164/746, train_loss: 3.6729\n",
      "165/746, train_loss: 4.1093\n",
      "166/746, train_loss: 3.7975\n",
      "167/746, train_loss: 3.9602\n",
      "168/746, train_loss: 3.9203\n",
      "169/746, train_loss: 3.8054\n",
      "170/746, train_loss: 4.0413\n",
      "171/746, train_loss: 3.9059\n",
      "172/746, train_loss: 3.8632\n",
      "173/746, train_loss: 3.8435\n",
      "174/746, train_loss: 3.6826\n",
      "175/746, train_loss: 4.2224\n",
      "176/746, train_loss: 3.7443\n",
      "177/746, train_loss: 3.8061\n",
      "178/746, train_loss: 3.9700\n",
      "179/746, train_loss: 3.6539\n",
      "180/746, train_loss: 3.9437\n",
      "181/746, train_loss: 3.9336\n",
      "182/746, train_loss: 3.7078\n",
      "183/746, train_loss: 3.9093\n",
      "184/746, train_loss: 3.8965\n",
      "185/746, train_loss: 3.7487\n",
      "186/746, train_loss: 3.8814\n",
      "187/746, train_loss: 3.8750\n",
      "188/746, train_loss: 3.9511\n",
      "189/746, train_loss: 3.7874\n",
      "190/746, train_loss: 3.6905\n",
      "191/746, train_loss: 3.9660\n",
      "192/746, train_loss: 3.7486\n",
      "193/746, train_loss: 3.7323\n",
      "194/746, train_loss: 3.8217\n",
      "195/746, train_loss: 3.8335\n",
      "196/746, train_loss: 3.8727\n",
      "197/746, train_loss: 3.8704\n",
      "198/746, train_loss: 3.9757\n",
      "199/746, train_loss: 3.9171\n",
      "200/746, train_loss: 3.9373\n",
      "201/746, train_loss: 4.1591\n",
      "202/746, train_loss: 3.8928\n",
      "203/746, train_loss: 3.9280\n",
      "204/746, train_loss: 3.9006\n",
      "205/746, train_loss: 4.1990\n",
      "206/746, train_loss: 4.0874\n",
      "207/746, train_loss: 3.6605\n",
      "208/746, train_loss: 3.7532\n",
      "209/746, train_loss: 4.0089\n",
      "210/746, train_loss: 3.9796\n",
      "211/746, train_loss: 3.7001\n",
      "212/746, train_loss: 3.7803\n",
      "213/746, train_loss: 3.8753\n",
      "214/746, train_loss: 3.8411\n",
      "215/746, train_loss: 3.9850\n",
      "216/746, train_loss: 4.0164\n",
      "217/746, train_loss: 4.0260\n",
      "218/746, train_loss: 4.2097\n",
      "219/746, train_loss: 3.7412\n",
      "220/746, train_loss: 4.0136\n",
      "221/746, train_loss: 4.0407\n",
      "222/746, train_loss: 3.8818\n",
      "223/746, train_loss: 3.8845\n",
      "224/746, train_loss: 3.9261\n",
      "225/746, train_loss: 3.9937\n",
      "226/746, train_loss: 3.8275\n",
      "227/746, train_loss: 3.8660\n",
      "228/746, train_loss: 3.9744\n",
      "229/746, train_loss: 3.7760\n",
      "230/746, train_loss: 3.8803\n",
      "231/746, train_loss: 4.0326\n",
      "232/746, train_loss: 3.8727\n",
      "233/746, train_loss: 3.6727\n",
      "234/746, train_loss: 4.0164\n",
      "235/746, train_loss: 3.8277\n",
      "236/746, train_loss: 3.7554\n",
      "237/746, train_loss: 3.8882\n",
      "238/746, train_loss: 3.9319\n",
      "239/746, train_loss: 3.7841\n",
      "240/746, train_loss: 3.9998\n",
      "241/746, train_loss: 3.8509\n",
      "242/746, train_loss: 4.1060\n",
      "243/746, train_loss: 3.8208\n",
      "244/746, train_loss: 3.8202\n",
      "245/746, train_loss: 3.9201\n",
      "246/746, train_loss: 3.6401\n",
      "247/746, train_loss: 3.7224\n",
      "248/746, train_loss: 3.8057\n",
      "249/746, train_loss: 3.8956\n",
      "250/746, train_loss: 3.8867\n",
      "251/746, train_loss: 3.9795\n",
      "252/746, train_loss: 3.8495\n",
      "253/746, train_loss: 3.7104\n",
      "254/746, train_loss: 4.0021\n",
      "255/746, train_loss: 3.7699\n",
      "256/746, train_loss: 3.8961\n",
      "257/746, train_loss: 4.0183\n",
      "258/746, train_loss: 3.6503\n",
      "259/746, train_loss: 3.9875\n",
      "260/746, train_loss: 3.8927\n",
      "261/746, train_loss: 3.8104\n",
      "262/746, train_loss: 4.0439\n",
      "263/746, train_loss: 3.7653\n",
      "264/746, train_loss: 3.9051\n",
      "265/746, train_loss: 4.2964\n",
      "266/746, train_loss: 3.8311\n",
      "267/746, train_loss: 3.8952\n",
      "268/746, train_loss: 3.6623\n",
      "269/746, train_loss: 3.8293\n",
      "270/746, train_loss: 3.9999\n",
      "271/746, train_loss: 3.6615\n",
      "272/746, train_loss: 3.9737\n",
      "273/746, train_loss: 3.7195\n",
      "274/746, train_loss: 3.9051\n",
      "275/746, train_loss: 3.6772\n",
      "276/746, train_loss: 3.7377\n",
      "277/746, train_loss: 3.9235\n",
      "278/746, train_loss: 3.9455\n",
      "279/746, train_loss: 3.9428\n",
      "280/746, train_loss: 3.9358\n",
      "281/746, train_loss: 3.9589\n",
      "282/746, train_loss: 3.9918\n",
      "283/746, train_loss: 4.1219\n",
      "284/746, train_loss: 3.7963\n",
      "285/746, train_loss: 4.2499\n",
      "286/746, train_loss: 3.9161\n",
      "287/746, train_loss: 3.8710\n",
      "288/746, train_loss: 4.0007\n",
      "289/746, train_loss: 3.9302\n",
      "290/746, train_loss: 3.8316\n",
      "291/746, train_loss: 3.8850\n",
      "292/746, train_loss: 4.0033\n",
      "293/746, train_loss: 3.7002\n",
      "294/746, train_loss: 3.7400\n",
      "295/746, train_loss: 3.9798\n",
      "296/746, train_loss: 3.9409\n",
      "297/746, train_loss: 4.1314\n",
      "298/746, train_loss: 4.0822\n",
      "299/746, train_loss: 3.7554\n",
      "300/746, train_loss: 3.6891\n",
      "301/746, train_loss: 3.7901\n",
      "302/746, train_loss: 3.7919\n",
      "303/746, train_loss: 3.8368\n",
      "304/746, train_loss: 3.8854\n",
      "305/746, train_loss: 4.0219\n",
      "306/746, train_loss: 3.6999\n",
      "307/746, train_loss: 4.0618\n",
      "308/746, train_loss: 3.8516\n",
      "309/746, train_loss: 3.7273\n",
      "310/746, train_loss: 3.9864\n",
      "311/746, train_loss: 4.1475\n",
      "312/746, train_loss: 3.8699\n",
      "313/746, train_loss: 3.5601\n",
      "314/746, train_loss: 3.9352\n",
      "315/746, train_loss: 3.8090\n",
      "316/746, train_loss: 3.8920\n",
      "317/746, train_loss: 3.6804\n",
      "318/746, train_loss: 3.8690\n",
      "319/746, train_loss: 3.7803\n",
      "320/746, train_loss: 3.7630\n",
      "321/746, train_loss: 3.8944\n",
      "322/746, train_loss: 4.0023\n",
      "323/746, train_loss: 3.8475\n",
      "324/746, train_loss: 3.8840\n",
      "325/746, train_loss: 3.5987\n",
      "326/746, train_loss: 3.7984\n",
      "327/746, train_loss: 3.6912\n",
      "328/746, train_loss: 3.9236\n",
      "329/746, train_loss: 3.9476\n",
      "330/746, train_loss: 3.8008\n",
      "331/746, train_loss: 3.9619\n",
      "332/746, train_loss: 3.9551\n",
      "333/746, train_loss: 4.0036\n",
      "334/746, train_loss: 3.7628\n",
      "335/746, train_loss: 3.5481\n",
      "336/746, train_loss: 3.8364\n",
      "337/746, train_loss: 3.8334\n",
      "338/746, train_loss: 3.9771\n",
      "339/746, train_loss: 3.8469\n",
      "340/746, train_loss: 4.0613\n",
      "341/746, train_loss: 3.7467\n",
      "342/746, train_loss: 3.8987\n",
      "343/746, train_loss: 3.8056\n",
      "344/746, train_loss: 3.7736\n",
      "345/746, train_loss: 3.9298\n",
      "346/746, train_loss: 4.1090\n",
      "347/746, train_loss: 3.6203\n",
      "348/746, train_loss: 3.7683\n",
      "349/746, train_loss: 3.8230\n",
      "350/746, train_loss: 3.8875\n",
      "351/746, train_loss: 3.9839\n",
      "352/746, train_loss: 3.6441\n",
      "353/746, train_loss: 3.8450\n",
      "354/746, train_loss: 3.7720\n",
      "355/746, train_loss: 3.9736\n",
      "356/746, train_loss: 4.1114\n",
      "357/746, train_loss: 3.5115\n",
      "358/746, train_loss: 3.9449\n",
      "359/746, train_loss: 3.8864\n",
      "360/746, train_loss: 4.1622\n",
      "361/746, train_loss: 3.9964\n",
      "362/746, train_loss: 3.7814\n",
      "363/746, train_loss: 3.9996\n",
      "364/746, train_loss: 4.0939\n",
      "365/746, train_loss: 3.8067\n",
      "366/746, train_loss: 3.8632\n",
      "367/746, train_loss: 3.9030\n",
      "368/746, train_loss: 3.9780\n",
      "369/746, train_loss: 3.8881\n",
      "370/746, train_loss: 3.8050\n",
      "371/746, train_loss: 3.8883\n",
      "372/746, train_loss: 3.8302\n",
      "373/746, train_loss: 3.7999\n",
      "374/746, train_loss: 3.8037\n",
      "375/746, train_loss: 3.7363\n",
      "376/746, train_loss: 3.9042\n",
      "377/746, train_loss: 4.0442\n",
      "378/746, train_loss: 3.8483\n",
      "379/746, train_loss: 3.5987\n",
      "380/746, train_loss: 3.9941\n",
      "381/746, train_loss: 3.9108\n",
      "382/746, train_loss: 3.8407\n",
      "383/746, train_loss: 3.9109\n",
      "384/746, train_loss: 3.9133\n",
      "385/746, train_loss: 3.6960\n",
      "386/746, train_loss: 3.8943\n",
      "387/746, train_loss: 3.5588\n",
      "388/746, train_loss: 3.9576\n",
      "389/746, train_loss: 4.0905\n",
      "390/746, train_loss: 3.8294\n",
      "391/746, train_loss: 3.9530\n",
      "392/746, train_loss: 3.7695\n",
      "393/746, train_loss: 3.9159\n",
      "394/746, train_loss: 3.6389\n",
      "395/746, train_loss: 4.0949\n",
      "396/746, train_loss: 3.8365\n",
      "397/746, train_loss: 3.8450\n",
      "398/746, train_loss: 3.9952\n",
      "399/746, train_loss: 3.6509\n",
      "400/746, train_loss: 3.7004\n",
      "401/746, train_loss: 3.9551\n",
      "402/746, train_loss: 4.0113\n",
      "403/746, train_loss: 3.8793\n",
      "404/746, train_loss: 3.6318\n",
      "405/746, train_loss: 4.0219\n",
      "406/746, train_loss: 3.8898\n",
      "407/746, train_loss: 3.7937\n",
      "408/746, train_loss: 3.8678\n",
      "409/746, train_loss: 3.9103\n",
      "410/746, train_loss: 3.8052\n",
      "411/746, train_loss: 3.9755\n",
      "412/746, train_loss: 3.8660\n",
      "413/746, train_loss: 3.9023\n",
      "414/746, train_loss: 3.6686\n",
      "415/746, train_loss: 3.7628\n",
      "416/746, train_loss: 3.8365\n",
      "417/746, train_loss: 3.7123\n",
      "418/746, train_loss: 4.0163\n",
      "419/746, train_loss: 3.9106\n",
      "420/746, train_loss: 3.7239\n",
      "421/746, train_loss: 3.7790\n",
      "422/746, train_loss: 3.9618\n",
      "423/746, train_loss: 3.9856\n",
      "424/746, train_loss: 3.9398\n",
      "425/746, train_loss: 3.9601\n",
      "426/746, train_loss: 3.6429\n",
      "427/746, train_loss: 3.7366\n",
      "428/746, train_loss: 3.8177\n",
      "429/746, train_loss: 3.7853\n",
      "430/746, train_loss: 3.7522\n",
      "431/746, train_loss: 3.7572\n",
      "432/746, train_loss: 3.9006\n",
      "433/746, train_loss: 3.6437\n",
      "434/746, train_loss: 3.6884\n",
      "435/746, train_loss: 3.9779\n",
      "436/746, train_loss: 3.9662\n",
      "437/746, train_loss: 3.8857\n",
      "438/746, train_loss: 3.7763\n",
      "439/746, train_loss: 3.6928\n",
      "440/746, train_loss: 3.6022\n",
      "441/746, train_loss: 3.6385\n",
      "442/746, train_loss: 3.8574\n",
      "443/746, train_loss: 3.8789\n",
      "444/746, train_loss: 3.8437\n",
      "445/746, train_loss: 3.8287\n",
      "446/746, train_loss: 3.8113\n",
      "447/746, train_loss: 3.9343\n",
      "448/746, train_loss: 3.8007\n",
      "449/746, train_loss: 3.7944\n",
      "450/746, train_loss: 3.7214\n",
      "451/746, train_loss: 4.0181\n",
      "452/746, train_loss: 3.8514\n",
      "453/746, train_loss: 3.9655\n",
      "454/746, train_loss: 3.9638\n",
      "455/746, train_loss: 3.5779\n",
      "456/746, train_loss: 3.8959\n",
      "457/746, train_loss: 3.7637\n",
      "458/746, train_loss: 3.6760\n",
      "459/746, train_loss: 3.7777\n",
      "460/746, train_loss: 3.8327\n",
      "461/746, train_loss: 3.8760\n",
      "462/746, train_loss: 3.9915\n",
      "463/746, train_loss: 3.7301\n",
      "464/746, train_loss: 3.8993\n",
      "465/746, train_loss: 3.8618\n",
      "466/746, train_loss: 3.5474\n",
      "467/746, train_loss: 3.6507\n",
      "468/746, train_loss: 3.7297\n",
      "469/746, train_loss: 4.0191\n",
      "470/746, train_loss: 3.8717\n",
      "471/746, train_loss: 3.9162\n",
      "472/746, train_loss: 3.6632\n",
      "473/746, train_loss: 3.6514\n",
      "474/746, train_loss: 3.8949\n",
      "475/746, train_loss: 3.7390\n",
      "476/746, train_loss: 3.9474\n",
      "477/746, train_loss: 3.7225\n",
      "478/746, train_loss: 3.9800\n",
      "479/746, train_loss: 3.8111\n",
      "480/746, train_loss: 3.7736\n",
      "481/746, train_loss: 3.7281\n",
      "482/746, train_loss: 3.7582\n",
      "483/746, train_loss: 3.9194\n",
      "484/746, train_loss: 3.9460\n",
      "485/746, train_loss: 3.8775\n",
      "486/746, train_loss: 3.8014\n",
      "487/746, train_loss: 3.7626\n",
      "488/746, train_loss: 3.5299\n",
      "489/746, train_loss: 3.8503\n",
      "490/746, train_loss: 3.6872\n",
      "491/746, train_loss: 3.7981\n",
      "492/746, train_loss: 3.9639\n",
      "493/746, train_loss: 3.9238\n",
      "494/746, train_loss: 3.9418\n",
      "495/746, train_loss: 3.9280\n",
      "496/746, train_loss: 3.8766\n",
      "497/746, train_loss: 3.9325\n",
      "498/746, train_loss: 3.9639\n",
      "499/746, train_loss: 3.6183\n",
      "500/746, train_loss: 3.9369\n",
      "501/746, train_loss: 4.0128\n",
      "502/746, train_loss: 4.0182\n",
      "503/746, train_loss: 3.7315\n",
      "504/746, train_loss: 3.9012\n",
      "505/746, train_loss: 3.8155\n",
      "506/746, train_loss: 3.6650\n",
      "507/746, train_loss: 3.7486\n",
      "508/746, train_loss: 3.9062\n",
      "509/746, train_loss: 3.7520\n",
      "510/746, train_loss: 3.9325\n",
      "511/746, train_loss: 3.7069\n",
      "512/746, train_loss: 3.5971\n",
      "513/746, train_loss: 3.8160\n",
      "514/746, train_loss: 3.9510\n",
      "515/746, train_loss: 3.5781\n",
      "516/746, train_loss: 3.7514\n",
      "517/746, train_loss: 3.6976\n",
      "518/746, train_loss: 3.8603\n",
      "519/746, train_loss: 3.8604\n",
      "520/746, train_loss: 3.7893\n",
      "521/746, train_loss: 4.1206\n",
      "522/746, train_loss: 3.8792\n",
      "523/746, train_loss: 3.7540\n",
      "524/746, train_loss: 3.7163\n",
      "525/746, train_loss: 3.8225\n",
      "526/746, train_loss: 3.7750\n",
      "527/746, train_loss: 3.8049\n",
      "528/746, train_loss: 3.7340\n",
      "529/746, train_loss: 4.0342\n",
      "530/746, train_loss: 3.8358\n",
      "531/746, train_loss: 3.7259\n",
      "532/746, train_loss: 3.6658\n",
      "533/746, train_loss: 3.7622\n",
      "534/746, train_loss: 3.9099\n",
      "535/746, train_loss: 3.8918\n",
      "536/746, train_loss: 3.8560\n",
      "537/746, train_loss: 3.7759\n",
      "538/746, train_loss: 3.8444\n",
      "539/746, train_loss: 3.6644\n",
      "540/746, train_loss: 3.5590\n",
      "541/746, train_loss: 3.7424\n",
      "542/746, train_loss: 3.7529\n",
      "543/746, train_loss: 3.9393\n",
      "544/746, train_loss: 3.8364\n",
      "545/746, train_loss: 3.7170\n",
      "546/746, train_loss: 3.7603\n",
      "547/746, train_loss: 3.9473\n",
      "548/746, train_loss: 3.8586\n",
      "549/746, train_loss: 3.5380\n",
      "550/746, train_loss: 3.9061\n",
      "551/746, train_loss: 3.7947\n",
      "552/746, train_loss: 3.7623\n",
      "553/746, train_loss: 3.7852\n",
      "554/746, train_loss: 3.8586\n",
      "555/746, train_loss: 3.6822\n",
      "556/746, train_loss: 3.5854\n",
      "557/746, train_loss: 3.6346\n",
      "558/746, train_loss: 3.8147\n",
      "559/746, train_loss: 3.5927\n",
      "560/746, train_loss: 3.7542\n",
      "561/746, train_loss: 3.6737\n",
      "562/746, train_loss: 3.8963\n",
      "563/746, train_loss: 3.8391\n",
      "564/746, train_loss: 3.9144\n",
      "565/746, train_loss: 3.6268\n",
      "566/746, train_loss: 3.7908\n",
      "567/746, train_loss: 3.7856\n",
      "568/746, train_loss: 3.7866\n",
      "569/746, train_loss: 3.5879\n",
      "570/746, train_loss: 3.9492\n",
      "571/746, train_loss: 3.8414\n",
      "572/746, train_loss: 3.8106\n",
      "573/746, train_loss: 3.7308\n",
      "574/746, train_loss: 3.9255\n",
      "575/746, train_loss: 3.8912\n",
      "576/746, train_loss: 3.6516\n",
      "577/746, train_loss: 3.7855\n",
      "578/746, train_loss: 3.8554\n",
      "579/746, train_loss: 3.8112\n",
      "580/746, train_loss: 3.9681\n",
      "581/746, train_loss: 3.6543\n",
      "582/746, train_loss: 3.6999\n",
      "583/746, train_loss: 3.7810\n",
      "584/746, train_loss: 3.8681\n",
      "585/746, train_loss: 3.8299\n",
      "586/746, train_loss: 3.7569\n",
      "587/746, train_loss: 3.7743\n",
      "588/746, train_loss: 3.9049\n",
      "589/746, train_loss: 3.7663\n",
      "590/746, train_loss: 3.9128\n",
      "591/746, train_loss: 3.7887\n",
      "592/746, train_loss: 3.9127\n",
      "593/746, train_loss: 3.8446\n",
      "594/746, train_loss: 3.9391\n",
      "595/746, train_loss: 3.8578\n",
      "596/746, train_loss: 3.7181\n",
      "597/746, train_loss: 3.8930\n",
      "598/746, train_loss: 3.9541\n",
      "599/746, train_loss: 3.6252\n",
      "600/746, train_loss: 3.8905\n",
      "601/746, train_loss: 3.9078\n",
      "602/746, train_loss: 3.9872\n",
      "603/746, train_loss: 3.8665\n",
      "604/746, train_loss: 3.7403\n",
      "605/746, train_loss: 3.9259\n",
      "606/746, train_loss: 3.6529\n",
      "607/746, train_loss: 3.9386\n",
      "608/746, train_loss: 3.7310\n",
      "609/746, train_loss: 3.6377\n",
      "610/746, train_loss: 3.6634\n",
      "611/746, train_loss: 3.8239\n",
      "612/746, train_loss: 4.0142\n",
      "613/746, train_loss: 3.7224\n",
      "614/746, train_loss: 3.6473\n",
      "615/746, train_loss: 3.9141\n",
      "616/746, train_loss: 3.6992\n",
      "617/746, train_loss: 3.8714\n",
      "618/746, train_loss: 3.7507\n",
      "619/746, train_loss: 3.6003\n",
      "620/746, train_loss: 3.8981\n",
      "621/746, train_loss: 3.7649\n",
      "622/746, train_loss: 3.6769\n",
      "623/746, train_loss: 3.8315\n",
      "624/746, train_loss: 3.7893\n",
      "625/746, train_loss: 3.7040\n",
      "626/746, train_loss: 3.6592\n",
      "627/746, train_loss: 3.7504\n",
      "628/746, train_loss: 3.7450\n",
      "629/746, train_loss: 3.6131\n",
      "630/746, train_loss: 3.7628\n",
      "631/746, train_loss: 3.7637\n",
      "632/746, train_loss: 3.9759\n",
      "633/746, train_loss: 3.5543\n",
      "634/746, train_loss: 3.6500\n",
      "635/746, train_loss: 3.6856\n",
      "636/746, train_loss: 3.7055\n",
      "637/746, train_loss: 3.8994\n",
      "638/746, train_loss: 3.7386\n",
      "639/746, train_loss: 3.8033\n",
      "640/746, train_loss: 3.6410\n",
      "641/746, train_loss: 3.6639\n",
      "642/746, train_loss: 3.8485\n",
      "643/746, train_loss: 4.0740\n",
      "644/746, train_loss: 3.7008\n",
      "645/746, train_loss: 3.7830\n",
      "646/746, train_loss: 3.8975\n",
      "647/746, train_loss: 3.6191\n",
      "648/746, train_loss: 3.5268\n",
      "649/746, train_loss: 3.6355\n",
      "650/746, train_loss: 3.9112\n",
      "651/746, train_loss: 3.7114\n",
      "652/746, train_loss: 3.9808\n",
      "653/746, train_loss: 3.6035\n",
      "654/746, train_loss: 3.9625\n",
      "655/746, train_loss: 3.6610\n",
      "656/746, train_loss: 3.8705\n",
      "657/746, train_loss: 3.8715\n",
      "658/746, train_loss: 3.9416\n",
      "659/746, train_loss: 3.8178\n",
      "660/746, train_loss: 3.8138\n",
      "661/746, train_loss: 3.6288\n",
      "662/746, train_loss: 3.8121\n",
      "663/746, train_loss: 3.7650\n",
      "664/746, train_loss: 3.6761\n",
      "665/746, train_loss: 3.7867\n",
      "666/746, train_loss: 3.6428\n",
      "667/746, train_loss: 3.6609\n",
      "668/746, train_loss: 3.5920\n",
      "669/746, train_loss: 3.9057\n",
      "670/746, train_loss: 3.6536\n",
      "671/746, train_loss: 3.6581\n",
      "672/746, train_loss: 3.8156\n",
      "673/746, train_loss: 3.8015\n",
      "674/746, train_loss: 3.7428\n",
      "675/746, train_loss: 3.7984\n",
      "676/746, train_loss: 4.1850\n",
      "677/746, train_loss: 3.6786\n",
      "678/746, train_loss: 3.5852\n",
      "679/746, train_loss: 3.6332\n",
      "680/746, train_loss: 3.7548\n",
      "681/746, train_loss: 3.5397\n",
      "682/746, train_loss: 3.7657\n",
      "683/746, train_loss: 3.6034\n",
      "684/746, train_loss: 3.6533\n",
      "685/746, train_loss: 3.5237\n",
      "686/746, train_loss: 3.5479\n",
      "687/746, train_loss: 3.9345\n",
      "688/746, train_loss: 3.8258\n",
      "689/746, train_loss: 3.8240\n",
      "690/746, train_loss: 3.7638\n",
      "691/746, train_loss: 3.5614\n",
      "692/746, train_loss: 3.8861\n",
      "693/746, train_loss: 3.8228\n",
      "694/746, train_loss: 3.7709\n",
      "695/746, train_loss: 3.5979\n",
      "696/746, train_loss: 3.7575\n",
      "697/746, train_loss: 3.8961\n",
      "698/746, train_loss: 3.6718\n",
      "699/746, train_loss: 3.7802\n",
      "700/746, train_loss: 3.9156\n",
      "701/746, train_loss: 3.6304\n",
      "702/746, train_loss: 3.7508\n",
      "703/746, train_loss: 3.7820\n",
      "704/746, train_loss: 3.6956\n",
      "705/746, train_loss: 3.9565\n",
      "706/746, train_loss: 3.7721\n",
      "707/746, train_loss: 3.6483\n",
      "708/746, train_loss: 3.7269\n",
      "709/746, train_loss: 3.7935\n",
      "710/746, train_loss: 3.8151\n",
      "711/746, train_loss: 3.8560\n",
      "712/746, train_loss: 3.8630\n",
      "713/746, train_loss: 3.5688\n",
      "714/746, train_loss: 3.7969\n",
      "715/746, train_loss: 3.7577\n",
      "716/746, train_loss: 3.7918\n",
      "717/746, train_loss: 3.9815\n",
      "718/746, train_loss: 3.9106\n",
      "719/746, train_loss: 3.7962\n",
      "720/746, train_loss: 3.5638\n",
      "721/746, train_loss: 3.8444\n",
      "722/746, train_loss: 3.7216\n",
      "723/746, train_loss: 3.9323\n",
      "724/746, train_loss: 3.5419\n",
      "725/746, train_loss: 3.7137\n",
      "726/746, train_loss: 3.9816\n",
      "727/746, train_loss: 3.6164\n",
      "728/746, train_loss: 3.7037\n",
      "729/746, train_loss: 3.4754\n",
      "730/746, train_loss: 3.6710\n",
      "731/746, train_loss: 3.6914\n",
      "732/746, train_loss: 3.9339\n",
      "733/746, train_loss: 3.3981\n",
      "734/746, train_loss: 3.9436\n",
      "735/746, train_loss: 3.5175\n",
      "736/746, train_loss: 3.7713\n",
      "737/746, train_loss: 3.7271\n",
      "738/746, train_loss: 3.4534\n",
      "739/746, train_loss: 3.9074\n",
      "740/746, train_loss: 3.5332\n",
      "741/746, train_loss: 3.7099\n",
      "742/746, train_loss: 3.7826\n",
      "743/746, train_loss: 3.8566\n",
      "744/746, train_loss: 3.6443\n",
      "745/746, train_loss: 3.5725\n",
      "746/746, train_loss: 3.7347\n",
      "747/746, train_loss: 3.9216\n",
      "epoch 8 average loss: 3.8485\n",
      "saved new best metric model\n",
      "current epoch: 8 current AUC: 0.9637 current accuracy: 0.3805 best AUC: 0.3805 at epoch: 8\n",
      "----------\n",
      "epoch 9/10\n",
      "1/746, train_loss: 3.7558\n",
      "2/746, train_loss: 3.7337\n",
      "3/746, train_loss: 3.9172\n",
      "4/746, train_loss: 3.5107\n",
      "5/746, train_loss: 3.6988\n",
      "6/746, train_loss: 3.7041\n",
      "7/746, train_loss: 3.5900\n",
      "8/746, train_loss: 3.8569\n",
      "9/746, train_loss: 3.4611\n",
      "10/746, train_loss: 3.5287\n",
      "11/746, train_loss: 3.7570\n",
      "12/746, train_loss: 3.8234\n",
      "13/746, train_loss: 3.6500\n",
      "14/746, train_loss: 3.7510\n",
      "15/746, train_loss: 3.3978\n",
      "16/746, train_loss: 3.4798\n",
      "17/746, train_loss: 3.7842\n",
      "18/746, train_loss: 3.8670\n",
      "19/746, train_loss: 3.7390\n",
      "20/746, train_loss: 3.8887\n",
      "21/746, train_loss: 3.5522\n",
      "22/746, train_loss: 3.4090\n",
      "23/746, train_loss: 3.7635\n",
      "24/746, train_loss: 3.7166\n",
      "25/746, train_loss: 3.6059\n",
      "26/746, train_loss: 3.7101\n",
      "27/746, train_loss: 3.5834\n",
      "28/746, train_loss: 3.4974\n",
      "29/746, train_loss: 3.5491\n",
      "30/746, train_loss: 3.4456\n",
      "31/746, train_loss: 3.6967\n",
      "32/746, train_loss: 3.6823\n",
      "33/746, train_loss: 3.5128\n",
      "34/746, train_loss: 3.3569\n",
      "35/746, train_loss: 3.7703\n",
      "36/746, train_loss: 3.6421\n",
      "37/746, train_loss: 3.4997\n",
      "38/746, train_loss: 3.6503\n",
      "39/746, train_loss: 3.6056\n",
      "40/746, train_loss: 3.7435\n",
      "41/746, train_loss: 3.6521\n",
      "42/746, train_loss: 3.6783\n",
      "43/746, train_loss: 3.5808\n",
      "44/746, train_loss: 3.7008\n",
      "45/746, train_loss: 3.6652\n",
      "46/746, train_loss: 3.7891\n",
      "47/746, train_loss: 3.8143\n",
      "48/746, train_loss: 3.5982\n",
      "49/746, train_loss: 3.7714\n",
      "50/746, train_loss: 3.5915\n",
      "51/746, train_loss: 3.7074\n",
      "52/746, train_loss: 3.5454\n",
      "53/746, train_loss: 3.7615\n",
      "54/746, train_loss: 3.5697\n",
      "55/746, train_loss: 3.7723\n",
      "56/746, train_loss: 3.6576\n",
      "57/746, train_loss: 3.6778\n",
      "58/746, train_loss: 3.6816\n",
      "59/746, train_loss: 3.5508\n",
      "60/746, train_loss: 3.5361\n",
      "61/746, train_loss: 3.5291\n",
      "62/746, train_loss: 3.5159\n",
      "63/746, train_loss: 3.7897\n",
      "64/746, train_loss: 3.4838\n",
      "65/746, train_loss: 3.5133\n",
      "66/746, train_loss: 3.8066\n",
      "67/746, train_loss: 3.5529\n",
      "68/746, train_loss: 3.4410\n",
      "69/746, train_loss: 3.6678\n",
      "70/746, train_loss: 3.6215\n",
      "71/746, train_loss: 3.5355\n",
      "72/746, train_loss: 3.5524\n",
      "73/746, train_loss: 3.6318\n",
      "74/746, train_loss: 3.6168\n",
      "75/746, train_loss: 3.7297\n",
      "76/746, train_loss: 3.4284\n",
      "77/746, train_loss: 3.3709\n",
      "78/746, train_loss: 3.6800\n",
      "79/746, train_loss: 3.6668\n",
      "80/746, train_loss: 3.3307\n",
      "81/746, train_loss: 3.7992\n",
      "82/746, train_loss: 3.5974\n",
      "83/746, train_loss: 3.4854\n",
      "84/746, train_loss: 3.6952\n",
      "85/746, train_loss: 3.6375\n",
      "86/746, train_loss: 3.7036\n",
      "87/746, train_loss: 3.4522\n",
      "88/746, train_loss: 3.8347\n",
      "89/746, train_loss: 3.5776\n",
      "90/746, train_loss: 3.5086\n",
      "91/746, train_loss: 3.6126\n",
      "92/746, train_loss: 3.5028\n",
      "93/746, train_loss: 3.6788\n",
      "94/746, train_loss: 3.6626\n",
      "95/746, train_loss: 3.6010\n",
      "96/746, train_loss: 3.6501\n",
      "97/746, train_loss: 3.5883\n",
      "98/746, train_loss: 3.5480\n",
      "99/746, train_loss: 3.6921\n",
      "100/746, train_loss: 3.9894\n",
      "101/746, train_loss: 3.6371\n",
      "102/746, train_loss: 3.6576\n",
      "103/746, train_loss: 3.9300\n",
      "104/746, train_loss: 3.8071\n",
      "105/746, train_loss: 3.4889\n",
      "106/746, train_loss: 3.6667\n",
      "107/746, train_loss: 3.5194\n",
      "108/746, train_loss: 3.7107\n",
      "109/746, train_loss: 3.5239\n",
      "110/746, train_loss: 3.6419\n",
      "111/746, train_loss: 3.6749\n",
      "112/746, train_loss: 3.7223\n",
      "113/746, train_loss: 3.5060\n",
      "114/746, train_loss: 3.7587\n",
      "115/746, train_loss: 3.6456\n",
      "116/746, train_loss: 3.6963\n",
      "117/746, train_loss: 3.6441\n",
      "118/746, train_loss: 3.5192\n",
      "119/746, train_loss: 3.8670\n",
      "120/746, train_loss: 3.6564\n",
      "121/746, train_loss: 3.6379\n",
      "122/746, train_loss: 3.7534\n",
      "123/746, train_loss: 3.7816\n",
      "124/746, train_loss: 3.6086\n",
      "125/746, train_loss: 3.3982\n",
      "126/746, train_loss: 3.4501\n",
      "127/746, train_loss: 3.5951\n",
      "128/746, train_loss: 3.6989\n",
      "129/746, train_loss: 3.6799\n",
      "130/746, train_loss: 3.6345\n",
      "131/746, train_loss: 3.4733\n",
      "132/746, train_loss: 3.8130\n",
      "133/746, train_loss: 3.6599\n",
      "134/746, train_loss: 3.6794\n",
      "135/746, train_loss: 3.6255\n",
      "136/746, train_loss: 3.4641\n",
      "137/746, train_loss: 3.8178\n",
      "138/746, train_loss: 3.5216\n",
      "139/746, train_loss: 3.7120\n",
      "140/746, train_loss: 3.5207\n",
      "141/746, train_loss: 3.7582\n",
      "142/746, train_loss: 3.6293\n",
      "143/746, train_loss: 3.7232\n",
      "144/746, train_loss: 3.7288\n",
      "145/746, train_loss: 3.6674\n",
      "146/746, train_loss: 3.5463\n",
      "147/746, train_loss: 3.4423\n",
      "148/746, train_loss: 3.4978\n",
      "149/746, train_loss: 3.6299\n",
      "150/746, train_loss: 3.8214\n",
      "151/746, train_loss: 3.5112\n",
      "152/746, train_loss: 3.6216\n",
      "153/746, train_loss: 3.7173\n",
      "154/746, train_loss: 3.6150\n",
      "155/746, train_loss: 3.5978\n",
      "156/746, train_loss: 3.4899\n",
      "157/746, train_loss: 3.4711\n",
      "158/746, train_loss: 3.6754\n",
      "159/746, train_loss: 3.7732\n",
      "160/746, train_loss: 3.5175\n",
      "161/746, train_loss: 3.5652\n",
      "162/746, train_loss: 3.7409\n",
      "163/746, train_loss: 3.6776\n",
      "164/746, train_loss: 3.6180\n",
      "165/746, train_loss: 3.6543\n",
      "166/746, train_loss: 3.3645\n",
      "167/746, train_loss: 3.7435\n",
      "168/746, train_loss: 3.6526\n",
      "169/746, train_loss: 3.7804\n",
      "170/746, train_loss: 3.5345\n",
      "171/746, train_loss: 3.5363\n",
      "172/746, train_loss: 3.6814\n",
      "173/746, train_loss: 3.6544\n",
      "174/746, train_loss: 3.7457\n",
      "175/746, train_loss: 3.5456\n",
      "176/746, train_loss: 3.5059\n",
      "177/746, train_loss: 3.5632\n",
      "178/746, train_loss: 3.4990\n",
      "179/746, train_loss: 3.6374\n",
      "180/746, train_loss: 3.7593\n",
      "181/746, train_loss: 3.6058\n",
      "182/746, train_loss: 3.3264\n",
      "183/746, train_loss: 3.7074\n",
      "184/746, train_loss: 3.5590\n",
      "185/746, train_loss: 3.6828\n",
      "186/746, train_loss: 3.6244\n",
      "187/746, train_loss: 3.6030\n",
      "188/746, train_loss: 3.5980\n",
      "189/746, train_loss: 3.4262\n",
      "190/746, train_loss: 3.7263\n",
      "191/746, train_loss: 3.4877\n",
      "192/746, train_loss: 3.2552\n",
      "193/746, train_loss: 3.5222\n",
      "194/746, train_loss: 3.6801\n",
      "195/746, train_loss: 3.6712\n",
      "196/746, train_loss: 3.4501\n",
      "197/746, train_loss: 3.6783\n",
      "198/746, train_loss: 3.5845\n",
      "199/746, train_loss: 3.5886\n",
      "200/746, train_loss: 3.4862\n",
      "201/746, train_loss: 3.6458\n",
      "202/746, train_loss: 3.5039\n",
      "203/746, train_loss: 3.7477\n",
      "204/746, train_loss: 3.6098\n",
      "205/746, train_loss: 3.5613\n",
      "206/746, train_loss: 3.7431\n",
      "207/746, train_loss: 3.5674\n",
      "208/746, train_loss: 3.5844\n",
      "209/746, train_loss: 3.7516\n",
      "210/746, train_loss: 3.5357\n",
      "211/746, train_loss: 3.3593\n",
      "212/746, train_loss: 3.6520\n",
      "213/746, train_loss: 3.6353\n",
      "214/746, train_loss: 3.7694\n",
      "215/746, train_loss: 3.7460\n",
      "216/746, train_loss: 3.6121\n",
      "217/746, train_loss: 3.6438\n",
      "218/746, train_loss: 3.5451\n",
      "219/746, train_loss: 3.6422\n",
      "220/746, train_loss: 3.6551\n",
      "221/746, train_loss: 3.6116\n",
      "222/746, train_loss: 3.6962\n",
      "223/746, train_loss: 3.7009\n",
      "224/746, train_loss: 3.6078\n",
      "225/746, train_loss: 3.3798\n",
      "226/746, train_loss: 3.6276\n",
      "227/746, train_loss: 3.7654\n",
      "228/746, train_loss: 3.6127\n",
      "229/746, train_loss: 3.6285\n",
      "230/746, train_loss: 3.7913\n",
      "231/746, train_loss: 3.2686\n",
      "232/746, train_loss: 3.5663\n",
      "233/746, train_loss: 3.6910\n",
      "234/746, train_loss: 3.7528\n",
      "235/746, train_loss: 3.6935\n",
      "236/746, train_loss: 3.5388\n",
      "237/746, train_loss: 3.4347\n",
      "238/746, train_loss: 3.3797\n",
      "239/746, train_loss: 3.6147\n",
      "240/746, train_loss: 3.7093\n",
      "241/746, train_loss: 3.7540\n",
      "242/746, train_loss: 3.6495\n",
      "243/746, train_loss: 3.4654\n",
      "244/746, train_loss: 3.9641\n",
      "245/746, train_loss: 3.4873\n",
      "246/746, train_loss: 3.6017\n",
      "247/746, train_loss: 3.5314\n",
      "248/746, train_loss: 3.6269\n",
      "249/746, train_loss: 3.3448\n",
      "250/746, train_loss: 3.7738\n",
      "251/746, train_loss: 3.4996\n",
      "252/746, train_loss: 3.7174\n",
      "253/746, train_loss: 3.8157\n",
      "254/746, train_loss: 3.5070\n",
      "255/746, train_loss: 3.5089\n",
      "256/746, train_loss: 3.2721\n",
      "257/746, train_loss: 3.8149\n",
      "258/746, train_loss: 3.5406\n",
      "259/746, train_loss: 3.4774\n",
      "260/746, train_loss: 3.4580\n",
      "261/746, train_loss: 3.7587\n",
      "262/746, train_loss: 3.4996\n",
      "263/746, train_loss: 3.6912\n",
      "264/746, train_loss: 3.7690\n",
      "265/746, train_loss: 3.7062\n",
      "266/746, train_loss: 3.5374\n",
      "267/746, train_loss: 3.4337\n",
      "268/746, train_loss: 3.4073\n",
      "269/746, train_loss: 3.7618\n",
      "270/746, train_loss: 3.5799\n",
      "271/746, train_loss: 3.5691\n",
      "272/746, train_loss: 3.5312\n",
      "273/746, train_loss: 3.4735\n",
      "274/746, train_loss: 3.6004\n",
      "275/746, train_loss: 3.4241\n",
      "276/746, train_loss: 3.5119\n",
      "277/746, train_loss: 3.7138\n",
      "278/746, train_loss: 3.6950\n",
      "279/746, train_loss: 3.6542\n",
      "280/746, train_loss: 3.5879\n",
      "281/746, train_loss: 3.6611\n",
      "282/746, train_loss: 3.5345\n",
      "283/746, train_loss: 3.5703\n",
      "284/746, train_loss: 3.8820\n",
      "285/746, train_loss: 3.6989\n",
      "286/746, train_loss: 3.4905\n",
      "287/746, train_loss: 3.6291\n",
      "288/746, train_loss: 3.4858\n",
      "289/746, train_loss: 3.4020\n",
      "290/746, train_loss: 3.4884\n",
      "291/746, train_loss: 3.4477\n",
      "292/746, train_loss: 3.5562\n",
      "293/746, train_loss: 3.3706\n",
      "294/746, train_loss: 3.7782\n",
      "295/746, train_loss: 3.6602\n",
      "296/746, train_loss: 3.4905\n",
      "297/746, train_loss: 3.4895\n",
      "298/746, train_loss: 3.6160\n",
      "299/746, train_loss: 3.5782\n",
      "300/746, train_loss: 3.6165\n",
      "301/746, train_loss: 3.6634\n",
      "302/746, train_loss: 3.5205\n",
      "303/746, train_loss: 3.6487\n",
      "304/746, train_loss: 3.5637\n",
      "305/746, train_loss: 3.5390\n",
      "306/746, train_loss: 3.6162\n",
      "307/746, train_loss: 3.6823\n",
      "308/746, train_loss: 3.5886\n",
      "309/746, train_loss: 3.4995\n",
      "310/746, train_loss: 3.7544\n",
      "311/746, train_loss: 3.5543\n",
      "312/746, train_loss: 3.7888\n",
      "313/746, train_loss: 3.4299\n",
      "314/746, train_loss: 3.5597\n",
      "315/746, train_loss: 3.5059\n",
      "316/746, train_loss: 3.6192\n",
      "317/746, train_loss: 3.6743\n",
      "318/746, train_loss: 3.5728\n",
      "319/746, train_loss: 3.7746\n",
      "320/746, train_loss: 3.5277\n",
      "321/746, train_loss: 3.3513\n",
      "322/746, train_loss: 3.6716\n",
      "323/746, train_loss: 3.4092\n",
      "324/746, train_loss: 3.6555\n",
      "325/746, train_loss: 3.6082\n",
      "326/746, train_loss: 3.4963\n",
      "327/746, train_loss: 3.3397\n",
      "328/746, train_loss: 3.3504\n",
      "329/746, train_loss: 3.7169\n",
      "330/746, train_loss: 3.4594\n",
      "331/746, train_loss: 3.6316\n",
      "332/746, train_loss: 3.6986\n",
      "333/746, train_loss: 3.4690\n",
      "334/746, train_loss: 3.4623\n",
      "335/746, train_loss: 3.6341\n",
      "336/746, train_loss: 3.6035\n",
      "337/746, train_loss: 3.4872\n",
      "338/746, train_loss: 3.7307\n",
      "339/746, train_loss: 3.4722\n",
      "340/746, train_loss: 3.6434\n",
      "341/746, train_loss: 3.3113\n",
      "342/746, train_loss: 3.6390\n",
      "343/746, train_loss: 3.4870\n",
      "344/746, train_loss: 3.5597\n",
      "345/746, train_loss: 3.2843\n",
      "346/746, train_loss: 3.6832\n",
      "347/746, train_loss: 3.4934\n",
      "348/746, train_loss: 3.5596\n",
      "349/746, train_loss: 3.7052\n",
      "350/746, train_loss: 3.6772\n",
      "351/746, train_loss: 3.3186\n",
      "352/746, train_loss: 3.5803\n",
      "353/746, train_loss: 3.4670\n",
      "354/746, train_loss: 3.4694\n",
      "355/746, train_loss: 3.4264\n",
      "356/746, train_loss: 3.5850\n",
      "357/746, train_loss: 3.5986\n",
      "358/746, train_loss: 3.7264\n",
      "359/746, train_loss: 3.5733\n",
      "360/746, train_loss: 3.6711\n",
      "361/746, train_loss: 3.5925\n",
      "362/746, train_loss: 3.6215\n",
      "363/746, train_loss: 3.2714\n",
      "364/746, train_loss: 3.5751\n",
      "365/746, train_loss: 3.5513\n",
      "366/746, train_loss: 3.5342\n",
      "367/746, train_loss: 3.4538\n",
      "368/746, train_loss: 3.5304\n",
      "369/746, train_loss: 3.7137\n",
      "370/746, train_loss: 3.5819\n",
      "371/746, train_loss: 3.5159\n",
      "372/746, train_loss: 3.8247\n",
      "373/746, train_loss: 3.8742\n",
      "374/746, train_loss: 3.5273\n",
      "375/746, train_loss: 3.6994\n",
      "376/746, train_loss: 3.7380\n",
      "377/746, train_loss: 3.8040\n",
      "378/746, train_loss: 3.4656\n",
      "379/746, train_loss: 3.7503\n",
      "380/746, train_loss: 3.5610\n",
      "381/746, train_loss: 3.1891\n",
      "382/746, train_loss: 3.6247\n",
      "383/746, train_loss: 3.4396\n",
      "384/746, train_loss: 3.5710\n",
      "385/746, train_loss: 3.4708\n",
      "386/746, train_loss: 3.4853\n",
      "387/746, train_loss: 3.5816\n",
      "388/746, train_loss: 3.6095\n",
      "389/746, train_loss: 3.5712\n",
      "390/746, train_loss: 3.4758\n",
      "391/746, train_loss: 3.4395\n",
      "392/746, train_loss: 3.4882\n",
      "393/746, train_loss: 3.6892\n",
      "394/746, train_loss: 3.3745\n",
      "395/746, train_loss: 3.5395\n",
      "396/746, train_loss: 3.6840\n",
      "397/746, train_loss: 3.2746\n",
      "398/746, train_loss: 3.5240\n",
      "399/746, train_loss: 3.5347\n",
      "400/746, train_loss: 3.6043\n",
      "401/746, train_loss: 3.2429\n",
      "402/746, train_loss: 3.3745\n",
      "403/746, train_loss: 3.6708\n",
      "404/746, train_loss: 3.6572\n",
      "405/746, train_loss: 3.6041\n",
      "406/746, train_loss: 3.4439\n",
      "407/746, train_loss: 3.5473\n",
      "408/746, train_loss: 3.5472\n",
      "409/746, train_loss: 3.3915\n",
      "410/746, train_loss: 3.4097\n",
      "411/746, train_loss: 3.6993\n",
      "412/746, train_loss: 3.3133\n",
      "413/746, train_loss: 3.2659\n",
      "414/746, train_loss: 3.6024\n",
      "415/746, train_loss: 3.7123\n",
      "416/746, train_loss: 3.4934\n",
      "417/746, train_loss: 3.3445\n",
      "418/746, train_loss: 3.6533\n",
      "419/746, train_loss: 3.5675\n",
      "420/746, train_loss: 3.5542\n",
      "421/746, train_loss: 3.4495\n",
      "422/746, train_loss: 3.5178\n",
      "423/746, train_loss: 3.4872\n",
      "424/746, train_loss: 3.4152\n",
      "425/746, train_loss: 3.5289\n",
      "426/746, train_loss: 3.5691\n",
      "427/746, train_loss: 3.4863\n",
      "428/746, train_loss: 3.6031\n",
      "429/746, train_loss: 3.2771\n",
      "430/746, train_loss: 3.5344\n",
      "431/746, train_loss: 3.8056\n",
      "432/746, train_loss: 3.3696\n",
      "433/746, train_loss: 3.6818\n",
      "434/746, train_loss: 3.5702\n",
      "435/746, train_loss: 3.4482\n",
      "436/746, train_loss: 3.3931\n",
      "437/746, train_loss: 3.2003\n",
      "438/746, train_loss: 3.3402\n",
      "439/746, train_loss: 3.3995\n",
      "440/746, train_loss: 3.7208\n",
      "441/746, train_loss: 3.4993\n",
      "442/746, train_loss: 3.4523\n",
      "443/746, train_loss: 3.6328\n",
      "444/746, train_loss: 3.4134\n",
      "445/746, train_loss: 3.3837\n",
      "446/746, train_loss: 3.3689\n",
      "447/746, train_loss: 3.4825\n",
      "448/746, train_loss: 3.6340\n",
      "449/746, train_loss: 3.5514\n",
      "450/746, train_loss: 3.4036\n",
      "451/746, train_loss: 3.2933\n",
      "452/746, train_loss: 3.5060\n",
      "453/746, train_loss: 3.8160\n",
      "454/746, train_loss: 3.4013\n",
      "455/746, train_loss: 3.4873\n",
      "456/746, train_loss: 3.3733\n",
      "457/746, train_loss: 3.4086\n",
      "458/746, train_loss: 3.6173\n",
      "459/746, train_loss: 3.3523\n",
      "460/746, train_loss: 3.6400\n",
      "461/746, train_loss: 3.5728\n",
      "462/746, train_loss: 3.5015\n",
      "463/746, train_loss: 3.8095\n",
      "464/746, train_loss: 3.4722\n",
      "465/746, train_loss: 3.4742\n",
      "466/746, train_loss: 3.4709\n",
      "467/746, train_loss: 3.5945\n",
      "468/746, train_loss: 3.4539\n",
      "469/746, train_loss: 3.2536\n",
      "470/746, train_loss: 3.5112\n",
      "471/746, train_loss: 3.6895\n",
      "472/746, train_loss: 3.3135\n",
      "473/746, train_loss: 3.2550\n",
      "474/746, train_loss: 3.3617\n",
      "475/746, train_loss: 3.4984\n",
      "476/746, train_loss: 3.3397\n",
      "477/746, train_loss: 3.5562\n",
      "478/746, train_loss: 3.4572\n",
      "479/746, train_loss: 3.3719\n",
      "480/746, train_loss: 3.4838\n",
      "481/746, train_loss: 3.4884\n",
      "482/746, train_loss: 3.4793\n",
      "483/746, train_loss: 3.5974\n",
      "484/746, train_loss: 3.4154\n",
      "485/746, train_loss: 3.3898\n",
      "486/746, train_loss: 3.3598\n",
      "487/746, train_loss: 3.5833\n",
      "488/746, train_loss: 3.4991\n",
      "489/746, train_loss: 3.4674\n",
      "490/746, train_loss: 3.4140\n",
      "491/746, train_loss: 3.7652\n",
      "492/746, train_loss: 3.6545\n",
      "493/746, train_loss: 3.3866\n",
      "494/746, train_loss: 3.7143\n",
      "495/746, train_loss: 3.5427\n",
      "496/746, train_loss: 3.5230\n",
      "497/746, train_loss: 3.4748\n",
      "498/746, train_loss: 3.6620\n",
      "499/746, train_loss: 3.4970\n",
      "500/746, train_loss: 3.4197\n",
      "501/746, train_loss: 3.4688\n",
      "502/746, train_loss: 3.5357\n",
      "503/746, train_loss: 3.4971\n",
      "504/746, train_loss: 3.3915\n",
      "505/746, train_loss: 3.5507\n",
      "506/746, train_loss: 3.5710\n",
      "507/746, train_loss: 3.4053\n",
      "508/746, train_loss: 3.7618\n",
      "509/746, train_loss: 3.5391\n",
      "510/746, train_loss: 3.5559\n",
      "511/746, train_loss: 3.3833\n",
      "512/746, train_loss: 3.3116\n",
      "513/746, train_loss: 3.5746\n",
      "514/746, train_loss: 3.5721\n",
      "515/746, train_loss: 3.2528\n",
      "516/746, train_loss: 3.5108\n",
      "517/746, train_loss: 3.3051\n",
      "518/746, train_loss: 3.5293\n",
      "519/746, train_loss: 3.3099\n",
      "520/746, train_loss: 3.3509\n",
      "521/746, train_loss: 3.4281\n",
      "522/746, train_loss: 3.3857\n",
      "523/746, train_loss: 3.4487\n",
      "524/746, train_loss: 3.6104\n",
      "525/746, train_loss: 3.2856\n",
      "526/746, train_loss: 3.6056\n",
      "527/746, train_loss: 3.3685\n",
      "528/746, train_loss: 3.3658\n",
      "529/746, train_loss: 3.4369\n",
      "530/746, train_loss: 3.3367\n",
      "531/746, train_loss: 3.3583\n",
      "532/746, train_loss: 3.6125\n",
      "533/746, train_loss: 3.4464\n",
      "534/746, train_loss: 3.5411\n",
      "535/746, train_loss: 3.6049\n",
      "536/746, train_loss: 3.6587\n",
      "537/746, train_loss: 3.5470\n",
      "538/746, train_loss: 3.6595\n",
      "539/746, train_loss: 3.3181\n",
      "540/746, train_loss: 3.6332\n",
      "541/746, train_loss: 3.3335\n",
      "542/746, train_loss: 3.3599\n",
      "543/746, train_loss: 3.4573\n",
      "544/746, train_loss: 3.5543\n",
      "545/746, train_loss: 3.3809\n",
      "546/746, train_loss: 3.5826\n",
      "547/746, train_loss: 3.5733\n",
      "548/746, train_loss: 3.6923\n",
      "549/746, train_loss: 3.6000\n",
      "550/746, train_loss: 3.2850\n",
      "551/746, train_loss: 3.6192\n",
      "552/746, train_loss: 3.6143\n",
      "553/746, train_loss: 3.7811\n",
      "554/746, train_loss: 3.4861\n",
      "555/746, train_loss: 3.3669\n",
      "556/746, train_loss: 3.5575\n",
      "557/746, train_loss: 3.3497\n",
      "558/746, train_loss: 3.3674\n",
      "559/746, train_loss: 3.5906\n",
      "560/746, train_loss: 3.6900\n",
      "561/746, train_loss: 3.5696\n",
      "562/746, train_loss: 3.6430\n",
      "563/746, train_loss: 3.4278\n",
      "564/746, train_loss: 3.4838\n",
      "565/746, train_loss: 3.4204\n",
      "566/746, train_loss: 3.3936\n",
      "567/746, train_loss: 3.5739\n",
      "568/746, train_loss: 3.4937\n",
      "569/746, train_loss: 3.2939\n",
      "570/746, train_loss: 3.4574\n",
      "571/746, train_loss: 3.6292\n",
      "572/746, train_loss: 3.3057\n",
      "573/746, train_loss: 3.2635\n",
      "574/746, train_loss: 3.3731\n",
      "575/746, train_loss: 3.2931\n",
      "576/746, train_loss: 3.4752\n",
      "577/746, train_loss: 3.5802\n",
      "578/746, train_loss: 3.3829\n",
      "579/746, train_loss: 3.6062\n",
      "580/746, train_loss: 3.3930\n",
      "581/746, train_loss: 3.6092\n",
      "582/746, train_loss: 3.4254\n",
      "583/746, train_loss: 3.6212\n",
      "584/746, train_loss: 3.3083\n",
      "585/746, train_loss: 3.3263\n",
      "586/746, train_loss: 3.5221\n",
      "587/746, train_loss: 3.3219\n",
      "588/746, train_loss: 3.3928\n",
      "589/746, train_loss: 3.3568\n",
      "590/746, train_loss: 3.5598\n",
      "591/746, train_loss: 3.4207\n",
      "592/746, train_loss: 3.5872\n",
      "593/746, train_loss: 3.7188\n",
      "594/746, train_loss: 3.6809\n",
      "595/746, train_loss: 3.5349\n",
      "596/746, train_loss: 3.4579\n",
      "597/746, train_loss: 3.3744\n",
      "598/746, train_loss: 3.5702\n",
      "599/746, train_loss: 3.5274\n",
      "600/746, train_loss: 3.5076\n",
      "601/746, train_loss: 3.5149\n",
      "602/746, train_loss: 3.5449\n",
      "603/746, train_loss: 3.4110\n",
      "604/746, train_loss: 3.5083\n",
      "605/746, train_loss: 3.4426\n",
      "606/746, train_loss: 3.4914\n",
      "607/746, train_loss: 3.4315\n",
      "608/746, train_loss: 3.3355\n",
      "609/746, train_loss: 3.4668\n",
      "610/746, train_loss: 3.4596\n",
      "611/746, train_loss: 3.5088\n",
      "612/746, train_loss: 3.6489\n",
      "613/746, train_loss: 3.3764\n",
      "614/746, train_loss: 3.2498\n",
      "615/746, train_loss: 3.1690\n",
      "616/746, train_loss: 3.4149\n",
      "617/746, train_loss: 3.6577\n",
      "618/746, train_loss: 3.4597\n",
      "619/746, train_loss: 3.4400\n",
      "620/746, train_loss: 3.4724\n",
      "621/746, train_loss: 3.4903\n",
      "622/746, train_loss: 3.5995\n",
      "623/746, train_loss: 3.3580\n",
      "624/746, train_loss: 3.3268\n",
      "625/746, train_loss: 3.4997\n",
      "626/746, train_loss: 3.4602\n",
      "627/746, train_loss: 3.5458\n",
      "628/746, train_loss: 3.2302\n",
      "629/746, train_loss: 3.3309\n",
      "630/746, train_loss: 3.4600\n",
      "631/746, train_loss: 3.6055\n",
      "632/746, train_loss: 3.5484\n",
      "633/746, train_loss: 3.4086\n",
      "634/746, train_loss: 3.4372\n",
      "635/746, train_loss: 3.5408\n",
      "636/746, train_loss: 3.3065\n",
      "637/746, train_loss: 3.5147\n",
      "638/746, train_loss: 3.5181\n",
      "639/746, train_loss: 3.4782\n",
      "640/746, train_loss: 3.5336\n",
      "641/746, train_loss: 3.5464\n",
      "642/746, train_loss: 3.4419\n",
      "643/746, train_loss: 3.2855\n",
      "644/746, train_loss: 3.4829\n",
      "645/746, train_loss: 3.5641\n",
      "646/746, train_loss: 3.2907\n",
      "647/746, train_loss: 3.3237\n",
      "648/746, train_loss: 3.5551\n",
      "649/746, train_loss: 3.5303\n",
      "650/746, train_loss: 3.5411\n",
      "651/746, train_loss: 3.8184\n",
      "652/746, train_loss: 3.1988\n",
      "653/746, train_loss: 3.4178\n",
      "654/746, train_loss: 3.3735\n",
      "655/746, train_loss: 3.5360\n",
      "656/746, train_loss: 3.4951\n",
      "657/746, train_loss: 3.5681\n",
      "658/746, train_loss: 3.4634\n",
      "659/746, train_loss: 3.4750\n",
      "660/746, train_loss: 3.5268\n",
      "661/746, train_loss: 3.9092\n",
      "662/746, train_loss: 3.1588\n",
      "663/746, train_loss: 3.2778\n",
      "664/746, train_loss: 3.6408\n",
      "665/746, train_loss: 3.6311\n",
      "666/746, train_loss: 3.3561\n",
      "667/746, train_loss: 3.5390\n",
      "668/746, train_loss: 3.5805\n",
      "669/746, train_loss: 3.5624\n",
      "670/746, train_loss: 3.4397\n",
      "671/746, train_loss: 3.5143\n",
      "672/746, train_loss: 3.3551\n",
      "673/746, train_loss: 3.4753\n",
      "674/746, train_loss: 3.4882\n",
      "675/746, train_loss: 3.4528\n",
      "676/746, train_loss: 3.6086\n",
      "677/746, train_loss: 3.4427\n",
      "678/746, train_loss: 3.6108\n",
      "679/746, train_loss: 3.5228\n",
      "680/746, train_loss: 3.5142\n",
      "681/746, train_loss: 3.5854\n",
      "682/746, train_loss: 3.4627\n",
      "683/746, train_loss: 3.4218\n",
      "684/746, train_loss: 3.3595\n",
      "685/746, train_loss: 3.3814\n",
      "686/746, train_loss: 3.3947\n",
      "687/746, train_loss: 3.3804\n",
      "688/746, train_loss: 3.5367\n",
      "689/746, train_loss: 3.1141\n",
      "690/746, train_loss: 3.1733\n",
      "691/746, train_loss: 3.2957\n",
      "692/746, train_loss: 3.6230\n",
      "693/746, train_loss: 3.3636\n",
      "694/746, train_loss: 3.3885\n",
      "695/746, train_loss: 3.3046\n",
      "696/746, train_loss: 3.3650\n",
      "697/746, train_loss: 3.5026\n",
      "698/746, train_loss: 3.4975\n",
      "699/746, train_loss: 3.3450\n",
      "700/746, train_loss: 3.3946\n",
      "701/746, train_loss: 3.4262\n",
      "702/746, train_loss: 3.1946\n",
      "703/746, train_loss: 3.5413\n",
      "704/746, train_loss: 3.3392\n",
      "705/746, train_loss: 3.4769\n",
      "706/746, train_loss: 3.6495\n",
      "707/746, train_loss: 3.6241\n",
      "708/746, train_loss: 3.1670\n",
      "709/746, train_loss: 3.2912\n",
      "710/746, train_loss: 3.2772\n",
      "711/746, train_loss: 3.5125\n",
      "712/746, train_loss: 3.4474\n",
      "713/746, train_loss: 3.1408\n",
      "714/746, train_loss: 3.2900\n",
      "715/746, train_loss: 3.3016\n",
      "716/746, train_loss: 3.3357\n",
      "717/746, train_loss: 3.3186\n",
      "718/746, train_loss: 3.5984\n",
      "719/746, train_loss: 3.2584\n",
      "720/746, train_loss: 3.2561\n",
      "721/746, train_loss: 3.3486\n",
      "722/746, train_loss: 3.0593\n",
      "723/746, train_loss: 3.3468\n",
      "724/746, train_loss: 3.2995\n",
      "725/746, train_loss: 3.3021\n",
      "726/746, train_loss: 3.4518\n",
      "727/746, train_loss: 3.4976\n",
      "728/746, train_loss: 3.6022\n",
      "729/746, train_loss: 3.3906\n",
      "730/746, train_loss: 3.2659\n",
      "731/746, train_loss: 3.5767\n",
      "732/746, train_loss: 3.6083\n",
      "733/746, train_loss: 3.1898\n",
      "734/746, train_loss: 3.3933\n",
      "735/746, train_loss: 3.3151\n",
      "736/746, train_loss: 3.3613\n",
      "737/746, train_loss: 3.5804\n",
      "738/746, train_loss: 3.2639\n",
      "739/746, train_loss: 3.3764\n",
      "740/746, train_loss: 3.4125\n",
      "741/746, train_loss: 3.4872\n",
      "742/746, train_loss: 3.2264\n",
      "743/746, train_loss: 3.3561\n",
      "744/746, train_loss: 3.4927\n",
      "745/746, train_loss: 3.5762\n",
      "746/746, train_loss: 3.2038\n",
      "747/746, train_loss: 3.5046\n",
      "epoch 9 average loss: 3.5387\n",
      "saved new best metric model\n",
      "current epoch: 9 current AUC: 0.9715 current accuracy: 0.4347 best AUC: 0.4347 at epoch: 9\n",
      "----------\n",
      "epoch 10/10\n",
      "1/746, train_loss: 3.2123\n",
      "2/746, train_loss: 3.5364\n",
      "3/746, train_loss: 3.2240\n",
      "4/746, train_loss: 3.5313\n",
      "5/746, train_loss: 3.2895\n",
      "6/746, train_loss: 3.4104\n",
      "7/746, train_loss: 3.4456\n",
      "8/746, train_loss: 3.3736\n",
      "9/746, train_loss: 3.2936\n",
      "10/746, train_loss: 3.1106\n",
      "11/746, train_loss: 3.3935\n",
      "12/746, train_loss: 3.3930\n",
      "13/746, train_loss: 3.2221\n",
      "14/746, train_loss: 3.3965\n",
      "15/746, train_loss: 3.3187\n",
      "16/746, train_loss: 3.2180\n",
      "17/746, train_loss: 3.5469\n",
      "18/746, train_loss: 3.3382\n",
      "19/746, train_loss: 3.4542\n",
      "20/746, train_loss: 3.3226\n",
      "21/746, train_loss: 3.3182\n",
      "22/746, train_loss: 3.3410\n",
      "23/746, train_loss: 3.4203\n",
      "24/746, train_loss: 3.3486\n",
      "25/746, train_loss: 3.2909\n",
      "26/746, train_loss: 3.5680\n",
      "27/746, train_loss: 3.5029\n",
      "28/746, train_loss: 3.1743\n",
      "29/746, train_loss: 3.2217\n",
      "30/746, train_loss: 3.3576\n",
      "31/746, train_loss: 3.1913\n",
      "32/746, train_loss: 3.1230\n",
      "33/746, train_loss: 3.2421\n",
      "34/746, train_loss: 3.3516\n",
      "35/746, train_loss: 3.4387\n",
      "36/746, train_loss: 3.2937\n",
      "37/746, train_loss: 3.8031\n",
      "38/746, train_loss: 3.3579\n",
      "39/746, train_loss: 3.2483\n",
      "40/746, train_loss: 3.4426\n",
      "41/746, train_loss: 3.4068\n",
      "42/746, train_loss: 3.4650\n",
      "43/746, train_loss: 3.5432\n",
      "44/746, train_loss: 3.2750\n",
      "45/746, train_loss: 3.3443\n",
      "46/746, train_loss: 3.5492\n",
      "47/746, train_loss: 3.3244\n",
      "48/746, train_loss: 3.4881\n",
      "49/746, train_loss: 3.0338\n",
      "50/746, train_loss: 3.4396\n",
      "51/746, train_loss: 3.4056\n",
      "52/746, train_loss: 3.4360\n",
      "53/746, train_loss: 3.2702\n",
      "54/746, train_loss: 3.3890\n",
      "55/746, train_loss: 3.3448\n",
      "56/746, train_loss: 3.2305\n",
      "57/746, train_loss: 3.2600\n",
      "58/746, train_loss: 3.5331\n",
      "59/746, train_loss: 3.2492\n",
      "60/746, train_loss: 3.4296\n",
      "61/746, train_loss: 3.3093\n",
      "62/746, train_loss: 3.3494\n",
      "63/746, train_loss: 3.2457\n",
      "64/746, train_loss: 3.2984\n",
      "65/746, train_loss: 3.3850\n",
      "66/746, train_loss: 3.2823\n",
      "67/746, train_loss: 3.2137\n",
      "68/746, train_loss: 3.2993\n",
      "69/746, train_loss: 3.3782\n",
      "70/746, train_loss: 3.2731\n",
      "71/746, train_loss: 3.4703\n",
      "72/746, train_loss: 3.1829\n",
      "73/746, train_loss: 3.1628\n",
      "74/746, train_loss: 3.3514\n",
      "75/746, train_loss: 3.3631\n",
      "76/746, train_loss: 3.4855\n",
      "77/746, train_loss: 3.3896\n",
      "78/746, train_loss: 3.4559\n",
      "79/746, train_loss: 3.4216\n",
      "80/746, train_loss: 3.2836\n",
      "81/746, train_loss: 3.2335\n",
      "82/746, train_loss: 3.5143\n",
      "83/746, train_loss: 3.2951\n",
      "84/746, train_loss: 3.2669\n",
      "85/746, train_loss: 3.2664\n",
      "86/746, train_loss: 3.1129\n",
      "87/746, train_loss: 3.2947\n",
      "88/746, train_loss: 3.3475\n",
      "89/746, train_loss: 3.2551\n",
      "90/746, train_loss: 3.2043\n",
      "91/746, train_loss: 3.5167\n",
      "92/746, train_loss: 3.2567\n",
      "93/746, train_loss: 3.1684\n",
      "94/746, train_loss: 3.1187\n",
      "95/746, train_loss: 3.2404\n",
      "96/746, train_loss: 3.4291\n",
      "97/746, train_loss: 3.4585\n",
      "98/746, train_loss: 3.3676\n",
      "99/746, train_loss: 3.4148\n",
      "100/746, train_loss: 3.1988\n",
      "101/746, train_loss: 3.3642\n",
      "102/746, train_loss: 3.4577\n",
      "103/746, train_loss: 3.3928\n",
      "104/746, train_loss: 3.0027\n",
      "105/746, train_loss: 3.2655\n",
      "106/746, train_loss: 3.0815\n",
      "107/746, train_loss: 3.4388\n",
      "108/746, train_loss: 3.5204\n",
      "109/746, train_loss: 3.3260\n",
      "110/746, train_loss: 3.2661\n",
      "111/746, train_loss: 3.3658\n",
      "112/746, train_loss: 3.1498\n",
      "113/746, train_loss: 3.5089\n",
      "114/746, train_loss: 3.1927\n",
      "115/746, train_loss: 3.3727\n",
      "116/746, train_loss: 3.2630\n",
      "117/746, train_loss: 3.2601\n",
      "118/746, train_loss: 3.3238\n",
      "119/746, train_loss: 3.2021\n",
      "120/746, train_loss: 3.3080\n",
      "121/746, train_loss: 3.3292\n",
      "122/746, train_loss: 3.2409\n",
      "123/746, train_loss: 3.3095\n",
      "124/746, train_loss: 3.2180\n",
      "125/746, train_loss: 3.3209\n",
      "126/746, train_loss: 3.3916\n",
      "127/746, train_loss: 3.4381\n",
      "128/746, train_loss: 3.2798\n",
      "129/746, train_loss: 3.3375\n",
      "130/746, train_loss: 3.4243\n",
      "131/746, train_loss: 3.3175\n",
      "132/746, train_loss: 3.3776\n",
      "133/746, train_loss: 3.2381\n",
      "134/746, train_loss: 3.1903\n",
      "135/746, train_loss: 3.2428\n",
      "136/746, train_loss: 3.0045\n",
      "137/746, train_loss: 3.5938\n",
      "138/746, train_loss: 3.1711\n",
      "139/746, train_loss: 3.4894\n",
      "140/746, train_loss: 3.2770\n",
      "141/746, train_loss: 3.4710\n",
      "142/746, train_loss: 3.0123\n",
      "143/746, train_loss: 3.2804\n",
      "144/746, train_loss: 3.4155\n",
      "145/746, train_loss: 2.8899\n",
      "146/746, train_loss: 3.5259\n",
      "147/746, train_loss: 3.2933\n",
      "148/746, train_loss: 3.4922\n",
      "149/746, train_loss: 3.3414\n",
      "150/746, train_loss: 3.4478\n",
      "151/746, train_loss: 3.2482\n",
      "152/746, train_loss: 3.2605\n",
      "153/746, train_loss: 3.1916\n",
      "154/746, train_loss: 3.0970\n",
      "155/746, train_loss: 3.0224\n",
      "156/746, train_loss: 3.2205\n",
      "157/746, train_loss: 3.3577\n",
      "158/746, train_loss: 3.2921\n",
      "159/746, train_loss: 3.2743\n",
      "160/746, train_loss: 2.9976\n",
      "161/746, train_loss: 3.2793\n",
      "162/746, train_loss: 3.2396\n",
      "163/746, train_loss: 3.3344\n",
      "164/746, train_loss: 3.1753\n",
      "165/746, train_loss: 3.3069\n",
      "166/746, train_loss: 3.2598\n",
      "167/746, train_loss: 3.1468\n",
      "168/746, train_loss: 3.2621\n",
      "169/746, train_loss: 3.2425\n",
      "170/746, train_loss: 3.0403\n",
      "171/746, train_loss: 3.1030\n",
      "172/746, train_loss: 3.2414\n",
      "173/746, train_loss: 3.2708\n",
      "174/746, train_loss: 3.1073\n",
      "175/746, train_loss: 3.4578\n",
      "176/746, train_loss: 3.4819\n",
      "177/746, train_loss: 3.3826\n",
      "178/746, train_loss: 3.4526\n",
      "179/746, train_loss: 3.3120\n",
      "180/746, train_loss: 3.6031\n",
      "181/746, train_loss: 3.4065\n",
      "182/746, train_loss: 3.4591\n",
      "183/746, train_loss: 3.1611\n",
      "184/746, train_loss: 3.4512\n",
      "185/746, train_loss: 3.4084\n",
      "186/746, train_loss: 3.1217\n",
      "187/746, train_loss: 3.3459\n",
      "188/746, train_loss: 3.1093\n",
      "189/746, train_loss: 3.3038\n",
      "190/746, train_loss: 3.3461\n",
      "191/746, train_loss: 3.3560\n",
      "192/746, train_loss: 3.2543\n",
      "193/746, train_loss: 3.4566\n",
      "194/746, train_loss: 3.4757\n",
      "195/746, train_loss: 3.3246\n",
      "196/746, train_loss: 3.3726\n",
      "197/746, train_loss: 3.2449\n",
      "198/746, train_loss: 3.2494\n",
      "199/746, train_loss: 3.3122\n",
      "200/746, train_loss: 3.2790\n",
      "201/746, train_loss: 3.2093\n",
      "202/746, train_loss: 3.1279\n",
      "203/746, train_loss: 3.3741\n",
      "204/746, train_loss: 3.3908\n",
      "205/746, train_loss: 3.2496\n",
      "206/746, train_loss: 3.5972\n",
      "207/746, train_loss: 3.1182\n",
      "208/746, train_loss: 3.1801\n",
      "209/746, train_loss: 3.1970\n",
      "210/746, train_loss: 3.1764\n",
      "211/746, train_loss: 3.4038\n",
      "212/746, train_loss: 3.1483\n",
      "213/746, train_loss: 3.1683\n",
      "214/746, train_loss: 3.4570\n",
      "215/746, train_loss: 3.3924\n",
      "216/746, train_loss: 3.4169\n",
      "217/746, train_loss: 3.4153\n",
      "218/746, train_loss: 3.3547\n",
      "219/746, train_loss: 3.4959\n",
      "220/746, train_loss: 3.1710\n",
      "221/746, train_loss: 3.1485\n",
      "222/746, train_loss: 3.3471\n",
      "223/746, train_loss: 3.0684\n",
      "224/746, train_loss: 3.3642\n",
      "225/746, train_loss: 3.4937\n",
      "226/746, train_loss: 3.1958\n",
      "227/746, train_loss: 3.3881\n",
      "228/746, train_loss: 3.0304\n",
      "229/746, train_loss: 3.0779\n",
      "230/746, train_loss: 3.1357\n",
      "231/746, train_loss: 3.2935\n",
      "232/746, train_loss: 3.1354\n",
      "233/746, train_loss: 3.3075\n",
      "234/746, train_loss: 3.4470\n",
      "235/746, train_loss: 3.4001\n",
      "236/746, train_loss: 3.6206\n",
      "237/746, train_loss: 3.4356\n",
      "238/746, train_loss: 3.4571\n",
      "239/746, train_loss: 3.2824\n",
      "240/746, train_loss: 3.2912\n",
      "241/746, train_loss: 3.3627\n",
      "242/746, train_loss: 3.4606\n",
      "243/746, train_loss: 3.2997\n",
      "244/746, train_loss: 3.1470\n",
      "245/746, train_loss: 3.2864\n",
      "246/746, train_loss: 3.4873\n",
      "247/746, train_loss: 3.2704\n",
      "248/746, train_loss: 3.2630\n",
      "249/746, train_loss: 3.4793\n",
      "250/746, train_loss: 3.1967\n",
      "251/746, train_loss: 3.2384\n",
      "252/746, train_loss: 3.1972\n",
      "253/746, train_loss: 3.3386\n",
      "254/746, train_loss: 3.1839\n",
      "255/746, train_loss: 3.4034\n",
      "256/746, train_loss: 3.0271\n",
      "257/746, train_loss: 3.3436\n",
      "258/746, train_loss: 3.3415\n",
      "259/746, train_loss: 3.1199\n",
      "260/746, train_loss: 3.4717\n",
      "261/746, train_loss: 3.1670\n",
      "262/746, train_loss: 3.4281\n",
      "263/746, train_loss: 3.1664\n",
      "264/746, train_loss: 3.2210\n",
      "265/746, train_loss: 3.4663\n",
      "266/746, train_loss: 3.4158\n",
      "267/746, train_loss: 3.1866\n",
      "268/746, train_loss: 3.0598\n",
      "269/746, train_loss: 3.2666\n",
      "270/746, train_loss: 3.1889\n",
      "271/746, train_loss: 3.2322\n",
      "272/746, train_loss: 3.2210\n",
      "273/746, train_loss: 3.2439\n",
      "274/746, train_loss: 3.2165\n",
      "275/746, train_loss: 3.0813\n",
      "276/746, train_loss: 3.5741\n",
      "277/746, train_loss: 3.1558\n",
      "278/746, train_loss: 3.1468\n",
      "279/746, train_loss: 3.2390\n",
      "280/746, train_loss: 3.1688\n",
      "281/746, train_loss: 3.3345\n",
      "282/746, train_loss: 3.1689\n",
      "283/746, train_loss: 3.3949\n",
      "284/746, train_loss: 3.2646\n",
      "285/746, train_loss: 2.8509\n",
      "286/746, train_loss: 3.2390\n",
      "287/746, train_loss: 3.1273\n",
      "288/746, train_loss: 3.2082\n",
      "289/746, train_loss: 3.3761\n",
      "290/746, train_loss: 3.4027\n",
      "291/746, train_loss: 3.1484\n",
      "292/746, train_loss: 3.3927\n",
      "293/746, train_loss: 3.3694\n",
      "294/746, train_loss: 3.4300\n",
      "295/746, train_loss: 2.9493\n",
      "296/746, train_loss: 3.2502\n",
      "297/746, train_loss: 3.1287\n",
      "298/746, train_loss: 3.2704\n",
      "299/746, train_loss: 3.6383\n",
      "300/746, train_loss: 3.3603\n",
      "301/746, train_loss: 3.2818\n",
      "302/746, train_loss: 3.2052\n",
      "303/746, train_loss: 3.1060\n",
      "304/746, train_loss: 3.1615\n",
      "305/746, train_loss: 3.2158\n",
      "306/746, train_loss: 3.1003\n",
      "307/746, train_loss: 3.2453\n",
      "308/746, train_loss: 3.3751\n",
      "309/746, train_loss: 3.1927\n",
      "310/746, train_loss: 3.2511\n",
      "311/746, train_loss: 3.1048\n",
      "312/746, train_loss: 3.2279\n",
      "313/746, train_loss: 3.4032\n",
      "314/746, train_loss: 3.1857\n",
      "315/746, train_loss: 3.3668\n",
      "316/746, train_loss: 3.3293\n",
      "317/746, train_loss: 3.0724\n",
      "318/746, train_loss: 3.3653\n",
      "319/746, train_loss: 3.3677\n",
      "320/746, train_loss: 3.5839\n",
      "321/746, train_loss: 3.3064\n",
      "322/746, train_loss: 3.1039\n",
      "323/746, train_loss: 2.8267\n",
      "324/746, train_loss: 3.1274\n",
      "325/746, train_loss: 3.3748\n",
      "326/746, train_loss: 3.3236\n",
      "327/746, train_loss: 3.1333\n",
      "328/746, train_loss: 3.2576\n",
      "329/746, train_loss: 3.3823\n",
      "330/746, train_loss: 3.3506\n",
      "331/746, train_loss: 3.4132\n",
      "332/746, train_loss: 3.3128\n",
      "333/746, train_loss: 3.4556\n",
      "334/746, train_loss: 3.3952\n",
      "335/746, train_loss: 3.2673\n",
      "336/746, train_loss: 3.2479\n",
      "337/746, train_loss: 3.1475\n",
      "338/746, train_loss: 3.4537\n",
      "339/746, train_loss: 3.2976\n",
      "340/746, train_loss: 3.4166\n",
      "341/746, train_loss: 3.4317\n",
      "342/746, train_loss: 3.4033\n",
      "343/746, train_loss: 3.2795\n",
      "344/746, train_loss: 3.2167\n",
      "345/746, train_loss: 3.2931\n",
      "346/746, train_loss: 3.2182\n",
      "347/746, train_loss: 3.4217\n",
      "348/746, train_loss: 3.2881\n",
      "349/746, train_loss: 3.1108\n",
      "350/746, train_loss: 3.1546\n",
      "351/746, train_loss: 3.1866\n",
      "352/746, train_loss: 3.1869\n",
      "353/746, train_loss: 3.2415\n",
      "354/746, train_loss: 3.2669\n",
      "355/746, train_loss: 3.1068\n",
      "356/746, train_loss: 3.1592\n",
      "357/746, train_loss: 3.0875\n",
      "358/746, train_loss: 3.4529\n",
      "359/746, train_loss: 3.3104\n",
      "360/746, train_loss: 3.4213\n",
      "361/746, train_loss: 3.2755\n",
      "362/746, train_loss: 3.4475\n",
      "363/746, train_loss: 3.1564\n",
      "364/746, train_loss: 3.3555\n",
      "365/746, train_loss: 3.4258\n",
      "366/746, train_loss: 2.9341\n",
      "367/746, train_loss: 3.0520\n",
      "368/746, train_loss: 3.2526\n",
      "369/746, train_loss: 3.2627\n",
      "370/746, train_loss: 3.1734\n",
      "371/746, train_loss: 3.0047\n",
      "372/746, train_loss: 3.2856\n",
      "373/746, train_loss: 3.2776\n",
      "374/746, train_loss: 3.3322\n",
      "375/746, train_loss: 3.3522\n",
      "376/746, train_loss: 3.2841\n",
      "377/746, train_loss: 3.2790\n",
      "378/746, train_loss: 3.1687\n",
      "379/746, train_loss: 3.0559\n",
      "380/746, train_loss: 3.2682\n",
      "381/746, train_loss: 3.2019\n",
      "382/746, train_loss: 3.2648\n",
      "383/746, train_loss: 3.2601\n",
      "384/746, train_loss: 3.3546\n",
      "385/746, train_loss: 3.2742\n",
      "386/746, train_loss: 3.2667\n",
      "387/746, train_loss: 3.2863\n",
      "388/746, train_loss: 3.2041\n",
      "389/746, train_loss: 3.3141\n",
      "390/746, train_loss: 3.0247\n",
      "391/746, train_loss: 3.3199\n",
      "392/746, train_loss: 3.3273\n",
      "393/746, train_loss: 3.3871\n",
      "394/746, train_loss: 3.0770\n",
      "395/746, train_loss: 3.0621\n",
      "396/746, train_loss: 3.2343\n",
      "397/746, train_loss: 3.3628\n",
      "398/746, train_loss: 2.9931\n",
      "399/746, train_loss: 3.1641\n",
      "400/746, train_loss: 3.0040\n",
      "401/746, train_loss: 3.1674\n",
      "402/746, train_loss: 3.3728\n",
      "403/746, train_loss: 3.2144\n",
      "404/746, train_loss: 3.2176\n",
      "405/746, train_loss: 2.9476\n",
      "406/746, train_loss: 3.2360\n",
      "407/746, train_loss: 3.0260\n",
      "408/746, train_loss: 3.0510\n",
      "409/746, train_loss: 2.9938\n",
      "410/746, train_loss: 3.3185\n",
      "411/746, train_loss: 3.2018\n",
      "412/746, train_loss: 3.4292\n",
      "413/746, train_loss: 3.1038\n",
      "414/746, train_loss: 3.5571\n",
      "415/746, train_loss: 3.1984\n",
      "416/746, train_loss: 3.4033\n",
      "417/746, train_loss: 3.4406\n",
      "418/746, train_loss: 3.0604\n",
      "419/746, train_loss: 3.3196\n",
      "420/746, train_loss: 3.3207\n",
      "421/746, train_loss: 3.1417\n",
      "422/746, train_loss: 3.3990\n",
      "423/746, train_loss: 3.2338\n",
      "424/746, train_loss: 3.1070\n",
      "425/746, train_loss: 3.1786\n",
      "426/746, train_loss: 3.4690\n",
      "427/746, train_loss: 3.0769\n",
      "428/746, train_loss: 3.3824\n",
      "429/746, train_loss: 3.0487\n",
      "430/746, train_loss: 3.3566\n",
      "431/746, train_loss: 3.0370\n",
      "432/746, train_loss: 3.1183\n",
      "433/746, train_loss: 3.1793\n",
      "434/746, train_loss: 3.1878\n",
      "435/746, train_loss: 2.9045\n",
      "436/746, train_loss: 3.1153\n",
      "437/746, train_loss: 3.1388\n",
      "438/746, train_loss: 3.2761\n",
      "439/746, train_loss: 3.0061\n",
      "440/746, train_loss: 3.3029\n",
      "441/746, train_loss: 2.8657\n",
      "442/746, train_loss: 3.2607\n",
      "443/746, train_loss: 3.1850\n",
      "444/746, train_loss: 3.2558\n",
      "445/746, train_loss: 3.1978\n",
      "446/746, train_loss: 3.3721\n",
      "447/746, train_loss: 3.2575\n",
      "448/746, train_loss: 3.2350\n",
      "449/746, train_loss: 3.1520\n",
      "450/746, train_loss: 3.1808\n",
      "451/746, train_loss: 3.2406\n",
      "452/746, train_loss: 3.3879\n",
      "453/746, train_loss: 3.0187\n",
      "454/746, train_loss: 3.2784\n",
      "455/746, train_loss: 2.8679\n",
      "456/746, train_loss: 3.0827\n",
      "457/746, train_loss: 3.1162\n",
      "458/746, train_loss: 2.9983\n",
      "459/746, train_loss: 3.4178\n",
      "460/746, train_loss: 3.2907\n",
      "461/746, train_loss: 3.1368\n",
      "462/746, train_loss: 2.9930\n",
      "463/746, train_loss: 3.0402\n",
      "464/746, train_loss: 3.1484\n",
      "465/746, train_loss: 3.2016\n",
      "466/746, train_loss: 3.1228\n",
      "467/746, train_loss: 2.9260\n",
      "468/746, train_loss: 3.2176\n",
      "469/746, train_loss: 3.2059\n",
      "470/746, train_loss: 3.2543\n",
      "471/746, train_loss: 3.2158\n",
      "472/746, train_loss: 3.2182\n",
      "473/746, train_loss: 3.3253\n",
      "474/746, train_loss: 3.3781\n",
      "475/746, train_loss: 2.9962\n",
      "476/746, train_loss: 3.2427\n",
      "477/746, train_loss: 3.1222\n",
      "478/746, train_loss: 3.3246\n",
      "479/746, train_loss: 3.2338\n",
      "480/746, train_loss: 3.2782\n",
      "481/746, train_loss: 3.2376\n",
      "482/746, train_loss: 3.2230\n",
      "483/746, train_loss: 3.1956\n",
      "484/746, train_loss: 3.1772\n",
      "485/746, train_loss: 3.0496\n",
      "486/746, train_loss: 3.2529\n",
      "487/746, train_loss: 3.3251\n",
      "488/746, train_loss: 3.5779\n",
      "489/746, train_loss: 3.2180\n",
      "490/746, train_loss: 3.2692\n",
      "491/746, train_loss: 3.1730\n",
      "492/746, train_loss: 3.4259\n",
      "493/746, train_loss: 3.2616\n",
      "494/746, train_loss: 2.9426\n",
      "495/746, train_loss: 3.2000\n",
      "496/746, train_loss: 3.3102\n",
      "497/746, train_loss: 3.1648\n",
      "498/746, train_loss: 3.0668\n",
      "499/746, train_loss: 3.0551\n",
      "500/746, train_loss: 3.5013\n",
      "501/746, train_loss: 3.2289\n",
      "502/746, train_loss: 3.2886\n",
      "503/746, train_loss: 3.0993\n",
      "504/746, train_loss: 3.1785\n",
      "505/746, train_loss: 3.3720\n",
      "506/746, train_loss: 2.9018\n",
      "507/746, train_loss: 3.5730\n",
      "508/746, train_loss: 3.2518\n",
      "509/746, train_loss: 3.1301\n",
      "510/746, train_loss: 3.2883\n",
      "511/746, train_loss: 3.1067\n",
      "512/746, train_loss: 3.2283\n",
      "513/746, train_loss: 3.0943\n",
      "514/746, train_loss: 3.2382\n",
      "515/746, train_loss: 3.2263\n",
      "516/746, train_loss: 3.3489\n",
      "517/746, train_loss: 3.0477\n",
      "518/746, train_loss: 3.2704\n",
      "519/746, train_loss: 3.0991\n",
      "520/746, train_loss: 3.2656\n",
      "521/746, train_loss: 3.3472\n",
      "522/746, train_loss: 3.2186\n",
      "523/746, train_loss: 3.0878\n",
      "524/746, train_loss: 3.1326\n",
      "525/746, train_loss: 2.9735\n",
      "526/746, train_loss: 3.2713\n",
      "527/746, train_loss: 3.3942\n",
      "528/746, train_loss: 3.0341\n",
      "529/746, train_loss: 3.5422\n",
      "530/746, train_loss: 3.1334\n",
      "531/746, train_loss: 3.0871\n",
      "532/746, train_loss: 3.2335\n",
      "533/746, train_loss: 3.0226\n",
      "534/746, train_loss: 3.1409\n",
      "535/746, train_loss: 3.0188\n",
      "536/746, train_loss: 3.1068\n",
      "537/746, train_loss: 3.0180\n",
      "538/746, train_loss: 3.0063\n",
      "539/746, train_loss: 2.9857\n",
      "540/746, train_loss: 2.9954\n",
      "541/746, train_loss: 3.1350\n",
      "542/746, train_loss: 3.2074\n",
      "543/746, train_loss: 3.1089\n",
      "544/746, train_loss: 3.1162\n",
      "545/746, train_loss: 3.5006\n",
      "546/746, train_loss: 3.2459\n",
      "547/746, train_loss: 3.0387\n",
      "548/746, train_loss: 3.1025\n",
      "549/746, train_loss: 3.1503\n",
      "550/746, train_loss: 3.0320\n",
      "551/746, train_loss: 3.3589\n",
      "552/746, train_loss: 3.0942\n",
      "553/746, train_loss: 3.2204\n",
      "554/746, train_loss: 3.1413\n",
      "555/746, train_loss: 3.4040\n",
      "556/746, train_loss: 3.2221\n",
      "557/746, train_loss: 3.0088\n",
      "558/746, train_loss: 3.0760\n",
      "559/746, train_loss: 3.1634\n",
      "560/746, train_loss: 3.2524\n",
      "561/746, train_loss: 3.2054\n",
      "562/746, train_loss: 3.2463\n",
      "563/746, train_loss: 3.0647\n",
      "564/746, train_loss: 3.0062\n",
      "565/746, train_loss: 3.4751\n",
      "566/746, train_loss: 3.0444\n",
      "567/746, train_loss: 3.0306\n",
      "568/746, train_loss: 3.0500\n",
      "569/746, train_loss: 3.0330\n",
      "570/746, train_loss: 3.2923\n",
      "571/746, train_loss: 3.1549\n",
      "572/746, train_loss: 2.9957\n",
      "573/746, train_loss: 2.9516\n",
      "574/746, train_loss: 2.9627\n",
      "575/746, train_loss: 3.3785\n",
      "576/746, train_loss: 2.9430\n",
      "577/746, train_loss: 3.2609\n",
      "578/746, train_loss: 3.1050\n",
      "579/746, train_loss: 3.1716\n",
      "580/746, train_loss: 3.0758\n",
      "581/746, train_loss: 2.9913\n",
      "582/746, train_loss: 3.0549\n",
      "583/746, train_loss: 3.1031\n",
      "584/746, train_loss: 2.9804\n",
      "585/746, train_loss: 3.1497\n",
      "586/746, train_loss: 3.3374\n",
      "587/746, train_loss: 3.3629\n",
      "588/746, train_loss: 3.0153\n",
      "589/746, train_loss: 3.0344\n",
      "590/746, train_loss: 3.3705\n",
      "591/746, train_loss: 3.0348\n",
      "592/746, train_loss: 3.3986\n",
      "593/746, train_loss: 3.0408\n",
      "594/746, train_loss: 3.3514\n",
      "595/746, train_loss: 3.0769\n",
      "596/746, train_loss: 3.1146\n",
      "597/746, train_loss: 3.1867\n",
      "598/746, train_loss: 3.1830\n",
      "599/746, train_loss: 2.9824\n",
      "600/746, train_loss: 3.1871\n",
      "601/746, train_loss: 3.3335\n",
      "602/746, train_loss: 2.8723\n",
      "603/746, train_loss: 3.1563\n",
      "604/746, train_loss: 3.1305\n",
      "605/746, train_loss: 2.9637\n",
      "606/746, train_loss: 3.1373\n",
      "607/746, train_loss: 3.4229\n",
      "608/746, train_loss: 3.5002\n",
      "609/746, train_loss: 3.0088\n",
      "610/746, train_loss: 3.0520\n",
      "611/746, train_loss: 3.3989\n",
      "612/746, train_loss: 3.2628\n",
      "613/746, train_loss: 3.2127\n",
      "614/746, train_loss: 3.2011\n",
      "615/746, train_loss: 3.1189\n",
      "616/746, train_loss: 3.3081\n",
      "617/746, train_loss: 3.1322\n",
      "618/746, train_loss: 2.9747\n",
      "619/746, train_loss: 3.0770\n",
      "620/746, train_loss: 3.2253\n",
      "621/746, train_loss: 3.2054\n",
      "622/746, train_loss: 3.0620\n",
      "623/746, train_loss: 3.0822\n",
      "624/746, train_loss: 3.1590\n",
      "625/746, train_loss: 3.2829\n",
      "626/746, train_loss: 3.2820\n",
      "627/746, train_loss: 3.1299\n",
      "628/746, train_loss: 2.8920\n",
      "629/746, train_loss: 3.1347\n",
      "630/746, train_loss: 3.0445\n",
      "631/746, train_loss: 3.3725\n",
      "632/746, train_loss: 2.8982\n",
      "633/746, train_loss: 3.0850\n",
      "634/746, train_loss: 2.9428\n",
      "635/746, train_loss: 3.4590\n",
      "636/746, train_loss: 3.1404\n",
      "637/746, train_loss: 3.3298\n",
      "638/746, train_loss: 3.2612\n",
      "639/746, train_loss: 3.4033\n",
      "640/746, train_loss: 3.1582\n",
      "641/746, train_loss: 3.1987\n",
      "642/746, train_loss: 2.8192\n",
      "643/746, train_loss: 3.2592\n",
      "644/746, train_loss: 3.3167\n",
      "645/746, train_loss: 3.1989\n",
      "646/746, train_loss: 3.3883\n",
      "647/746, train_loss: 3.5407\n",
      "648/746, train_loss: 3.0203\n",
      "649/746, train_loss: 3.0258\n",
      "650/746, train_loss: 3.1127\n",
      "651/746, train_loss: 3.0560\n",
      "652/746, train_loss: 3.3500\n",
      "653/746, train_loss: 3.0752\n",
      "654/746, train_loss: 2.9437\n",
      "655/746, train_loss: 3.1797\n",
      "656/746, train_loss: 3.2262\n",
      "657/746, train_loss: 3.1408\n",
      "658/746, train_loss: 3.0688\n",
      "659/746, train_loss: 3.0781\n",
      "660/746, train_loss: 3.2221\n",
      "661/746, train_loss: 3.1030\n",
      "662/746, train_loss: 3.3890\n",
      "663/746, train_loss: 3.2648\n",
      "664/746, train_loss: 3.0913\n",
      "665/746, train_loss: 3.5155\n",
      "666/746, train_loss: 3.1767\n",
      "667/746, train_loss: 3.1935\n",
      "668/746, train_loss: 2.8927\n",
      "669/746, train_loss: 3.1301\n",
      "670/746, train_loss: 3.0106\n",
      "671/746, train_loss: 3.3484\n",
      "672/746, train_loss: 3.4073\n",
      "673/746, train_loss: 3.1939\n",
      "674/746, train_loss: 3.1810\n",
      "675/746, train_loss: 3.1110\n",
      "676/746, train_loss: 3.0540\n",
      "677/746, train_loss: 3.1211\n",
      "678/746, train_loss: 3.2180\n",
      "679/746, train_loss: 3.2698\n",
      "680/746, train_loss: 3.0598\n",
      "681/746, train_loss: 3.2146\n",
      "682/746, train_loss: 3.4002\n",
      "683/746, train_loss: 3.1656\n",
      "684/746, train_loss: 2.7350\n",
      "685/746, train_loss: 3.1867\n",
      "686/746, train_loss: 3.0233\n",
      "687/746, train_loss: 3.1091\n",
      "688/746, train_loss: 3.2082\n",
      "689/746, train_loss: 3.1234\n",
      "690/746, train_loss: 3.2509\n",
      "691/746, train_loss: 3.1584\n",
      "692/746, train_loss: 3.2787\n",
      "693/746, train_loss: 2.8602\n",
      "694/746, train_loss: 3.0449\n",
      "695/746, train_loss: 3.1578\n",
      "696/746, train_loss: 2.9059\n",
      "697/746, train_loss: 3.1105\n",
      "698/746, train_loss: 3.0050\n",
      "699/746, train_loss: 3.1643\n",
      "700/746, train_loss: 3.0538\n",
      "701/746, train_loss: 3.2870\n",
      "702/746, train_loss: 3.0208\n",
      "703/746, train_loss: 3.0951\n",
      "704/746, train_loss: 3.0901\n",
      "705/746, train_loss: 3.1527\n",
      "706/746, train_loss: 2.9451\n",
      "707/746, train_loss: 3.0636\n",
      "708/746, train_loss: 3.3311\n",
      "709/746, train_loss: 3.4277\n",
      "710/746, train_loss: 3.0261\n",
      "711/746, train_loss: 2.8988\n",
      "712/746, train_loss: 3.0666\n",
      "713/746, train_loss: 3.3510\n",
      "714/746, train_loss: 3.1911\n",
      "715/746, train_loss: 3.0102\n",
      "716/746, train_loss: 3.2901\n",
      "717/746, train_loss: 3.3018\n",
      "718/746, train_loss: 3.1447\n",
      "719/746, train_loss: 3.0126\n",
      "720/746, train_loss: 3.3149\n",
      "721/746, train_loss: 2.8923\n",
      "722/746, train_loss: 3.3775\n",
      "723/746, train_loss: 3.0754\n",
      "724/746, train_loss: 2.9099\n",
      "725/746, train_loss: 2.9311\n",
      "726/746, train_loss: 2.8851\n",
      "727/746, train_loss: 3.1379\n",
      "728/746, train_loss: 3.2376\n",
      "729/746, train_loss: 3.1959\n",
      "730/746, train_loss: 3.0831\n",
      "731/746, train_loss: 3.0084\n",
      "732/746, train_loss: 3.0402\n",
      "733/746, train_loss: 3.0553\n",
      "734/746, train_loss: 2.8937\n",
      "735/746, train_loss: 3.2050\n",
      "736/746, train_loss: 3.1565\n",
      "737/746, train_loss: 3.1850\n",
      "738/746, train_loss: 3.1204\n",
      "739/746, train_loss: 3.2784\n",
      "740/746, train_loss: 3.0377\n",
      "741/746, train_loss: 2.9300\n",
      "742/746, train_loss: 3.1409\n",
      "743/746, train_loss: 3.4194\n",
      "744/746, train_loss: 3.1764\n",
      "745/746, train_loss: 3.0046\n",
      "746/746, train_loss: 3.1577\n",
      "747/746, train_loss: 3.0364\n",
      "epoch 10 average loss: 3.2332\n",
      "saved new best metric model\n",
      "current epoch: 10 current AUC: 0.9771 current accuracy: 0.4729 best AUC: 0.4729 at epoch: 10\n",
      "train completed, best_metric: 0.4729 at epoch: 10\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = list()\n",
    "auc_metric = ROCAUCMetric()\n",
    "metric_values = list()\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print('-' * 10)\n",
    "    print(f\"epoch {epoch + 1}/{epoch_num}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())     ##### .float()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, train_loss: {loss.item():.4f}\")\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                y_pred = torch.cat([y_pred, model(val_images.float())], dim=0)    ##### .float()\n",
    "                y = torch.cat([y, val_labels], dim=0)\n",
    "                \n",
    "            y_onehot = [to_onehot(i) for i in y]\n",
    "            y_pred_act = [act(i) for i in y_pred]\n",
    "            auc_metric(y_pred_act, y_onehot)\n",
    "            auc_result = auc_metric.aggregate()\n",
    "            auc_metric.reset()\n",
    "            del y_pred_act, y_onehot\n",
    "            metric_values.append(auc_result)\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\n",
    "            \n",
    "            if acc_metric > best_metric:\n",
    "                best_metric = acc_metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), 'best_metric_model.pth')\n",
    "                print('saved new best metric model')\n",
    "                \n",
    "            print(f\"current epoch: {epoch + 1} current AUC: {auc_result:.4f}\"\n",
    "                  f\" current accuracy: {acc_metric:.4f} best AUC: {best_metric:.4f}\"\n",
    "                  f\" at epoch: {best_metric_epoch}\")\n",
    "            \n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fa91581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T03:02:43.800417Z",
     "iopub.status.busy": "2021-11-28T03:02:43.799281Z",
     "iopub.status.idle": "2021-11-28T03:02:44.594865Z",
     "shell.execute_reply": "2021-11-28T03:02:44.603737Z"
    },
    "id": "7P1BlRfsDuz4",
    "papermill": {
     "duration": 2.351028,
     "end_time": "2021-11-28T03:02:44.604029",
     "exception": false,
     "start_time": "2021-11-28T03:02:42.253001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsIAAAGDCAYAAAAh/naNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABZnUlEQVR4nO3deXhU1f3H8fc3e0hC2MK+7/saAogrVKVqRVsXUHYVbbW2alu1tmq1tv11s1qtiqyuaK1aba1WwV1ZlZ1A2JQ9YQsJkJDl/P6Yix1jgABJ7iyf1/PMw51778x8Zsic+c6dc88x5xwiIiIiItEmxu8AIiIiIiJ+UCEsIiIiIlFJhbCIiIiIRCUVwiIiIiISlVQIi4iIiEhUUiEsIiIiIlFJhbDUKDNzZtbR7xwiIscS3FaZ2eNm9suq7HsSj3O1mf33ZHNGq5r+LDGzmWb265q6fwldKoSjiJltMrNDZlYYdHnE71wVmdkEr9G70u8sp8rM2nrPJc7vLCKRzMzeNLP7Klk/0sx2nMh70Dl3g3Pu/mrI9I33v3PuWefcead63yeQoZ2ZlZvZY7X1mKHO+4z5qAbv/z0zK/I+Y3eZ2ctm1qzCPt3N7DUzyzezAjN718xOq7BPgpnda2Y5ZnbA+wyfbmZtayp7NFIhHH2+45xLDbrc5HegSowH9gDjauLOVZSKRKRZwBgzswrrxwLPOudKfcgUCsYBe4ErzSzxaDtFarvo4/O6yTmXCnQEUoE/BmXqAHwMLAfaAc2BV4D/mtmQoPt4CbgYuApIB/oAi4HhNRXaAqKqNoyqJytH531D/tjMHvG+oWab2fCg7c29b697zGydmV0XtC3WzH5uZuu9b7aLzaxV0N1/y/tGu8/MHq3kgyo4RxvgLGAycL6ZNfXWP2Zmf6yw7z/N7NagfP8wszwz22hmNwftd6+ZvWRmz5jZfmCCmWWZ2adepu3e804Ius15ZrbGey3+Zmbvm9m1QdsnmdlqM9trZm95uU/0NT/Wa5plZovMbL+Z7TSzP3vrk7znsdvLvtDMmpzoY4tEoFeBhsAZR1aYWX3gIuCp473ng1mFn8nN7KfebbaZ2aQK+15oZp9779XNZnZv0OYPvH/3eUcHh1iFo5Fmdpr3Ps73/j0taNt7Zna/1zYXmNl/zaxRVV8Qr60dB/wCKAG+U2G7M7MbzSwHyPHWXWRmS7zX6RMz6x20/x1B7fwqM7v0GI9d8TU828y2BF3fZGY/MbNl3nN/wcySgrYf6zVPNLM/mtmXXvv4uJklBz+Omd1uZjuAGRVu2w14HBji/Z/sC9pc38z+7T2/+RYoWI/crquZve2112vM7Iqjv/L/45zbR+Bvs2/Q6nuBT51zdznn9jjnCpxzDwNPA//nPd63gHOBkc65hc65UudcvnPuUefctMoey8xaWeDoc573GfGIt/5eM3smaL+v/VLh/Z09YGYfAweBn5rZogr3fYuZveYtH/X1D0vOOV2i5AJsAr51lG0TgFLgFiAeuBLIBxp42z8A/gYkEXhD5wHDvG0/JfDNtgtgBL61NvS2OeBfQD2gtXe7EcfI+Etggbe8HLjNWz4T2AyYd70+cIjAN+kYAt+S7wYSgPbABuB8b997CXwIXOLtmwwMAAYDcUBbYDXwY2//RsB+4Lve9h95t7/W2z4SWAd087b/AvjkKM+nrfcaxFWy7Viv6afAWG85FRjsLV8PvA7UAWK951HX778tXXQJhQvwJDA16Pr1wBJv+ajveW+7Azp6yzOBX3vLI4CdQE8gBXiuwr5nA728tqW3t+8l3rZvvP8JtLUfecsNCBytHevlGu1dP9J+vgesBzp77dZ7wO+C7msZcNUxXo8zgGIC7eVfgdcrbHfA216OZKAfkAsM8tqX8QQ+NxK9/S/nf23ulcABoNlRHvur1zDoddoSdH0TsMC7vwbe/8cNVXzNHwRe826X5rWJvw16nFICBWUikFxJtq/+Dyrk3Q1kef8XzwKzvW0pBD5/Jnrb+gG7gO5Hee7v8b/Pi4bAO8A/g7bvACZWcrtzgDLv/+J3wPsn8LcfCyz1XpsUAp8rp3vb7gWeCdq3LUF/l17eL4Ee3vNLBwqATkG3WQiMOt7rH44X3wPoUov/2YGGpxDYF3S5zts2AdiGV2h66xYQaKBbeW/OtKBtvwVmestrCHxrrewx3ZE3o3f9ReCOY2TM4X8F6Z3AUm/ZvDfqmd7164C53vIg4MsK93MnMMNbvhf44DivzY+BV7zlcQS+rRP02JuDGrb/ANcEbY8h8C26TSX3+7UGJ2j98V7TD4BfAY0q3G4S8AnQ2++/J110CbULcLrXriV51z8GbjnKvl+9573rRyuEp/P14rNz8L6V3O9fgAe95W+8//l6ITwW74t/0PZPgQne8nvAL4K2/QB48wRej6nAq97yEAJf6BtXeM7Dgq4/Btxf4T7WAGcd5f6XcPS2/6vX0Lt+Nt8shMcEXf898PjxXnOvPT4AdAjaPgTYGPQ4h4/8DRwl21f/BxXyBn+JugDI9pavBD6ssP8TwD1Huf/3CHwm5Hu5lwCtg7aXUskBIaCrt38LAl/qZp/A//UQAgdTKjvoci/HL4Tvq3CbZ4C7veVOBArjOsd7/cPxoq4R0ecS51y9oMuTQdu2Ou+v2vMFgW/rzYE9zrmCCttaeMutCBy1OJodQcsHCRzh/AYzG0qgv9Rsb9VzQC8z6+vlmk3giAkE+kw96y23AZp7P+Xt837q+jkQ3GVgc4XH6mxm/7LASTT7gd8QOBKM93y/2t977C1BN28DPBT0WHsINA4tqLrjvabXEGj8s72fSy/y1j8NvAXM9n4y/L2ZxZ/A44pELOfcRwSO1F3i/aydRaAdOd57/li+1h4QeJ9+xcwGWeBEpzwzywduqOL9HrnvLyqsC24HoIrtZ0XeT9WX47WTzrlPCRxMuKrCrsHPrQ1wW4W2tJWXEzMbF9RtYh+BI7ZV7qpRiaM9t2O95hkECrLFQTne9NYfkeecK6rGPG2AQRVel6uBpse4r5udc+kEfiWoD7QM2rYLaFbJbZoB5QR+Fdh9lH2OphXwhTv5vvCbK1x/jq9/3r7qnDtI1V7/sKJCWIK18PqUHdGawFHibUADM0ursG2rt7wZ6MCpG0+goFzi9e2aH7Qe4HngMgv0xx0E/CPo8TdWKPDTnHMXBN13cIEPgSMf2QR++qlLoHA+8ty3E9Roea9JcCO2Gbi+wuMlO+c+OYHneszX1DmX45wbDTQm8BPfS2aW4pwrcc79yjnXHTiNQP/HGjmpUCRMPUXgPTEGeMs5t9Nbf6z3/LFsJ1BkHNG6wvbnCPxM3MorfB4Put+K7U5F2wgUWcGC29ZTcSlQF/ibV/zvIFBgj6+wX3DGzcADFdq2Os65571290ngJgJdN+oBKzj6a3iAQMF0xLGKxoqO9ZrvItAtrkdQxnQXODGtsudUmeNtr2gzgW4Kwa9LqnPu+8e7oXNuOfBrIPj8mHcIfEmp6AoCv0Ye9PbJMrOWlex3tIytrfKTA6vyf1HxNXkbyDCzvgQK4ue89VV5/cOKCmEJ1hi42czizexyAn1g33DObSbwc/xvLXCyVm8CRyyPdL6fCtxvZp0soLeZNTyRB/ZOkriCwElyfYMuPwSuMrM459znBN6EUwl8wO3zbr4AKPBOjki2wMl7Pc1s4DEeMo1AP+BCM+sKBDdo/yZwJPoSr1G5ka83HI8Dd5pZDy97uvd6HUui99olec91K8d4Tc1sjJllOOfKCfzUC1BuZueYWS8zi/XylxA4giAiAU8B3yLQfWpW0PpjveeP5UUCJ9h2N7M6wD0VtqcR+HWnyMyy+PoR1zwC78/2R7nvN4DOZnaVmcVZYMjI7gTOqzhV4wl0MejF/9rToUAfM+t1lNs8CdzgHeU2M0uxwMmAaQT6nTrvOWFmEwkcET6aJcAFZtbAAic9//gEsh/1NffaxCeBB82ssZelhZmdfwL3vxNoaUc5WbIS/yLw/zTW+3yMN7OBFjjxripmEfiF8mLv+q+A0yxwgloDM0szsx8S+AJ3O4Bz7h0CxegrZjbA+/tIM7MbrMLJg54FBL5A/M77f0uywK+sEPi/ONPMWptZOoGug8fknCsB/g78gUBf4Le99dXx+ocUFcLR53X7+jjCrwRtm0+gL9Au4AHgMufcbm/baAL9irYRGOblHu+NCvBnAg3Xfwl80Ewj0Nn/RFxC4FvmU865HUcuBBryOAInT0DgW+m3+N+3U5xzZQSOjPYFNvK/Yjn9GI/3EwIfWAUE3tQvBN3fLgLf1n9P4Oep7sAiAied4Jx7hcBR2tneT6wrgG8f5/kVes/vyGUYx35NRwArzawQeIjASQqHCBTkLxF4nVcD7xPoLiEigHNuE4EvmSkEjtQecdT3/HHu7z8E+v3OJXCS7NwKu/wAuM/MCgicsPti0G0PEmhLP/Z+Rh5c4b53E2i7biPQ1vwMuMhrg47LzFaa2dWVrG9BYIitvwS3p865xQR+xq54VPhInkUEvkA8QuDn+XUE+tPinFsF/IlAH+adBArsj48R72kCJ29tIvDZUKXX23us473mt3vr53lt8DsETtauqrnASmCHmR33tfa6sJ0HjCLQXu/gfyfjHZdz7jCBdvyX3vUcAv3Z+xB4fbYD3yNwgnfwa3oZgS9LLxDob7wCyCTwfCs+RhmBUUE6EugCs4VA32acc29797GMwInlVf2ideTz9u8Vulyc6usfUo6cgS9RzswmEDgZ7HS/s4QaC4ypuAW42jn3rt95REREpHroiLBIJczsfDOrZ4EB6I/0JZzncywRERGpRiqERSo3hMBIGLsI/Nx0idc1QURERCKEukaIiIiISFTSEWERERERiUoqhEVEREQkKlU28HKtaNSokWvbtq1fDy8ickoWL168yzkXtrMpnSi12SISzo7WZlepEDazegTGZe1JYEDtSd50jUe2G4Ex8i4gMC3hBOfcZ8e6z7Zt27Jo0aIqPwERkVBiZhWnxo1oarNFJJwdrc2u6hHhh4A3nXOXeTOx1Kmw/dsEJmLoRGDq28e8f0VEREREQtJx+wh70/GdSWC2MJxzh4Omtj1iJIEZwZxzbh5Qz8yaVXdYEREREZHqUpWT5doRmFt8hpl9bmZTzSylwj4tgM1B17d4677GzCab2SIzW5SXl3fSoUVERERETlVVCuE4oD/wmHOuH3AAuONkHsw5N8U5l+mcy8zIiJpzTEREREQkBFWlEN4CbHHOzfeuv0SgMA62FWgVdL2lt05EREREJCQdtxB2zu0ANptZF2/VcGBVhd1eA8ZZwGAg3zm3vXqjioiIiIhUn6qOGvFD4FlvxIgNwEQzuwHAOfc48AaBodPWERg+bWINZBURERERqTZVKoSdc0uAzAqrHw/a7oAbqy+WiIiIiEjN0hTLIiIiIhKVVAiLiIiISFRSISwiIiIiUamqJ8uFhHW5hRw6XEavlul+RxERERGRWlJYXMranQUA9G9dv9ruN2wKYeccP31pKRvyDvDstYPo2ULFsIiIiEgkOVxazvq8QtbuLGDNDu+ys4Atew8BcHrHRjxz7aBqe7ywKYTNjIdH9ePKJz5l7LT5PD95MF2b1vU7loiIiIicoPJyx+a9B8neUcBar9hds6OAjbsOUFruAIiLMdpnpNCvdX1GDWxF5yZpdGtWvbVf2BTCAK0a1OH5yYO58ol5XP3kfGZPHkynJml+xxIRERGRSjjnyCso/qrQXbOjgLU7C1i7s5BDJWVf7deqQTJdmqRxXo8mdG6SRpemabRvlEpCXM2ezhZWhTBAm4YpPHfdIK6cMo/RT87nhesH0yEj1e9YIiIiIlFtf1HJ147uHil69x4s+WqfRqmJdGmayqisVnRtmkbnJoFLSqI/JWnYFcIA7TNSef66QYyaMo+rnpzHC5OH0LZRit+xRERERCJeUUkZ6/MKv+q/u9YrerflF321T2piHJ2bpDKiZ9OvjvB2aZJGw9REH5N/U1gWwgAdG6fx7LWDGf2kVwxfP4RWDer4HUtEREQkouwqLGbhxj0s2LSHBRv3kL2jgDKvH29CbAztM1LIateAzk3TvjrK26JeMmbmc/LjC9tCGKBL0zSeuWYQo5+cx6gp83jh+sG0rK9iWERERORkOOfYsvcQC72id8GmPWzIOwBAUnwM/VrV5/tndaBrs0DR26ZhCvGx4TstRVgXwgDdm9flmWsGcdXUeVzl9Rlulp7sdywRERGRkOecY11u4VdHexdu3PNVF4e6SXEMbNuAKzJbkdWuAT2bp9f4yWu1LewLYYBeLdN5+ppBjJk6n6u80SSa1E3yO5aIiIhISCktK2fV9v2Bo70b97Bw056vTmZrnJbIwHYNuL5tA7LaNaBLkzRiYkK/e8OpiIhCGKBvq3rMmjSQcdMWcNWT85g9eQgZaaHVIVtERESkNhWVlLF0876vujl89sVeDhwODFvWpmEdhndrQla7BmS1bUCbhnXCol9vdYqYQhhgQJsGzJiYxfjpC7h66jyev25wyJ2dKCIiIlJT9heVsPiLvYGT2zbuYdmWfA6XlQPQtWka3+3fMlD4tmugX8+JsEIYIKtdA6ZNyGTijIVcPXU+z183mPopCX7HEhEREal2FUd0WL19P+UuMCtbzxbpTBzaloFtG5DZtj716qgeqijiCmGA0zo0Yur4TK6ZtYgx0+bz3LWDSa8T73csERERkVNSVFLGRzm7mJO9k/kbvz6iQ//W9fnhsE5ktWtAv9b1qJMQkWVetYrYV+iMThk8MXYA1z+1mHHT5/P0tYOom6RiWERERMJLQVEJc7Nz+e/Knby7JpeDh8tIS4wjq11kj+hQGyK2EAY4p0tj/nZ1f254ZjETpi/gqWsGkerTFH4iIiIiVbWrsJh3Vu3krZU7+Hjdbg6XldMoNZFL+7Xg/B5NGdy+oQrfahDxVeG3ujfhkav6ceNznzNpxkJmThqonwpEREQk5Gzdd4i3VuzgzZU7WLRpD+UOWjVIZvxpbTi/R1P6ta5PbIQPZ1bboqIiHNGzGQ+Nctz8/OdMmrmQGROySE6I9TuWiIiIRLl1uQW8tXInb67YwfKt+QB0aZLGTcM6cX6PJnRvVjfqhjSrTVFRCANc1Ls5pWWOW15cwnVPLWLq+EyS4lUMi4iISO1xzrF8az5vrdzBmyt2sN472a1vq3rc8e2unN+jKe0apficMnpETSEMcEm/FpSWO3760lKuf3oxU8YNIDFOxbCIiIjUnLJyx8JNe3hr5Q7+u3InW/cdIjbGGNy+AeNPa8t53ZvSNF1j+vohqgphgMsGtKS0rJw7Xl7OD575jMfGDFBncxEREalWxaVlfLJuN2+u2ME7q3ey+8BhEuJiOLNTBj/+Vie+1a2J5jkIAVFXCAOMympNSbnjl6+u4IfPf8YjV/UnPlbFsIiIiJy8A8WlvLcmjzdX7uDd7FwKi0tJS4zjnK6NGdGzKWd1ziBFo1eFlKj93xg7uA1lZeXc+/oqfjx7CQ+N6kucimERERE5AXsPHOad1YFhzj7I2cXh0nIapiRwUe9mnN+zKad1aKhumCEsagthgAlD21Fa7vj1v1cTF2v8+Yq+GpZEREREjqms3DFn9U6envcFn6zfTVm5o0W9ZMYMasP5PZqQ2baB6okwEdWFMMC1Z7TncFk5v39zDXExMfzhst7E6I9XRMKAmY0AHgJiganOud9V2N4GmA5kAHuAMc65Ld62MmC5t+uXzrmLay24SJjKP1TCiws3M+vTTWzZe4jm6Ul8/6wOnN+jKT1baJizcBT1hTDAD87uSGmZ489vryUuxvjtd3upGBaRkGZmscCjwLnAFmChmb3mnFsVtNsfgaecc7PMbBjwW2Cst+2Qc65vbWYWCVfrcguY+ckm/rF4K4dKyshq14C7LujGud2bqFtlmFMh7Ll5eCdKysr569x1xMUav76kp77ZiUgoywLWOec2AJjZbGAkEFwIdwdu9ZbfBV6tzYAi4ay83PHe2lxmfLyJD3N2kRAXw8g+zRl/Wlt6tkj3O55UExXCQW49tzMlZY7H319PfGwM93ynu4phEQlVLYDNQde3AIMq7LMU+C6B7hOXAmlm1tA5txtIMrNFQCnwO+fcqxUfwMwmA5MBWrduXe1PQCQUFRSV8NLiLcz6ZBObdh+kSd1EfnJeZ0ZntaZhaqLf8aSaqRAOYmbcPqILpWXlTP1oI3Exxl0XdlMxLCLh6ifAI2Y2AfgA2AqUedvaOOe2mll7YK6ZLXfOrQ++sXNuCjAFIDMz09VebJHat3HXAWZ9somXFm+hsLiU/q3rcet5Xfh2z6YaYjWCqRCuwCxQ/JaWO6Z+tJH4uBh+dn4XFcMiEmq2Aq2Crrf01n3FObeNwBFhzCwV+J5zbp+3bav37wYzew/oB3ytEBaJdM45PszZxcxPNvHumlziYoyLejdnwmlt6dOqnt/xpBaoEK6EmXHPd7pTUlbOY++tJz7GuPW8Ln7HEhEJthDoZGbtCBTAo4Crgncws0bAHudcOXAngREkMLP6wEHnXLG3z1Dg97UZXsRPB4pLefnzrcz8eCPr8w7QKDWRm4d14urBrWmcpqmOo4kK4aMwM+4f2ZPSMsfDc9cRFxvDzcM7+R1LRAQA51ypmd0EvEVg+LTpzrmVZnYfsMg59xpwNvBbM3MEukbc6N28G/CEmZUDMQT6CK/6xoOIRJjNew7y1KebmL1wMwVFpfRqkc6fr+jDhb2badKLKKVC+BhivKHUSsrL+fPbazHgpmEd1U1CREKCc+4N4I0K6+4OWn4JeKmS230C9KrxgCIhwDnHpxt2M+PjTbyzeicxZny7Z1MmDm1H/9b19Jke5VQIH0dMjPGHy/qAgz+9vZbCw6XcMaKr3jgiIiIh7NDhMv65ZCszP9lE9o4C6teJ5wdnd2DM4DY0S0/2O56ECBXCVRAbY/zx8j7USYzlifc3UFhUyv0je2rSDRERkRCzbd8hnvr0C2Yv/JJ9B0vo1qwuv7+sNxf3aU5SvLo/yNepEK6imJhAn+HUxHgef389Bw+X8YfLemtGGREREZ8551i4aS8zP9nIWyt34pzjvO5NmTi0LVntGuhXXDkqFcInwMy449tdSUuK4w9vreHg4VIeHt1PHexFRER84JzjjeU7+Nt761i5bT91k+K49vR2jB3Shpb16/gdT8KACuGTcOM5HUlJiOXe11dx7axFPDF2AHUS9FKKiIjUls17DnLXqyv4YG0eHRun8sClPbm0Xwt9HssJ0V/LSZowtB0piXHc/o9ljJu2gGkTBpKeHO93LBERkYhWUlbO1A838tCctcR64/6PG9KWWJ23IydBhfApuDyzFSmJcfxo9udc9eQ8npqUpXnIRUREashnX+7l5y8vJ3tHAed1b8KvRvbQCBBySlQIn6ILejUjOSGWG55ezJVT5vHMNYNomq5ZaURERKrL/qIS/vDmGp6Z/wVN6ybxxNgBnN+jqd+xJAJoyINqcE6XxsyalMWO/CIuf+ITvtx90O9IIiIiYS9wMtx2vvWn93l2/hdMOK0tb996lopgqTYqhKvJ4PYNefbaQew/VMrlT3xCzs4CvyOJiIiErS17D3LNrEX84NnPyEhL5NUbh3LPd3qQmqgfs6X6VKkQNrNNZrbczJaY2aJKtp9tZvne9iVmdndl9xPp+rSqxwvXD6asHK6cMo8VW/P9jiQiIhJWSsvKmfrhBs798wd8un43v7iwG/+8cSi9W9bzO5pEoBP5WnWOc27XMbZ/6Jy76FQDhbuuTevy9xuGMGbqfEZPmceMiQPJbNvA71giIiIhb9mWfdz58nJWbtvPsK6NuW9kD40HLDVKXSNqQLtGKfz9hiFkpCUydtoCPszJ8zuSiIhIyCosLuVXr6/kkkc/Jq+gmL9d3Z9p4zNVBEuNq2oh7ID/mtliM5t8lH2GmNlSM/uPmfWobAczm2xmi8xsUV5eZBeHzesl88L1Q2jTsA7XzFzEWyt3+B1JREQk5Px35Q7O/fP7zPxkE1cPasM7t53FBb2aaVpkqRVVLYRPd871B74N3GhmZ1bY/hnQxjnXB/gr8Gpld+Kcm+Kcy3TOZWZkZJxs5rCRkZbIC5OH0L15XX7w7Ge88vkWvyOJiIiEhO35h5j81CImP72Y9OR4XrrhNO6/pCd1kzQ5ldSeKhXCzrmt3r+5wCtAVoXt+51zhd7yG0C8mTWq5qxhKb1OPM9cO4istg249cWlPDPvC78jiYiI+Kas3DHz442c++cP+CAnj9tHdOX1H57OgDb1/Y4mUei4J8uZWQoQ45wr8JbPA+6rsE9TYKdzzplZFoECe3dNBA5HqYlxzJg4kBuf/YxfvLqCwuJSbjirg9+xREREatXKbfn8/OXlLN2SzxmdGvHAJb1o3VD9gMU/VRk1ognwitdXJw54zjn3ppndAOCcexy4DPi+mZUCh4BRzjlXQ5nDUlJ8LI+PHcAtLyzhd//JprColNvO66w+UCIiEvEOHi7lL+/kMO2jjdSvE89Do/pycZ/m+gwU3x23EHbObQD6VLL+8aDlR4BHqjda5ImPjeGhUf1ITYzjkXfXUVhcyt0XdScmRg2BiIhEpnezc/nFqyvYuu8Qo7NacfuIrtSrk+B3LBHgxMYRlmoQG2P89ru9SEmMY9pHGzlQXMrvvtebWBXDIiISQXL3F/Gr11fx7+Xb6dg4lRevH0JWO42rL6FFhbAPzIxfXNiN1MQ4HpqTw8HDZTx4ZV8S4jSss4iIhLfycsezC77k9//JprisnNvO7cz1Z3XQZ5yEJBXCPjEzbjm3M6mJcTzwxmoOHC7l8TEDSIqP9TuaiIjIScnesZ87X17O51/u47QODXng0l60a5TidyyRo1Ih7LPrzmxPSmIcd726nPHTFzB1fCZpGkNRRETCSFFJGQ/NyeHJDzZQNzmeP1/Rh0v7tdDJcBLyVAiHgKsGtSYlMZZbX1zKmKnzmTkxi/opOpFARERCX0FRCZNmLmThpr1cNqAlP7+gGw30GSZhQh12QsTIvi14fMwAVu8oYNSUeeQWFPkdSURE5JjyD5UwdtoCPvtyH38d3Y8/Xt5HRbCEFRXCIeTc7k2YMWEgX+45yBWPf8qWvQf9jiQiIlKpvQcOc/XUeazcls/fru7Pd/o09zuSyAlTIRxihnZsxDPXZrH7wGGuePxTNuQV+h1JRETka3YVFjP6yXms3VnIlLGZnN+jqd+RRE6KCuEQNKBNA2ZPHkxxaTlXPPEpq7bt9zuSiIgIEBgfeNSUeWzafYBp4zM5p2tjvyOJnDQVwiGqR/N0Xrh+CHExMYya8inLtuzzO5KIiES57fmHuHLKPLbtO8TMiVmc0SnD70gip0SFcAjr2DiVv98whLrJ8YyZOp8VW/P9jiQiIlFq856DXPHEp+wqKObpa7IY3L6h35FETpkK4RDXqkEdnr9uMGlJ8VytYlhERHywadcBrnziU/IPlvDMtYMY0EZTJUtkUCEcBlo1qMPsyYNJSYhlzLT56jMsIiK1Zl1uIVdO+ZRDJWU8P3kwfVrV8zuSSLVRIRwmWjWow/OTB5McH8vVU+eRvUPFsIiI1Kw1OwoYNeVTysodsycPoUfzdL8jiVQrFcJhpE3DFJ6/bjCJcbFc/eR81u4s8DuSiIhEqBVb8xk15VNizJg9eQhdmqb5HUmk2qkQDjNtG6Xw/OTBxMYYVz05jxwVwyIiUs2Wbt7HVU/OIzk+lhevH0LHxql+RxKpESqEw1A7rxg2M0Y/OZ91uZp0Q0REqsfiL/YwZup80uvE88L1Q2jbKMXvSCI1RoVwmOqQkcrz1w0CYPST81ivGehEROQUzduwm7HTFtAoLZEXJg+hVYM6fkcSqVEqhMNYx8ZpPH/dIMrLHaOnzGPjrgN+RxIRkTD1Uc4uJsxYQPN6ybwweTDN6yX7HUmkxqkQDnOdmqTx3HWDKfWK4U0qhkWihpmNMLM1ZrbOzO6oZHsbM5tjZsvM7D0zaxm0bbyZ5XiX8bWbXELNu9m5TJq1kLYNU5g9eTCN6yb5HUmkVqgQjgBdmqbx3HWDKC4tY/ST8/hy90G/I4lIDTOzWOBR4NtAd2C0mXWvsNsfgaecc72B+4DferdtANwDDAKygHvMrH5tZZfQ8tbKHUx+ehGdm6Ty/HWDaZSa6HckkVqjQjhCdG1al2evHcyhkkAxvHmPimGRCJcFrHPObXDOHQZmAyMr7NMdmOstvxu0/XzgbefcHufcXuBtYEQtZJYQ869l27jx2c/o0TydZ68dTP2UBL8jidQqFcIRpHvzujxzzSAKi0sZNWUeW/aqGBaJYC2AzUHXt3jrgi0FvustXwqkmVnDKt5WItwrn2/h5uc/p1/rejx9TRbpyfF+RxKpdSqEI0zPFuk8c80gCopKGDVlHlv3HfI7koj45yfAWWb2OXAWsBUoq+qNzWyymS0ys0V5eXk1lVF88OLCzdz64lIGtWvIrElZpCWpCJbopEI4AvVqmc4z1w4i/1AJo6fMY5uKYZFItBVoFXS9pbfuK865bc657zrn+gF3eev2VeW23r5TnHOZzrnMjIyMao4vfnl63hf87B/LOKNTBjMmDqROQpzfkUR8o0I4QvVuWY+nrxnE3gOHGf3kPHbkF/kdSUSq10Kgk5m1M7MEYBTwWvAOZtbIzI6083cC073lt4DzzKy+d5Lced46iXDTPtrIL19dwfCujZkydgBJ8bF+RxLxlQrhCNa3VT1mXZPF7sJAMbxzv4phkUjhnCsFbiJQwK4GXnTOrTSz+8zsYm+3s4E1ZrYWaAI84N12D3A/gWJ6IXCft04i2GPvref+f61iRI+mPDZGRbAIgDnnfHngzMxMt2jRIl8eO9os/mIP46YtoEl6ErOv0/iQItXBzBY75zL9zlFb1GaHL+ccD89Zx4PvrOXiPs358xV9iIvVcTCJLkdrs/VOiAID2jRg5qQsduQXMfrJeeQVFPsdSUREaoFzjj/+dw0PvrOW7/VvyYNX9lURLBJE74YoMbBtA2ZMGMi2fUVc9eQ8dhWqGBYRiWTOOR7492oefXc9o7Na8YfLehMbY37HEgkpKoSjyKD2DZk+YSCb9x7kqifnsVvFsIhIRCovd9zz2kqmfrSR8UPa8JtLexGjIljkG1QIR5khHRoyffxAvth9kKunzmfPgcN+RxIRkWpUXu74+SvLeerTL7jujHbce3EPzFQEi1RGhXAUOq1jI6aNH8jGXQe4eup89qoYFhGJCGXljp+8tJTZCzdz0zkd+fkF3VQEixyDCuEodXqnRjw5LpP1eYWMmTaffQdVDIuIhLPSsnJ+/MISXv5sK7ee25mfnN9FRbDIcagQjmJnds5gytgB5OwsZOy0BeQfLPE7koiInKTH3lvP60u3cfuIrtw8vJPfcUTCggrhKHd2l8Y8MXYAa3YUMG76fPIPqRgWEQk3q7bt5+G5OVzUuxnfP7uD33FEwoYKYeGcro15bEx/Vm3fz/jpC9hfpGJYRCRcHC4t59YXl5CenMD9I3v6HUckrKgQFgCGd2vCo1f1Z8XWfMZPX0CBimERkbDw17k5ZO8o4Lff7UX9lAS/44iEFRXC8pXzejTlkav6s3xLPhNmLKSwuNTvSCIicgxLN+/jb++t57v9W3Bu9yZ+xxEJOyqE5WtG9GzKX0f3Y8nmfUycsYADKoZFREJSUUkZt/19KRmpidzznR5+xxEJSyqE5Ru+3asZD4/qx2df7mPSzIUcOlzmdyQREangwbfXsi63kN99rxfpyfF+xxEJSyqEpVIX9m7Gg1f2ZcGmPXz/2cUcLi33O5KIiHgWf7GHKR9uYHRWK87u0tjvOCJhS4WwHNXFfZrz20t78d6aPG55cQll5c7vSCIiUe/Q4TJ+8vdlNE9P5q4Lu/sdRySsxfkdQELbqKzWFBaX8ut/ryYtMY7ffreXZioSEfHR/72ZzcZdB3juukGkJupjXORU6B0kx3XtGe3Zf6iEh+euIzUxjrsu1Nz1IiJ++HT9bmZ+sonxQ9pwWodGfscRCXsqhKVKbjm3M/uLSpn60UbSk+P5oabvFBGpVYXFpfz0paW0bViH27/d1e84IhFBhbBUiZlx90XdKSgq5U9vryU1KY6JQ9v5HUtEJGr85o3VbN13iL9fP4Q6Cfr4FqkOVXonmdkmoAAoA0qdc5kVthvwEHABcBCY4Jz7rHqjit9iYoz/+14vCotL+NXrq0hLiueyAS39jiUiEvE+WJvHc/O/5Loz2pHZtoHfcUQixomMGnGOc65vxSLY822gk3eZDDxWHeEk9MTFxvDw6H6c0akRP3tpKW+u2O53JBGRiJZ/qITb/7GMDhkp3HZeF7/jiESU6ho+bSTwlAuYB9Qzs2bVdN8SYhLjYnli7AD6tqrHzc8v4cOcPL8jiYhErPv/tYqd+4v40xV9SYqP9TuOSESpaiHsgP+a2WIzm1zJ9hbA5qDrW7x1X2Nmk81skZktystT8RTO6iTEMWNiFh0apzL5qcUs/mKP35FERCLOnNU7eWnxFr5/dgf6tqrndxyRiFPVQvh051x/Al0gbjSzM0/mwZxzU5xzmc65zIyMjJO5Cwkh6cnxPDUpi6bpSUyYsZBV2/b7HUlEJGLsPXCYO15eTtemadyskXpEakSVCmHn3Fbv31zgFSCrwi5bgVZB11t66yTCZaQl8sy1g0hLjGPc9PlsyCv0O5KISES49/WV7D1wmD9e3ofEOHWJEKkJxy2EzSzFzNKOLAPnASsq7PYaMM4CBgP5zjmdRRUlWtRL5ulrB+EcjJk6n637DvkdSUQkrL25Yjv/XLKNm4Z1pGeLdL/jiESsqhwRbgJ8ZGZLgQXAv51zb5rZDWZ2g7fPG8AGYB3wJPCDGkkrIatDRipPXZNFQXEpY6fOJ6+g2O9IIiJhaXdhMXe9soKeLepy4zkd/Y4jEtGOO46wc24D0KeS9Y8HLTvgxuqNJuGmR/N0ZkwYyNhpCxg3fQGzJw8mPTne71giImHDOccvXl1BQVEpz13el/jY6hrcSUQqo3eYVKvMtg14YuwA1ucWMmnmQg4eLvU7kohI2Hht6Tb+s2IHPz63E12apvkdRyTiqRCWandm5wweHt2Xz7/cy/VPL6a4tMzvSCIiIS93fxF3/3MlfVvVY/IZ7f2OIxIVVAhLjRjRsxn/973efJizix89v4TSsnK/I4mIhCznHHe+vJyikjL+dEUf4tQlQqRW6J0mNebyzFbcfVF33ly5gzteXk55ufM7kohISHpp8RbmZOfy0/O70CEj1e84IlHjuCfLiZyKSae3o6ColAffWUtqYhz3fKc7ZuZ3LBGRkLFt3yHue30VWW0bMGloO7/jiEQVFcJS424e3pGCohKmfrSRusnx3HpuZ78jiYiEBOcct/9jGaXljj9c3puYGB0oEKlNKoSlxpkZd13YjYKiUh6ek0PdpDiu1YkgIiI8t+BLPszZxf0je9CmYYrfcUSijgphqRVmxm++24vC4lJ+/e/VpCXFceXA1n7HEhHxzeY9B3ng36sZ2rEhVw9q43cckaikQlhqTWyM8eCVfSksLuXOl5eTmhjPhb2b+R1LRKTWlZc7fvL3pcSY8fvL+qhLhIhPNGqE1KqEuBgeHzOAAW3q8+MXPufdNbl+RxIRqXWzPt3E/I17+OVF3WhRL9nvOCJRS4Ww1LrkhFimTRhI5yZpfP+ZxSzYuMfvSCIitWZDXiH/92Y253TJ4IrMVn7HEYlqKoTFF3WT4nlqUhYt6iVzzcyFrNia73ckkbBjZiPMbI2ZrTOzOyrZ3trM3jWzz81smZld4K1va2aHzGyJd3m89tNHpzKvS0RCbAy/+15vDScp4jMVwuKbhqmJPHPtIOomxzNu+gLW5Rb4HUkkbJhZLPAo8G2gOzDazLpX2O0XwIvOuX7AKOBvQdvWO+f6epcbaiW0MPXDDXz25T5+NbIHTeom+R1HJOqpEBZfNUtP5tlrBxFjxpipC9i856DfkUTCRRawzjm3wTl3GJgNjKywjwPqesvpwLZazCcV5Ows4E9vr+W87k24pG8Lv+OICCqEJQS0bZTCM9dmcaikjDHT5pO7v8jvSCLhoAWwOej6Fm9dsHuBMWa2BXgD+GHQtnZel4n3zeyMyh7AzCab2SIzW5SXl1eN0aNPaVk5t/19KSkJsTxwaS91iRAJESqEJSR0bVqXGRMHkldQzNhpC9h38LDfkUQiwWhgpnOuJXAB8LSZxQDbgdZel4lbgefMrG7FGzvnpjjnMp1zmRkZGbUaPNI89t56lm3J5/5LepKRluh3HBHxqBCWkNG/dX2mjstk4+4DjJ+xkMLiUr8jiYSyrUDwkAMtvXXBrgFeBHDOfQokAY2cc8XOud3e+sXAekBzn9eQVdv28/DcHC7s3YyLejf3O46IBFEhLCHltI6NeGR0P1Zszef6pxdxuLTc70gioWoh0MnM2plZAoGT4V6rsM+XwHAAM+tGoBDOM7MM72Q7zKw90AnYUGvJo8jh0nJufXEJ6cnx3D+yp99xRKQCFcIScs7r0ZTff683H6/bzU/+vpTycud3JJGQ45wrBW4C3gJWExgdYqWZ3WdmF3u73QZcZ2ZLgeeBCc45B5wJLDOzJcBLwA3OOQ3oXQP+OjeH7B0F/ObSXjRISfA7johUoCmWJSR9b0BLdhYU8fs319A0PYmfX9DN70giIcc59waBk+CC190dtLwKGFrJ7f4B/KPGA0a5pZv38bf31vPdfi04r0dTv+OISCVUCEvI+v5ZHdiZX8SUDzbQpG4S15zezu9IIiJVUlRSxm1/X0qj1ATu+U4Pv+OIyFGoEJaQZWbc/Z0e5BYUc/+/VtE4LZHv9NGJJiIS+h58Zy3rcguZOXEg6XXi/Y4jIkehPsIS0mJjjAev7EtW2wbc9uJSPlm/y+9IIiLHtPiLvTz5wQZGDWzF2V0a+x1HRI5BhbCEvKT4WJ4cl0nbRnW4/qnFrN6+3+9IIiKVKikr56d/X0qz9GTuulDnNoiEOhXCEhbS68Qzc2IWKYlxTJixgK37DvkdSUTkG17+bAsbdh3g3ot7kJakLhEioU6FsISN5vWSmTUpi4OHyxg/XbPPiUhoKSkr55F319GrRTrf6qYuESLhQIWwhJUuTdN4clwmX+4+yDWzFlFUUuZ3JBERAF79fCub9xzi5uGdMDO/44hIFagQlrAzuH1D/jKqL599uZebn/+cMk24ISI+K/WOBvdoXldHg0XCiAphCUsX9GrGPRd157+rdnLPaysITJYlIuKPfy7Zxhe7D+posEiY0TjCErYmDG3H9v1FPPH+BprWTeKmYZ38jiQiUejI0eCuTdM4t1sTv+OIyAlQISxh7fbzu5K7v5g//nctjesmcUVmK78jiUiUeX3ZNjbuOsBjV/cnJkZHg0XCiQphCWsxMcb/fa83uwqLufPl5WSkJXKOBrAXkVpSVu7469x1dGmSxvk9mvodR0ROkPoIS9hLiIvhsTED6No0jR888xlLN+/zO5KIRIl/LdvGhrwD3Dy8k44Gi4QhFcISEVIT45gxcSCN0hKYNHMhm3Yd8DuSiES4I0eDOzdJ5ds9dTRYJBypEJaI0TgtiVkTsyh3jnHTF5BXUOx3JBGJYG8s38663EJ+OExHg0XClQphiSjtM1KZPmEguQVFTJq5kAPFpX5HEpEIVF7u+OvcHDpkpHBBr2Z+xxGRk6RCWCJOv9b1efSq/qzavp/vP/sZJWXlfkcSkQjz5sodrN1ZyM3DOxGro8EiYUuFsESk4d2a8MAlPflgbR63/2OZJtwQkWpTXu54eE4O7TNSuKh3c7/jiMgp0PBpErFGZbVm5/5iHnxnLU3rJvGzEV39jiQiEeC/q3aQvaOAB6/so6PBImFOhbBEtJuHd2TH/kP87b31NE1PYtyQtn5HEpEwVl7ueGjOOto1SuE7OhosEvZUCEtEMzPuH9mTvILD3PPaShqnJTKip05sEZGT8/bqnazevp8/Xd6HuFj1LhQJd3oXS8SLi43hr6P70bdVPW6evYQFG/f4HUlEwpBzgb7BbRrWYWRfHQ0WiQQqhCUqJCfEMm38QFrWS+baWQtZu7PA70giEmbmrM5l5bb93HhORx0NFokQeidL1GiQksCsSVkkxscyfvoCtucf8juSiIQJ5xwPzcmhVYNkLu3Xwu84IlJNVAhLVGnVoA4zJw6koKiUCdMXkn+oxO9IIhIG3l2Ty/Kt+dx0TkfidTRYJGLo3SxRp0fzdJ4YO4ANuwqZ/NQiikrK/I4kIiHMOcdD7+TQsn4y3+3f0u84IlKNqlwIm1msmX1uZv+qZNsEM8szsyXe5drqjSlSvYZ2bMQfL+/D/I17uO3FpZSXa8INEance2vzWLolnxt1NFgk4pzI8Gk/AlYDdY+y/QXn3E2nHkmkdozs24Kd+4v4zRvZZKQlcs93umOmwfFF5H+OHA1uUS+Z7+losEjEqdJXWzNrCVwITK3ZOCK167oz2jNpaDtmfrKJKR9s8DuOiISYD3N2sWTzPr5/dgcS4nQ0WCTSVPVd/RfgZ0D5Mfb5npktM7OXzKxVZTuY2WQzW2Rmi/Ly8k4wqkj1MzN+cWE3LuzdjN/+J5tXPt/idyQRCRFHRopolp7E5Zk6GiwSiY5bCJvZRUCuc27xMXZ7HWjrnOsNvA3Mqmwn59wU51ymcy4zIyPjpAKLVLeYGOPPV/RhcPsG/PTvy/gwR1/SRAQ+XrebxV/s5QdndyAxLtbvOCJSA6pyRHgocLGZbQJmA8PM7JngHZxzu51zxd7VqcCAak0pUsMS42J5YmwmHRuncsPTi1mxNd/vSCLio8DR4LU0rZvEFQMr/ZFTRCLAcQth59ydzrmWzrm2wChgrnNuTPA+ZtYs6OrFBE6qEwkr6cnxzJyYRXpyPBNnLmTL3oN+RxIRn3y6YTcLN+3l+zoaLBLRTrrnv5ndZ2YXe1dvNrOVZrYUuBmYUB3hRGpb0/QkZk7KoqikjAkzFpJ/UBNuiESjh97JoXFaIlfqaLBIRDuhQtg5955z7iJv+W7n3Gve8p3OuR7OuT7OuXOcc9k1EVakNnRuksaUsZl8ufsg1z29iOJSTbghEk3mbdjN/I17uOGsDiTF62iwSCTTWDAilRjSoSF/uLw3CzThhkjUeeidHDLSErlqUGu/o4hIDTuRCTVEosrIvi3Ynl/E7/6TTfN6yfz8gm5+RxKRGrZg4x4+3bCbX1zYTUeDRaKACmGRY7j+zPZs3XuIKR9soEW9ZMaf1tbvSCJSgx6ek0Oj1ESuHtTG7ygiUgtUCIscg5lx78U92LG/iHtfX0mTukmM6NnU71giUgMWf7GHj9bt4q4LupGcoKPBItFAfYRFjiM2xnh4VD/6tKzHj2Z/zuIv9vodSQQAMxthZmvMbJ2Z3VHJ9tZm9q6Zfe7N/HlB0LY7vdutMbPzazd5aPrLOzk0TEng6sHqGywSLVQIi1RBckIs08Zn0jQ9iWtnLWTjrgN+R5IoZ2axwKPAt4HuwGgz615ht18ALzrn+hEYB/5v3m27e9d7ACOAv3n3F7U++3IvH+bs4roz21MnQT+WikQLFcIiVdQwNZFZE7MwMybMWMCuwuLj30ik5mQB65xzG5xzhwnM/Dmywj4OqOstpwPbvOWRwGznXLFzbiOwzru/qPXQOzk0SElg7GD1DRaJJiqERU5A20YpTB2fyc79RVwzaxEHD5f6HUmiVwtgc9D1Ld66YPcCY8xsC/AG8MMTuC1mNtnMFpnZory8vOrKHXKWbN7H+2vzuPaMdqQk6miwSDRRISxygvq3rs9Do/qxbMs+bn5+CWUaY1hC12hgpnOuJXAB8LSZVbndd85Ncc5lOucyMzIyaiyk3x6ek0O9OvGMG9LW7ygiUstUCIuchPN7NOVXF/fgndU7uee1FTinYlhq3VYgeP7flt66YNcALwI45z4FkoBGVbxtVFi2ZR9zs3O57oz2pOposEjUUSEscpLGDWnL9We255l5X/L4+xv8jiPRZyHQyczamVkCgZPfXquwz5fAcAAz60agEM7z9htlZolm1g7oBCyoteQh5OE5OaQnxzNuiPoGi0Qjff0VOQW3j+jKtvwi/u/NbJrXS2Jk3290sxSpEc65UjO7CXgLiAWmO+dWmtl9wCLn3GvAbcCTZnYLgRPnJrjAzxcrzexFYBVQCtzonCvz55n4Z8XWfN5Zncut53YmLSne7zgi4gMVwiKnICbG+OPlvcndX8RP/r6UjLRETuvQyO9YEiWcc28QOAkueN3dQcurgKFHue0DwAM1GjDEPTQnh7pJcUwY2tbvKCLiE3WNEDlFiXGxTBmbSduGKVz/9GLW7CjwO5KIHMfKbfm8vWonk05vR10dDRaJWiqERapBep14ZkwcSHJ8LBNnLGDn/iK/I4nIMfx1zjrSEuOYOLSd31FExEcqhEWqScv6dZgxcSD5h0qYMGMhBUUlfkcSkUqs3r6fN1fuYOLQtqQn62iwSDRTISxSjXo0T+exMQPI2VnAD579jJKycr8jiUgFf52bQ2piHJNO19FgkWinQlikmp3ZOYPffrcXH+bs4o5/LNcYwyIhZM2OAt5YvoMJp7WlXp0Ev+OIiM80aoRIDbg8sxVb9x3iL+/k0KJ+Mree29nvSCJC4GhwSkIs1+hosIigQlikxvxoeCe27TvEw3NyaJ6exKis1n5HEolqOTsL+Pfy7Xz/rA7UT9HRYBFRISxSY8yMBy7txY79xdz16gqapCdxTpfGfscSiVp/nbuO5PhYrj2jvd9RRCREqI+wSA2Kj43hb1f3p2vTNG589jNWbM33O5JIVFqXW8jry7YxdkgbGuhosIh4VAiL1LDUxDhmTBhI/ToJTJixkM17DvodSSTqPDI3h6S4WCbraLCIBFEhLFILGtdNYtakgRwuLWPCjAXsO3jY70giUWNDXiGvLQ0cDW6Ymuh3HBEJISqERWpJx8ZpPDkuk817DjH5qcUUlZT5HUkkKjzy7joS4mK4TkeDRaQCFcIitWhQ+4b86Yo+LNi0h9v+vpTyco0xLFKTNu06wD+XbGPMoDZkpOlosIh8nUaNEKll3+nTnO35h/jNG9k0T0/irgu7+x1JJGI98u464mKMyWfpaLCIfJMKYREfXHdGe7btK+LJDzfSvF4yE4dqcH+R6vbF7gO88vlWxg1pQ+O0JL/jiEgIUiEs4gMz45cXdWd7/iHu+9cqmqUnMaJnM79jiUSUv727ntgY44azOvgdRURClPoIi/gkNsZ4aFQ/+rWqx49mL2HxF3v8jiQSMTbvOcg/PtvCVVmtaVJXR4NFpHIqhEV8lBQfy9TxA2leL5lrZy1iQ16h35FEIsLj768nxnQ0WESOTYWwiM8apCQwc+JAYswYP2MBeQXFfkcSCWtl5Y43lm/ngl5NaZquo8EicnQqhEVCQJuGKUybMJBdBYe5ZtZCDhSX+h1JJGwt2byXvQdL+Fb3Jn5HEZEQp0JYJET0bVWPR67qx4qt+dz43GeUlJX7HUkkLM1ZnUtsjHFGpwy/o4hIiFMhLBJChndrwgOX9uK9NXnc9cpynNOEGyInam52LgPb1ic9Od7vKCIS4lQIi4SY0VmtuXl4J15ctIW/vJPjdxyRsLJ13yGydxQwvKu6RYjI8WkcYZEQdMu3OrEj/xAPzcmhaXoSo7Na+x1JJCzMzc4F4JyujX1OIiLhQIWwSAgyMx64tBe5BcXc9cpyGqclMrybjnCJHM+72bm0aViHDhkpfkcRkTCgrhEiISo+NoZHr+pPzxbp3PjcZ3z+5V6/I4mEtEOHy/h43S6GdW2MmfkdR0TCgAphkRCWkhjH9AkDaZyWxDWzFrFx1wG/I4mErE/W76K4tJxh6hYhIlWkQlgkxDVKTWTWpCwAxk/XhBsiRzM3O5eUhFiy2jXwO4qIhAkVwiJhoF2jFKaNzyS3oEgTbohUwjnH3OxczuiUQWJcrN9xRCRMqBAWCRP9Wtfn0av6a8INkUqs3l7A9vwidYsQkROiQlgkjGjCDZHKvbsmMGza2V01m5yIVJ2GTxMJM6OzWrM9v4iH5+TQLD2ZW87t7HckEd/NWb2TPi3TaZyW5HcUEQkjOiIsEoZu+VYnrshsyUNzcnh+wZd+xxHx1e7CYj7fvE+TaIjICatyIWxmsWb2uZn9q5JtiWb2gpmtM7P5Zta2WlOKyNccmXDj7C4Z3PXKcuas3ul3JBHfvL82D+fQtMoicsJO5Ijwj4DVR9l2DbDXOdcReBD4v1MNJiLHdmTCjR7NNeGGRLc52bk0TkukR/O6fkcRkTBTpULYzFoCFwJTj7LLSGCWt/wSMNw0rY9IjdOEGxLtSsrK+WBNHud0aUxMjD52ROTEVPWI8F+AnwFHG6+pBbAZwDlXCuQDDU81nIgcX0aaJtyQ6LVo014KiksZ1k39g0XkxB23EDazi4Bc59ziU30wM5tsZovMbFFeXt6p3p2IeDThhkSrudk7SYiN4fSOjfyOIiJhqCpHhIcCF5vZJmA2MMzMnqmwz1agFYCZxQHpwO6Kd+Scm+Kcy3TOZWZkaKxHkeqkCTeij5mNMLM13onKd1Sy/UEzW+Jd1prZvqBtZUHbXqvV4NVoTnYug9o3ICVRo4GKyIk7biHsnLvTOdfSOdcWGAXMdc6NqbDba8B4b/kybx+N9C9Sy4Z3a8KvL9GEG9HAzGKBR4FvA92B0WbWPXgf59wtzrm+zrm+wF+Bl4M2HzqyzTl3cW3lrk6bdh1gQ94BhmvYNBE5SSc9jrCZ3WdmRxrPaUBDM1sH3Ap848iEiNSOqwa15uZhHXlx0Rb+8k6O33Gk5mQB65xzG5xzhwn8YjfyGPuPBp6vlWS1ZG52YDa5YRo2TURO0gn9luScew94z1u+O2h9EXB5dQYTkZN3y7md2Z5fxENzcmiansTorNZ+R5Lq99VJyp4twKDKdjSzNkA7YG7Q6iQzWwSUAr9zzr1aQzlrzNzsXDo2TqV1wzp+RxGRMKVOVSIRyMz4zXd7kVtQzC9eXUHjtESGd9NRsyg2CnjJOVcWtK6Nc26rmbUH5prZcufc+uAbmdlkYDJA69ah9WWqsLiU+Rt3M2loO7+jiEgY0xTLIhEqPjaGv13dn+7N6mrCjcj01UnKnpbeusqMokK3COfcVu/fDQR+6etX8UahfILzRzl5lJQ5hql/sIicAhXCIhFME25EtIVAJzNrZ2YJBIrdb4z+YGZdgfrAp0Hr6ptZorfciMDoQKtqJXU1mbM6l7pJcQxoU9/vKCISxlQIi0Q4TbgRmbzJi24C3gJWAy8651ZWOJEZAgXy7Aoj+XQDFpnZUuBdAn2Ew6YQLi93vLsmj7O6NCYuVh9jInLy1EdYJAocmXBj9JPzuGbWQp6/brDGXY0Azrk3gDcqrLu7wvV7K7ndJ0CvGg1Xg5ZvzWdXYTHDuoZWdw0RCT/6Ki0SJfq1rs8jozXhhoS/udm5xBic1Vn9g0Xk1KgQFoki3+quCTck/M3NzqV/6/o0SEnwO4qIhDkVwiJRRhNuSDjL3V/E8q35nKPRIkSkGqiToEgU0oQbEq7eXROYTW54NxXCInLqVAiLRCFNuCHhas7qXFrUS6ZLkzS/o4hIBFDXCJEopQk3JNwUl5bx0bpdnNM1AzPzO46IRAAVwiJR7MiEGxlpiZpwQ0Le/A17OHi4jOFd9euFiFQPFcIiUS4jLZFZEwMTboybPl8TbkjImpudS1J8DEM6NPQ7iohECBXCIkL7jFSmjc9kV8FhJs5cQGFxqd+RRL7GOcec7J0M7dCIpPhYv+OISIRQISwiQGDCjUev7sfq7QV8/5nFHC7VhBsSOtbnFbJ5zyGGabQIEalGKoRF5CvDujbht5f24sOcXdzxj2WacENCxpzVgWHThmn8YBGpRho+TUS+5oqBrdixv4g/v72WxnWTuOPbXf2OJMKc7Fy6NatLs/Rkv6OISARRISwi3/DDYR3Zsb+Ix99fT9O6iUwY2s7vSBLF8g+WsPiLvXz/rA5+RxGRCKNCWES+wcy4f2RP8gqK+dW/VpGRlsSFvZv5HUui1Ps5eZSVO/UPFpFqpz7CIlKp2Bjjr6P70b91fW55YQnzNuz2O5JEqbmrd9IgJYE+Lev5HUVEIowKYRE5qqT4WKaNz6RVg2Sue2oR2Tv2+x1JokxZueO9tXmc3SWD2BjNJici1UuFsIgcU706CcyalEWdhFgmTF/Itn2H/I4kUeTzL/ey72CJZpMTkRqhQlhEjqtl/TrMnJjFgeJSxk9fwL6Dh/2OJFFiTnYucTHGGZ0b+R1FRCKQCmERqZJuzeoyZVwmX+w+yHVPLaKopMzvSBIF3s3OZWDbBtRNivc7iohEIBXCIlJlQzo05M9X9mHRF3v50ezPKSvXhBtSc7bsPUj2jgKGa7QIEakhKoRF5IRc1Ls5v7ywO2+t3Mm9r63U7HNSY97NDswmd45mkxORGqJxhEXkhE06vR079xfxxAcbaJqexI3ndPQ7kkSgudm5tG1Yh/aNUvyOIiIRSoWwiJyU20d0Zef+Iv7w1hoapyVyeWYrvyNJBDl4uJSP1+9mzKA2mGnYNBGpGSqEReSkxMQYv7+sD7sPHOaOl5fTKC2Rc7roJ2ypHp+s283h0nKGqVuEiNQg9REWkZOWEBfDY2MG0LVpGj945jOWbt7ndySJEHPX5JKSEEtWuwZ+RxGRCKZCWEROSWpiHDMmDqRRWgKTZi5k064DfkeSMOecY+7qXM7snEFCnD6mRKTmqIURkVPWOC2JWROzcMC46QvIKyj2O5KEsVXb97Njf5FGixCRGqdCWESqRfuMVKaNzySvoJhJMxdyoLjU70gSpr4aNk19zkWkhqkQFpFq0691fR69uh+rtu/n+89+RklZud+RJAzNyc6lT6t6ZKQl+h1FRCKcCmERqVbDujbht5f24oO1edz+j2WacENOyO7CYpZs3scwHQ0WkVqg4dNEpNpdMbAVO/YX8ee319KkbhK3j+jqdyQJE++tycM5NK2yiNQKFcIiUiN+OKwjO/YX8dh762laN4nxp7X1O5KEgbnZuTSpm0iP5nX9jiIiUUCFsIjUCDPj/pE9ySso5t7XV5KRlsgFvZr5HUtCWElZOR+szePC3s00m5yI1Ar1ERaRGhMbY/x1dD/6t67Pj19YwvwNu/2OJCFs4aY9FBSXajY5Eak1KoRFpEYlxccybXwmreonc+1Ti1izo8DvSBKi5q7OJSEuhqEdG/kdRUSihAphEalx9eokMGtSFnUSYhk/fQHb9h3yO5KEoLnZuQxu35CURPXaE5HaoUJYRGpFy/p1mDkxiwPFpYyfvoD8gyV+R5IQsnHXATbsOsBwdYsQkVqkQlhEak23ZnV5YtwAvth9kOueWkRRSZnfkSREzPVmk1P/YBGpTSqERaRWndahEX+6og8LNu3hx7OXUFauCTcE5mbvpFPjVFo1qON3FBGJIiqERaTWfadPc355UXfeXLmDX72+UrPPnSQzG2Fma8xsnZndUcn2B81siXdZa2b7graNN7Mc7zK+VoNXUFBUwoKNeximSTREpJbpjAQR8cU1p7dj5/4ipnywgSZ1k7jxnI5+RworZhYLPAqcC2wBFprZa865VUf2cc7dErT/D4F+3nID4B4gE3DAYu+2e2vxKXzlo5xdlJQ5hndt4sfDi0gU0xFhEfHNHSO6cknf5vzhrTW8uHCz33HCTRawzjm3wTl3GJgNjDzG/qOB573l84G3nXN7vOL3bWBEjaY9hjnZuaQnx9O/dT2/IohIlDpuIWxmSWa2wMyWmtlKM/tVJftMMLO8oJ/grq2ZuCISSWJijN9f1oczO2dwx8vLeHvVTr8jhZMWQPC3hy3eum8wszZAO2DuidzWzCab2SIzW5SXl1ctoSsqL3e8tyaXszpnEBerYzMiUruq0uoUA8Occ32AvsAIMxtcyX4vOOf6epep1RlSRCJXQlwMj13dn14t0rnpuc9YtGmP35Ei0SjgJefcCQ3T4Zyb4pzLdM5lZmRk1EiwZVvz2VV4WKNFiIgvjlsIu4BC72q8d9GZLSJSbVIS45g+YSAt6iUzaeZC1u7U7HNVsBVoFXS9pbeuMqP4X7eIE71tjZq7eicxBmd1rplCW0TkWKr0O5SZxZrZEiCXQL+y+ZXs9j0zW2ZmL5lZq0q218rPbCISnhqmJjJrUhZJ8bGMm7aArZp97ngWAp3MrJ2ZJRAodl+ruJOZdQXqA58GrX4LOM/M6ptZfeA8b12tm7smlwFt6lM/JcGPhxeRKFelQtg5V+ac60vgqEGWmfWssMvrQFvnXG8CJ13MOsr91PjPbCISvlo1qMOsSVkcOFzKuGnz2XPgsN+RQpZzrhS4iUABuxp40Tm30szuM7OLg3YdBcx2QWPUOef2APcTKKYXAvd562rVjvwiVmzdzznqFiEiPjmhMxOcc/uAd6lwdrFzbrdzrti7OhUYUC3pRCTqdGtWl6njMtm89xCTZi7k4OFSvyOFLOfcG865zs65Ds65B7x1dzvnXgva517n3DfGGHbOTXfOdfQuM2oz9xHvrgnMJqdh00TEL1UZNSLDzOp5y8kExqzMrrBPs6CrFxM4OiEiclIGtW/IX0f3Y9mWffzg2c8oKSv3O5LUgLnZubSol0znJql+RxGRKFWVI8LNgHfNbBmBn9Deds79q8LPbzd7Q6stBW4GJtRMXBGJFuf3aMoDl/bivTV53P7SMso1FXNEKSop46OcXQzr2hgz8zuOiESp484s55xbhjcbUYX1dwct3wncWb3RRCTajc5qza6CYv709loy0hK584JufkeSajJvw24OlZRpWmUR8ZWmWBaRkHbTsI7kFRbzxAcbaJSayHVntvc7klSDd7NzSY6PZUj7hn5HEZEopkJYREKamXHPd3qwu/AwD7yxmoapCXy3f0u/Y8kpcM4xJzuXoR0bkhQf63ccEYlims9SREJebIzx5yv7cFqHhvzspWVfjTYg4Sknt5Atew8xTKNFiIjPVAiLSFhIjIvlibED6NI0jR888xmff7nX70hykuZmB77IaFplEfGbCmERCRtpSfHMnJhF47qJTJq5kHW5hce/kYScuatz6d6sLk3Tk/yOIiJRToWwiISVjLREnpqURWyMMX76ArbnayrmcLLv4GEWfbGH4RotQkRCgAphEQk7bRqmMHNiFvmHShg/fQH5B0v8jiRV9P7aPMqdukWISGhQISwiYalni3SmjB3Apl0HuWbWQopKyvyOJFUwNzuXhikJ9GlZz+8oIiIqhEUkfJ3WsREPXtmXxV/u5abnPqdUUzGHtNKyct5bk8fZXRoTE6PZ5ETEfyqERSSsXdi7Gfdd3IN3Vu/krldW4JymYg5Vn2/eR/6hEvUPFpGQoQk1RCTsjR3SlryCYh6eu45GaQn89PyufkeSSsxZnUtcjHF6p0Z+RxERAVQIi0iEuOXczuQVFvPou+tplJrIxKHt/I4kFczN3klWuwbUTYr3O4qICKBCWEQihJlx/8ie7C48zH3/WkXD1EQu7tPc71ji2bznIGt3FnJFZiu/o4iIfEV9hEUkYsTFxvDw6H4MbNOA215cwkc5u/yOJJ4j02Jr2DQRCSUqhEUkoiTFx/Lk+Ew6ZKRy/dOLWL4l3+9IQqB/cLtGKbTPSPU7iojIV1QIi0jESU+OZ9akLOrVSWDCjAVs3HXA70hR7eDhUj7dsFtHg0Uk5KgQFpGI1KRuEk9fk4UDxk2fT+7+Ir8jRa2P1+3mcGm5CmERCTkqhEUkYrXPSGX6hIHsLjzM+BkL2V+kqZj9MDd7J6mJcQxs28DvKCIiX6NCWEQiWt9W9XhszABydhYw+alFmoq5ljnnmJudy5mdG5EQp48cEQktapVEJOKd1TmDP17eh3kb9nDLC0soK9fsc7Vl5bb97NxfzDld1C1CREKPCmERiQqX9GvBLy7sxn9W7ODuf2oq5trybnYuZnC2CmERCUGaUENEosa1Z7RnV+FhHn9/PRlpifz4W539jhTx5mTn0qdlPTLSEv2OIiLyDToiLCJR5fYRXfhe/5b85Z0cnpn3hd9xItquwmKWbtmn0SJEJGTpiLCIRBUz43ff68Xeg4f55T9X0DAlgW/3auZ3rIj03po8nNNsciISunREWESiTnxsDI9e1Z8h7RsSF6tmsKbExxpDOzakR/O6fkcREamUjgiLSFRKTojl2WsHYWZ+R4lYI/u2YGTfFn7HEBE5Kh0KEZGopSJYRCS6qRAWERERkaikQlhEREREopIKYRERERGJSiqERURERCQqqRAWERERkaikQlhEREREopIKYRERERGJSiqERURERCQqqRAWEQlTZjbCzNaY2Tozu+Mo+1xhZqvMbKWZPRe0vszMlniX12ovtYhI6NAUyyIiYcjMYoFHgXOBLcBCM3vNObcqaJ9OwJ3AUOfcXjNrHHQXh5xzfWszs4hIqNERYRGR8JQFrHPObXDOHQZmAyMr7HMd8Khzbi+Acy63ljOKiIQ0FcIiIuGpBbA56PoWb12wzkBnM/vYzOaZ2YigbUlmtshbf0kNZxURCUm+dY1YvHjxLjP7wq/HP0mNgF1+h6hl0ficITqfdzQ+Zzj5592muoPUgDigE3A20BL4wMx6Oef2AW2cc1vNrD0w18yWO+fWB9/YzCYDk72rhWa2pvaiVwv9TUcPPefoUa1ttm+FsHMuw6/HPllmtsg5l+l3jtoUjc8ZovN5R+NzhrB+3luBVkHXW3rrgm0B5jvnSoCNZraWQGG80Dm3FcA5t8HM3gP6AV8rhJ1zU4ApNRO/5oXx/+0picbnreccPar7eatrhIhIeFoIdDKzdmaWAIwCKo7+8CqBo8GYWSMCXSU2mFl9M0sMWj8UWIWISJTRqBEiImHIOVdqZjcBbwGxwHTn3Eozuw9Y5Jx7zdt2npmtAsqAnzrndpvZacATZlZO4IDI74JHmxARiRYqhE9M2P5EeAqi8TlDdD7vaHzOEMbP2zn3BvBGhXV3By074FbvErzPJ0Cv2sjos7D9vz1F0fi89ZyjR7U+bwu0kyIiIiIi0UV9hEVEREQkKqkQPg4za2Vm7wZNUfojvzPVJjOLNbPPzexffmepDWZWz8xeMrNsM1ttZkP8zlQbzOwW7+97hZk9b2ZJfmeqCWY23cxyzWxF0LoGZva2meV4/9b3M6Ocumhut6OtzYbobLfVZldfm61C+PhKgducc92BwcCNZtbd50y16UfAar9D1KKHgDedc12BPkTBczezFsDNQKZzrieBE69G+ZuqxswERlRYdwcwxznXCZjjXZfwFs3tdrS12RBl7bba7Opts1UIH4dzbrtz7jNvuYDAG6zi7E0RycxaAhcCU/3OUhvMLB04E5gG4Jw77E08EA3igGQziwPqANt8zlMjnHMfAHsqrB4JzPKWZwGX1GYmqX7R2m5HW5sNUd1uq80OOOU2W4XwCTCztgQGnZ/vc5Ta8hfgZ0C5zzlqSzsgD5jh/bQ41cxS/A5V07yJFf4IfAlsB/Kdc//1N1WtauKc2+4t7wCa+BlGqleUtdt/IbrabIjCdlttdvW22SqEq8jMUoF/AD92zu33O09NM7OLgFzn3GK/s9SiOKA/8Jhzrh9wgCj4mdzrXzWSwAdKcyDFzMb4m8of3nBjGkonQkRTux2lbTZEYbutNvt/qqPNViFcBWYWT6AxfdY597LfeWrJUOBiM9sEzAaGmdkz/kaqcVuALc65I0eOXiLQwEa6bwEbnXN53lS8LwOn+ZypNu00s2YA3r+5PueRahCF7XY0ttkQne222uxqbLNVCB+HmRmBvkernXN/9jtPbXHO3emca+mca0ugE/5c51xEf+N0zu0ANptZF2/VcKJj2tkvgcFmVsf7ex9OhJ9sUsFrwHhveTzwTx+zSDWIxnY7GttsiNp2W212NbbZKoSPbygwlsC36yXe5QK/Q0mN+SHwrJktA/oCv/E3Ts3zjqS8BHwGLCfQLkTkjEVm9jzwKdDFzLaY2TXA74BzzSyHwJGW3/mZUaqF2u3oElXtttrs6m2zNbOciIiIiEQlHREWERERkaikQlhEREREopIKYRERERGJSiqERURERCQqqRAWERERkaikQliilpmdbWb/8juHiIgcn9psqQkqhEVEREQkKqkQlpBnZmPMbIE3KP4TZhZrZoVm9qCZrTSzOWaW4e3b18zmmdkyM3vFm5MdM+toZu+Y2VIz+8zMOnh3n2pmL5lZtpk9683SIyIiJ0lttoQTFcIS0sysG3AlMNQ51xcoA64GUoBFzrkewPvAPd5NngJud871JjDjzpH1zwKPOuf6EJiTfbu3vh/wY6A70J7AjFQiInIS1GZLuInzO4DIcQwHBgALvS/+yUAuUA684O3zDPCymaUD9Zxz73vrZwF/N7M0oIVz7hUA51wRgHd/C5xzW7zrS4C2wEc1/qxERCKT2mwJKyqEJdQZMMs5d+fXVpr9ssJ+JztXeHHQchl6T4iInAq12RJW1DVCQt0c4DIzawxgZg3MrA2Bv93LvH2uAj5yzuUDe83sDG/9WOB951wBsMXMLvHuI9HM6tTmkxARiRJqsyWs6JuUhDTn3Coz+wXwXzOLAUqAG4EDQJa3LZdAnzSA8cDjXqO5AZjorR8LPGFm93n3cXktPg0RkaigNlvCjTl3sr9OiPjHzAqdc6l+5xARkeNTmy2hSl0jRERERCQq6YiwiIiIiEQlHREWERERkaikQlhEREREopIKYRERERGJSiqERURERCQqqRAWERERkaikQlhEREREotL/A/I/cCexfiebAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure('train', (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Validation: Area under the ROC curve\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae605cb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T03:02:48.085587Z",
     "iopub.status.busy": "2021-11-28T03:02:48.084494Z",
     "iopub.status.idle": "2021-11-28T03:04:04.270341Z",
     "shell.execute_reply": "2021-11-28T03:04:04.269364Z"
    },
    "id": "uHAA3LUxD2b6",
    "papermill": {
     "duration": 77.72312,
     "end_time": "2021-11-28T03:04:04.270520",
     "exception": false,
     "start_time": "2021-11-28T03:02:46.547400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_metric_model.pth'))\n",
    "model.eval()\n",
    "y_true = list()\n",
    "y_pred = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n",
    "        pred = model(test_images.float()).argmax(dim=1)   ##### .float()\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5b39302",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-28T03:04:07.229972Z",
     "iopub.status.busy": "2021-11-28T03:04:07.229015Z",
     "iopub.status.idle": "2021-11-28T03:04:07.310226Z",
     "shell.execute_reply": "2021-11-28T03:04:07.309570Z"
    },
    "id": "zOy8uzlwD8se",
    "papermill": {
     "duration": 1.548993,
     "end_time": "2021-11-28T03:04:07.310377",
     "exception": false,
     "start_time": "2021-11-28T03:04:05.761384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A     0.3387    0.7000    0.4565        60\n",
      "          AA     0.3643    0.8500    0.5100        60\n",
      "          Ai     0.6000    0.5500    0.5739        60\n",
      "         Ala     0.6406    0.6833    0.6613        60\n",
      "        Alaa     0.4301    0.6667    0.5229        60\n",
      "        Alai     0.7429    0.4333    0.5474        60\n",
      "        Alam     1.0000    0.1500    0.2609        60\n",
      "        Alau     0.5000    0.4333    0.4643        60\n",
      "         Ale     0.7308    0.3167    0.4419        60\n",
      "        Alee     0.6379    0.6167    0.6271        60\n",
      "         Ali     0.4242    0.4667    0.4444        60\n",
      "         Alo     0.4615    0.3000    0.3636        60\n",
      "        Aloo     0.7429    0.4333    0.5474        60\n",
      "         Alu     0.4306    0.5167    0.4697        60\n",
      "          Am     0.7255    0.6167    0.6667        60\n",
      "         Ana     0.5385    0.8167    0.6490        60\n",
      "        Anaa     0.5357    0.5000    0.5172        60\n",
      "        Anai     0.7273    0.5333    0.6154        60\n",
      "        Anam     0.0000    0.0000    0.0000        60\n",
      "        Anau     0.3833    0.7667    0.5111        60\n",
      "         Ane     0.4310    0.4167    0.4237        60\n",
      "        Anee     0.7222    0.2167    0.3333        60\n",
      "         Ani     0.3788    0.4167    0.3968        60\n",
      "         Ano     0.5200    0.6500    0.5778        60\n",
      "        Anoo     0.4634    0.3167    0.3762        60\n",
      "         Anu     0.8462    0.1833    0.3014        60\n",
      "          Au     0.5439    0.5167    0.5299        60\n",
      "          Ba     0.5244    0.7167    0.6056        60\n",
      "         Baa     0.6000    0.5000    0.5455        60\n",
      "         Bai     1.0000    0.1833    0.3099        60\n",
      "         Bam     1.0000    0.1333    0.2353        60\n",
      "         Bau     0.4545    0.5000    0.4762        60\n",
      "          Be     0.3778    0.2833    0.3238        60\n",
      "         Bee     0.5789    0.3667    0.4490        60\n",
      "         Bha     0.5119    0.7167    0.5972        60\n",
      "        Bhaa     0.5091    0.4667    0.4870        60\n",
      "        Bhai     0.4000    0.3667    0.3826        60\n",
      "        Bham     1.0000    0.2167    0.3562        60\n",
      "        Bhau     0.2921    0.4333    0.3490        60\n",
      "         Bhe     0.5091    0.4667    0.4870        60\n",
      "        Bhee     0.5319    0.4167    0.4673        60\n",
      "         Bhi     0.2963    0.5333    0.3810        60\n",
      "         Bho     0.5111    0.3833    0.4381        60\n",
      "        Bhoo     0.7917    0.3167    0.4524        60\n",
      "         Bhu     0.4737    0.3000    0.3673        60\n",
      "          Bi     0.3611    0.4333    0.3939        60\n",
      "          Bo     0.7143    0.1667    0.2703        60\n",
      "         Boo     0.5769    0.2500    0.3488        60\n",
      "          Bu     0.5000    0.3833    0.4340        60\n",
      "         Cha     0.6164    0.7500    0.6767        60\n",
      "        Chaa     0.4400    0.5500    0.4889        60\n",
      "        Chai     0.2243    0.4000    0.2874        60\n",
      "        Cham     0.0000    0.0000    0.0000        60\n",
      "        Chau     0.4648    0.5500    0.5038        60\n",
      "         Che     0.4615    0.6000    0.5217        60\n",
      "        Chee     0.3382    0.3833    0.3594        60\n",
      "        Chha     0.6866    0.7667    0.7244        60\n",
      "       Chhaa     0.8276    0.4000    0.5393        60\n",
      "       Chhai     0.3016    0.6333    0.4086        60\n",
      "       Chham     1.0000    0.1667    0.2857        60\n",
      "       Chhau     0.5781    0.6167    0.5968        60\n",
      "        Chhe     0.4750    0.3167    0.3800        60\n",
      "       Chhee     0.6207    0.3000    0.4045        60\n",
      "        Chhi     0.3429    0.4000    0.3692        60\n",
      "        Chho     0.7000    0.3500    0.4667        60\n",
      "       Chhoo     0.3488    0.5000    0.4110        60\n",
      "        Chhu     0.2076    0.8167    0.3311        60\n",
      "         Chi     0.5455    0.4000    0.4615        60\n",
      "         Cho     0.5490    0.4667    0.5045        60\n",
      "        Choo     0.4259    0.3833    0.4035        60\n",
      "         Chu     0.4894    0.3833    0.4299        60\n",
      "         DDO     0.5556    0.4167    0.4762        60\n",
      "         DDa     0.4902    0.8333    0.6173        60\n",
      "        DDaa     0.5962    0.5167    0.5536        60\n",
      "        DDai     0.7391    0.5667    0.6415        60\n",
      "        DDam     0.0000    0.0000    0.0000        60\n",
      "        DDau     0.6094    0.6500    0.6290        60\n",
      "         DDe     0.3182    0.7000    0.4375        60\n",
      "        DDee     0.3099    0.3667    0.3359        60\n",
      "        DDha     0.5208    0.8333    0.6410        60\n",
      "       DDhaa     0.6552    0.3167    0.4270        60\n",
      "       DDhai     0.5119    0.7167    0.5972        60\n",
      "       DDham     0.0000    0.0000    0.0000        60\n",
      "       DDhau     0.4112    0.7333    0.5269        60\n",
      "        DDhe     0.4040    0.6667    0.5031        60\n",
      "       DDhee     0.7727    0.2833    0.4146        60\n",
      "        DDhi     0.4359    0.2833    0.3434        60\n",
      "        DDho     0.5366    0.3667    0.4356        60\n",
      "       DDhoo     0.6552    0.3167    0.4270        60\n",
      "        DDhu     0.3582    0.8000    0.4948        60\n",
      "         DDi     0.4918    0.5000    0.4959        60\n",
      "        DDoo     0.4565    0.3500    0.3962        60\n",
      "         DDu     0.5088    0.4833    0.4957        60\n",
      "          Da     0.8103    0.7833    0.7966        60\n",
      "         Daa     0.3636    0.4667    0.4088        60\n",
      "         Dai     0.3750    0.4500    0.4091        60\n",
      "         Dam     0.0000    0.0000    0.0000        60\n",
      "         Dau     0.5614    0.5333    0.5470        60\n",
      "          De     0.3803    0.4500    0.4122        60\n",
      "         Dee     0.3836    0.4667    0.4211        60\n",
      "         Dha     0.4500    0.6000    0.5143        60\n",
      "        Dhaa     0.2867    0.6833    0.4039        60\n",
      "        Dhai     0.6667    0.2667    0.3810        60\n",
      "        Dham     0.7692    0.6667    0.7143        60\n",
      "        Dhau     0.4304    0.5667    0.4892        60\n",
      "         Dhe     0.4308    0.4667    0.4480        60\n",
      "        Dhee     0.4032    0.4167    0.4098        60\n",
      "         Dhi     0.5128    0.3333    0.4040        60\n",
      "         Dho     0.8421    0.2667    0.4051        60\n",
      "        Dhoo     0.4082    0.3333    0.3670        60\n",
      "         Dhu     0.3301    0.5667    0.4172        60\n",
      "          Di     0.4182    0.3833    0.4000        60\n",
      "          Do     0.4878    0.3333    0.3960        60\n",
      "         Doo     0.2885    0.5000    0.3659        60\n",
      "          Du     0.3857    0.4500    0.4154        60\n",
      "           E     0.5139    0.6167    0.5606        60\n",
      "          EE     0.9032    0.4667    0.6154        60\n",
      "          Ga     0.6000    0.6500    0.6240        60\n",
      "         Gaa     0.4588    0.6500    0.5379        60\n",
      "         Gai     0.3723    0.5833    0.4545        60\n",
      "         Gam     1.0000    0.1333    0.2353        60\n",
      "         Gau     0.6250    0.2500    0.3571        60\n",
      "          Ge     0.4324    0.2667    0.3299        60\n",
      "         Gee     0.6000    0.3500    0.4421        60\n",
      "         Gha     0.4762    0.8333    0.6061        60\n",
      "        Ghaa     0.4030    0.4500    0.4252        60\n",
      "        Ghai     0.5556    0.4167    0.4762        60\n",
      "        Gham     1.0000    0.0667    0.1250        60\n",
      "        Ghau     0.5185    0.4667    0.4912        60\n",
      "         Ghe     0.2692    0.7000    0.3889        60\n",
      "        Ghee     0.4407    0.4333    0.4370        60\n",
      "         Ghi     0.5625    0.1500    0.2368        60\n",
      "         Gho     0.7273    0.4000    0.5161        60\n",
      "        Ghoo     0.2303    0.6333    0.3378        60\n",
      "         Ghu     0.4110    0.5000    0.4511        60\n",
      "          Gi     0.3778    0.2833    0.3238        60\n",
      "         Gna     0.5862    0.5667    0.5763        60\n",
      "        Gnaa     0.3444    0.5167    0.4133        60\n",
      "        Gnai     0.5758    0.3167    0.4086        60\n",
      "        Gnam     1.0000    0.1833    0.3099        60\n",
      "        Gnau     0.5800    0.4833    0.5273        60\n",
      "         Gne     0.5370    0.4833    0.5088        60\n",
      "        Gnee     0.4348    0.3333    0.3774        60\n",
      "         Gni     0.3750    0.5000    0.4286        60\n",
      "         Gno     0.5870    0.4500    0.5094        60\n",
      "        Gnoo     0.5517    0.2667    0.3596        60\n",
      "         Gnu     0.3878    0.3167    0.3486        60\n",
      "          Go     0.3083    0.6833    0.4249        60\n",
      "         Goo     0.5000    0.3167    0.3878        60\n",
      "          Gu     0.4182    0.3833    0.4000        60\n",
      "          Ha     0.5811    0.7167    0.6418        60\n",
      "         Haa     0.5918    0.4833    0.5321        60\n",
      "         Hai     0.3878    0.6333    0.4810        60\n",
      "         Ham     1.0000    0.0833    0.1538        60\n",
      "         Hau     0.4688    0.5000    0.4839        60\n",
      "          He     0.3333    0.6667    0.4444        60\n",
      "         Hee     0.4706    0.4000    0.4324        60\n",
      "          Hi     0.4722    0.2833    0.3542        60\n",
      "          Ho     0.9444    0.2833    0.4359        60\n",
      "         Hoo     0.6061    0.3333    0.4301        60\n",
      "          Hu     0.3864    0.5667    0.4595        60\n",
      "           I     0.7143    0.5000    0.5882        60\n",
      "          Ja     0.4636    0.8500    0.6000        60\n",
      "         Jaa     0.3333    0.4833    0.3946        60\n",
      "         Jai     0.4118    0.8167    0.5475        60\n",
      "         Jam     0.0000    0.0000    0.0000        60\n",
      "         Jau     0.7143    0.5000    0.5882        60\n",
      "          Je     0.3580    0.4833    0.4113        60\n",
      "         Jee     0.6066    0.6167    0.6116        60\n",
      "          Ji     0.4750    0.6333    0.5429        60\n",
      "          Jo     0.5373    0.6000    0.5669        60\n",
      "         Joo     0.5854    0.4000    0.4752        60\n",
      "          Ju     0.3333    0.3833    0.3566        60\n",
      "          Ka     0.5778    0.8667    0.6933        60\n",
      "         Kaa     0.4318    0.3167    0.3654        60\n",
      "         Kai     0.6667    0.5333    0.5926        60\n",
      "         Kam     0.0000    0.0000    0.0000        60\n",
      "         Kau     0.5714    0.1333    0.2162        60\n",
      "          Ke     0.5370    0.4833    0.5088        60\n",
      "         Kee     0.4030    0.4500    0.4252        60\n",
      "         Kha     0.5248    0.8833    0.6584        60\n",
      "        Khaa     0.3626    0.5500    0.4371        60\n",
      "        Khai     0.6230    0.6333    0.6281        60\n",
      "        Kham     0.0000    0.0000    0.0000        60\n",
      "        Khau     0.4677    0.4833    0.4754        60\n",
      "         Khe     0.5122    0.3500    0.4158        60\n",
      "        Khee     0.5286    0.6167    0.5692        60\n",
      "         Khi     0.4545    0.1667    0.2439        60\n",
      "         Kho     0.6111    0.5500    0.5789        60\n",
      "        Khoo     0.2130    0.3833    0.2738        60\n",
      "         Khu     0.6471    0.3667    0.4681        60\n",
      "          Ki     0.5714    0.3333    0.4211        60\n",
      "          Ko     0.7037    0.3167    0.4368        60\n",
      "         Koo     0.5000    0.1667    0.2500        60\n",
      "         Ksh     0.4143    0.4833    0.4462        60\n",
      "        Ksha     0.5098    0.4333    0.4685        60\n",
      "       Kshai     0.2463    0.5500    0.3402        60\n",
      "       Ksham     1.0000    0.1500    0.2609        60\n",
      "       Kshau     0.4000    0.6333    0.4903        60\n",
      "        Kshe     0.5000    0.4667    0.4828        60\n",
      "       Kshee     0.4000    0.5667    0.4690        60\n",
      "        Kshi     0.3412    0.4833    0.4000        60\n",
      "        Ksho     0.2574    0.4333    0.3230        60\n",
      "       Kshoo     0.3364    0.6167    0.4353        60\n",
      "        Kshu     0.2162    0.4000    0.2807        60\n",
      "          Ku     0.6250    0.3333    0.4348        60\n",
      "          La     0.5128    0.6667    0.5797        60\n",
      "         Laa     0.3393    0.3167    0.3276        60\n",
      "         Lai     0.7143    0.4167    0.5263        60\n",
      "         Lam     0.0000    0.0000    0.0000        60\n",
      "         Lau     0.4043    0.3167    0.3551        60\n",
      "          Le     0.3380    0.4000    0.3664        60\n",
      "         Lee     0.4706    0.2667    0.3404        60\n",
      "          Li     0.3939    0.2167    0.2796        60\n",
      "          Lo     0.6667    0.5000    0.5714        60\n",
      "         Loo     0.6970    0.3833    0.4946        60\n",
      "          Lu     0.7857    0.3667    0.5000        60\n",
      "          Ma     0.4722    0.8500    0.6071        60\n",
      "         Maa     0.4516    0.4667    0.4590        60\n",
      "         Mai     0.3425    0.4167    0.3759        60\n",
      "         Mam     1.0000    0.1333    0.2353        60\n",
      "         Mau     0.6389    0.3833    0.4792        60\n",
      "          Me     0.3958    0.6333    0.4872        60\n",
      "         Mee     0.4079    0.5167    0.4559        60\n",
      "          Mi     0.3409    0.5000    0.4054        60\n",
      "          Mo     0.3438    0.3667    0.3548        60\n",
      "         Moo     0.6667    0.3667    0.4731        60\n",
      "          Mu     0.5385    0.2333    0.3256        60\n",
      "          Na     0.5300    0.8833    0.6625        60\n",
      "         Naa     0.5538    0.6000    0.5760        60\n",
      "         Nai     0.7209    0.5167    0.6019        60\n",
      "         Nam     1.0000    0.0833    0.1538        60\n",
      "         Nau     0.4458    0.6167    0.5175        60\n",
      "          Ne     0.5079    0.5333    0.5203        60\n",
      "         Nee     0.2891    0.6167    0.3936        60\n",
      "          Ni     0.5556    0.2500    0.3448        60\n",
      "          No     0.5581    0.4000    0.4660        60\n",
      "         Noo     0.5167    0.5167    0.5167        60\n",
      "          Nu     0.4127    0.4333    0.4228        60\n",
      "           O     0.4493    0.5167    0.4806        60\n",
      "          OO     0.6667    0.4000    0.5000        60\n",
      "          Pa     0.4098    0.8333    0.5495        60\n",
      "         Paa     0.4138    0.6000    0.4898        60\n",
      "         Pai     0.6190    0.2167    0.3210        60\n",
      "         Pam     1.0000    0.1167    0.2090        60\n",
      "         Pau     0.6286    0.3667    0.4632        60\n",
      "          Pe     0.3108    0.3833    0.3433        60\n",
      "         Pee     0.5536    0.5167    0.5345        60\n",
      "         Pha     0.3902    0.5333    0.4507        60\n",
      "        Phaa     0.4800    0.2000    0.2824        60\n",
      "        Phai     0.2527    0.7667    0.3802        60\n",
      "        Pham     0.0000    0.0000    0.0000        60\n",
      "        Phau     0.5556    0.4167    0.4762        60\n",
      "         Phe     0.7273    0.4000    0.5161        60\n",
      "        Phee     0.2345    0.5667    0.3317        60\n",
      "         Phi     0.6000    0.2000    0.3000        60\n",
      "         Pho     0.6129    0.3167    0.4176        60\n",
      "        Phoo     0.4848    0.2667    0.3441        60\n",
      "         Phu     0.5116    0.3667    0.4272        60\n",
      "          Pi     0.5000    0.2167    0.3023        60\n",
      "          Po     0.3846    0.6667    0.4878        60\n",
      "         Poo     0.2701    0.7833    0.4017        60\n",
      "          Pu     0.2500    0.5667    0.3469        60\n",
      "          Ra     0.5495    0.8333    0.6623        60\n",
      "         Raa     0.4359    0.2833    0.3434        60\n",
      "         Rai     0.3208    0.5667    0.4096        60\n",
      "         Ram     0.0000    0.0000    0.0000        60\n",
      "         Rau     0.4531    0.4833    0.4677        60\n",
      "          Re     0.3680    0.7667    0.4973        60\n",
      "         Ree     0.6190    0.4333    0.5098        60\n",
      "          Ri     0.3333    0.3667    0.3492        60\n",
      "          Ro     0.6939    0.5667    0.6239        60\n",
      "         Roo     0.8667    0.2167    0.3467        60\n",
      "          Ru     0.4194    0.6500    0.5098        60\n",
      "         SSh     0.4646    0.7667    0.5786        60\n",
      "        SSha     0.3684    0.3500    0.3590        60\n",
      "       SShai     0.3696    0.2833    0.3208        60\n",
      "       SSham     0.0000    0.0000    0.0000        60\n",
      "       SShau     0.4933    0.6167    0.5481        60\n",
      "        SShe     0.3559    0.3500    0.3529        60\n",
      "       SShee     0.6522    0.2500    0.3614        60\n",
      "        SShi     0.7333    0.1833    0.2933        60\n",
      "        SSho     0.4211    0.4000    0.4103        60\n",
      "       SShoo     0.6000    0.1500    0.2400        60\n",
      "        SShu     0.5806    0.3000    0.3956        60\n",
      "          Sa     0.4778    0.7167    0.5733        60\n",
      "         Saa     0.5278    0.3167    0.3958        60\n",
      "         Sai     0.4167    0.5833    0.4861        60\n",
      "         Sam     1.0000    0.0167    0.0328        60\n",
      "         Sau     0.5319    0.4167    0.4673        60\n",
      "          Se     0.4783    0.5500    0.5116        60\n",
      "         See     0.3636    0.5333    0.4324        60\n",
      "         Sha     0.5897    0.7667    0.6667        60\n",
      "        Shaa     0.3654    0.6333    0.4634        60\n",
      "        Shai     0.5303    0.5833    0.5556        60\n",
      "        Sham     0.0000    0.0000    0.0000        60\n",
      "        Shau     0.4062    0.4333    0.4194        60\n",
      "         She     0.4444    0.4000    0.4211        60\n",
      "        Shee     0.4872    0.6333    0.5507        60\n",
      "         Shi     0.4111    0.6167    0.4933        60\n",
      "         Sho     0.3977    0.5833    0.4730        60\n",
      "        Shoo     0.4107    0.3833    0.3966        60\n",
      "         Shu     0.2892    0.8000    0.4248        60\n",
      "          Si     1.0000    0.1167    0.2090        60\n",
      "          So     0.6250    0.2500    0.3571        60\n",
      "         Soo     0.6071    0.2833    0.3864        60\n",
      "          Su     0.4348    0.3333    0.3774        60\n",
      "         TTa     0.7797    0.7667    0.7731        60\n",
      "        TTaa     0.4286    0.2000    0.2727        60\n",
      "        TTai     0.8000    0.3333    0.4706        60\n",
      "        TTam     0.5143    0.6000    0.5538        60\n",
      "        TTau     0.5833    0.3500    0.4375        60\n",
      "         TTe     0.4286    0.5500    0.4818        60\n",
      "        TTee     0.4328    0.4833    0.4567        60\n",
      "        TTha     0.6027    0.7333    0.6617        60\n",
      "       TThaa     0.7391    0.2833    0.4096        60\n",
      "       TThai     0.3784    0.2333    0.2887        60\n",
      "       TTham     1.0000    0.0500    0.0952        60\n",
      "       TThau     0.4697    0.5167    0.4921        60\n",
      "        TThe     0.7333    0.1833    0.2933        60\n",
      "       TThee     0.3226    0.3333    0.3279        60\n",
      "        TThi     0.6667    0.3667    0.4731        60\n",
      "        TTho     0.5692    0.6167    0.5920        60\n",
      "       TThoo     0.2424    0.6667    0.3556        60\n",
      "        TThu     0.3111    0.7000    0.4308        60\n",
      "         TTi     0.6296    0.2833    0.3908        60\n",
      "         TTo     0.6429    0.3000    0.4091        60\n",
      "        TToo     0.3130    0.6833    0.4293        60\n",
      "         TTu     0.3883    0.6667    0.4908        60\n",
      "          Ta     0.5545    0.9333    0.6957        60\n",
      "         Taa     0.5476    0.3833    0.4510        60\n",
      "         Tai     0.6122    0.5000    0.5505        60\n",
      "         Tam     1.0000    0.1000    0.1818        60\n",
      "         Tau     0.4079    0.5167    0.4559        60\n",
      "          Te     0.4000    0.5667    0.4690        60\n",
      "         Tee     0.4839    0.5000    0.4918        60\n",
      "         Tha     0.5146    0.8833    0.6503        60\n",
      "        Thaa     0.3684    0.4667    0.4118        60\n",
      "        Thai     0.2700    0.4500    0.3375        60\n",
      "        Tham     1.0000    0.1667    0.2857        60\n",
      "        Thau     0.7188    0.3833    0.5000        60\n",
      "         The     0.3846    0.4167    0.4000        60\n",
      "        Thee     0.8824    0.2500    0.3896        60\n",
      "         Thi     0.6000    0.2000    0.3000        60\n",
      "         Tho     0.6957    0.5333    0.6038        60\n",
      "        Thoo     0.1348    0.7167    0.2269        60\n",
      "         Thu     0.2030    0.6833    0.3130        60\n",
      "          Ti     0.3611    0.4333    0.3939        60\n",
      "          To     0.5135    0.3167    0.3918        60\n",
      "         Too     0.2869    0.5833    0.3846        60\n",
      "          Tu     0.3827    0.5167    0.4397        60\n",
      "           U     0.6066    0.6167    0.6116        60\n",
      "          Va     0.4274    0.8333    0.5650        60\n",
      "         Vaa     0.6486    0.4000    0.4948        60\n",
      "         Vai     0.4400    0.3667    0.4000        60\n",
      "         Vam     0.0000    0.0000    0.0000        60\n",
      "         Vau     0.3793    0.5500    0.4490        60\n",
      "          Ve     0.1775    0.6833    0.2818        60\n",
      "         Vee     0.3380    0.4000    0.3664        60\n",
      "          Vi     0.2879    0.3167    0.3016        60\n",
      "          Vo     0.5625    0.3000    0.3913        60\n",
      "         Voo     0.4000    0.3667    0.3826        60\n",
      "          Vu     0.5882    0.3333    0.4255        60\n",
      "          Ya     0.6000    0.5500    0.5739        60\n",
      "         Yaa     0.5085    0.5000    0.5042        60\n",
      "         Yai     0.6000    0.2500    0.3529        60\n",
      "         Yam     0.0000    0.0000    0.0000        60\n",
      "         Yau     0.6200    0.5167    0.5636        60\n",
      "          Ye     0.6190    0.2167    0.3210        60\n",
      "         Yee     1.0000    0.1333    0.2353        60\n",
      "          Yi     0.4082    0.3333    0.3670        60\n",
      "          Yo     0.5263    0.3333    0.4082        60\n",
      "         Yoo     0.5000    0.2500    0.3333        60\n",
      "          Yu     0.8889    0.4000    0.5517        60\n",
      "          Za     0.4182    0.7667    0.5412        60\n",
      "         Zaa     0.5660    0.5000    0.5310        60\n",
      "         Zai     0.3684    0.5833    0.4516        60\n",
      "         Zam     1.0000    0.0500    0.0952        60\n",
      "         Zau     0.5060    0.7000    0.5874        60\n",
      "          Ze     0.4727    0.4333    0.4522        60\n",
      "         Zee     0.4318    0.3167    0.3654        60\n",
      "          Zi     0.6250    0.2500    0.3571        60\n",
      "          Zo     0.5918    0.4833    0.5321        60\n",
      "         Zoo     0.5556    0.3333    0.4167        60\n",
      "          Zu     0.4306    0.5167    0.4697        60\n",
      "\n",
      "    accuracy                         0.4416     23100\n",
      "   macro avg     0.5027    0.4416    0.4252     23100\n",
      "weighted avg     0.5027    0.4416    0.4252     23100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1890.832664,
   "end_time": "2021-11-28T03:04:21.678688",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-28T02:32:50.846024",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
